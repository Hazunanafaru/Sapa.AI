{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Putting All Together.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "V2eh045riNbB",
        "vA5D6r8HiUdJ",
        "DjPo-JOn3xjd",
        "9GDp38INUAlm",
        "g8rDmX9gZF2z"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GDp38INUAlm"
      },
      "source": [
        "# FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H48VvZzHTMFz",
        "outputId": "d9d50a1c-acc1-4ab2-f533-16f73d510472"
      },
      "source": [
        "!pip install PySastrawi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PySastrawi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/84/b0a5454a040f81e81e6a95a5d5635f20ad43cc0c288f8b4966b339084962/PySastrawi-1.2.0-py2.py3-none-any.whl (210kB)\n",
            "\r\u001b[K     |█▋                              | 10kB 19.4MB/s eta 0:00:01\r\u001b[K     |███▏                            | 20kB 18.3MB/s eta 0:00:01\r\u001b[K     |████▊                           | 30kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 40kB 12.9MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 51kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 61kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 71kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 81kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 92kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 102kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 112kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 122kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 133kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 143kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 153kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 163kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 174kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 184kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 194kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 204kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 215kB 8.4MB/s \n",
            "\u001b[?25hInstalling collected packages: PySastrawi\n",
            "Successfully installed PySastrawi-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oriBwwIZDWS",
        "outputId": "39d17fd0-7cb4-46fd-a39c-01aa63294601"
      },
      "source": [
        "!pip install fasttext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\r\u001b[K     |████▊                           | 10kB 16.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20kB 21.0MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30kB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext) (2.6.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (56.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3096488 sha256=f1db527460a175cde540d58a83be2bf0d1ec2df386171baa4341fbc1d96017a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZBf5-hOabZi",
        "outputId": "6b0cfbc7-180d-4efe-ee9e-00fff0e0cec5"
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.id.300.bin.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-08 23:58:46--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.id.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4507049071 (4.2G) [application/octet-stream]\n",
            "Saving to: ‘cc.id.300.bin.gz’\n",
            "\n",
            "cc.id.300.bin.gz    100%[===================>]   4.20G  51.4MB/s    in 84s     \n",
            "\n",
            "2021-05-09 00:00:10 (51.2 MB/s) - ‘cc.id.300.bin.gz’ saved [4507049071/4507049071]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY4Fjudoa1zg"
      },
      "source": [
        "!gunzip cc.id.300.bin.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPieB43db-8-",
        "outputId": "9eec602b-87a9-4116-b48b-b237f2690b15"
      },
      "source": [
        "# step 1 - stemming\n",
        "\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "sentence = 'Ass pak,,saya mau tanya jika seorang anak dtahan oleh bapaknya...dan tidak boleh bertemu dengan ibu nya... apa yang harus saya lakukan...saya sdh bicara baik2 dgn mantan suami saya... mohon sarannya terimakasih'\n",
        "output_stem   = stemmer.stem(sentence)\n",
        "\n",
        "print(output_stem)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ass pak saya mau tanya jika orang anak dtahan oleh bapak dan tidak boleh temu dengan ibu nya apa yang harus saya laku saya sdh bicara baik2 dgn mantan suami saya mohon saran terimakasih\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7qF5L-qR1jQ",
        "outputId": "7ea346f9-f309-4681-c2e7-f493c02e895d"
      },
      "source": [
        "# step 2 - remove stop word & tokenize\n",
        "\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "factory = StopWordRemoverFactory()\n",
        "stopword = factory.create_stop_word_remover()\n",
        "output_stopwordremoved = stopword.remove(output_stem)\n",
        "output_token = nltk.tokenize.word_tokenize(output_stopwordremoved)\n",
        "print(output_token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['ass', 'anak', 'dtahan', 'temu', 'sdh', 'bicara', 'baik2', 'dgn', 'mantan', 'suami', 'saran', 'terimakasih']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulApXbiKTCbW",
        "outputId": "ebc361a4-382c-44f0-a7c3-84e26262b625"
      },
      "source": [
        "# step 3 - vectorize\n",
        "\n",
        "import fasttext\n",
        "\n",
        "model = fasttext.load_model(\"cc.id.300.bin\")\n",
        "\n",
        "for i in output_token:\n",
        "  print('The vector of word {} is {} \\n \\n'.format(i, model[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The vector of word ass is [ 8.80907476e-02  3.17557417e-02  1.21719670e-02 -8.42830464e-02\n",
            " -1.43240821e-02  9.20897946e-02  9.01683941e-02  1.14310436e-01\n",
            "  1.80601463e-01 -2.41307616e-02 -3.83066162e-02  8.96381438e-02\n",
            "  7.91572481e-02  1.47737831e-01 -1.38649866e-01 -1.27829716e-01\n",
            " -2.92995777e-02  1.60466135e-01 -1.61451146e-01  1.14374071e-01\n",
            " -7.95074850e-02 -8.35267380e-02 -1.61519628e-02 -1.93460152e-01\n",
            " -2.01510191e-01 -2.78504007e-02 -6.95109591e-02 -1.19584881e-01\n",
            " -9.42851603e-02  3.81834395e-02 -4.03484851e-02  3.04525439e-02\n",
            " -1.22284524e-01  7.99141638e-03 -4.37111221e-02  7.39776157e-03\n",
            "  7.50823393e-02  6.78003132e-02  7.94992000e-02  2.90384702e-02\n",
            " -6.21190574e-03  7.39689395e-02  6.46714270e-02  4.07305285e-02\n",
            " -2.20952064e-01 -6.08962588e-02 -1.33563682e-01 -1.03997551e-01\n",
            "  1.21809289e-01  3.01540885e-02 -1.73372135e-01 -8.01222324e-02\n",
            " -1.02392040e-01  1.20919056e-01  7.31596947e-02 -7.50856772e-02\n",
            " -1.99016631e-01  1.03791140e-01 -1.10355377e-01 -4.01893258e-02\n",
            "  5.88911027e-02 -1.59599051e-01  1.78789608e-02 -1.57531947e-02\n",
            " -1.20282426e-01 -1.01584122e-01 -1.85612753e-01  1.16647780e-01\n",
            " -1.21631712e-01 -4.74517494e-02 -1.62536830e-01 -1.80979118e-01\n",
            " -4.09307703e-02  5.91463521e-02 -8.09075683e-02  1.06226429e-02\n",
            " -1.33602187e-01 -2.29420811e-01 -1.96007371e-01  6.51088357e-02\n",
            "  1.45301342e-01 -4.91788425e-02  6.78680241e-02 -5.54721951e-02\n",
            " -1.00595564e-01 -3.00397370e-02  9.34517533e-02  1.12056971e-01\n",
            "  8.99139717e-02  5.93849160e-02 -2.29958668e-02 -1.56915039e-01\n",
            " -3.04387137e-02 -6.84267748e-03  3.86926718e-02 -4.23219293e-01\n",
            " -3.27723660e-03  5.19445762e-02  9.03597474e-02 -2.31226031e-02\n",
            "  5.83817158e-03  9.94740240e-03  3.84716466e-02 -1.34014025e-01\n",
            " -6.26234338e-04 -6.51708618e-02 -1.87837631e-02 -3.62489186e-02\n",
            " -2.39505414e-02  3.45789082e-02  1.29221201e-01 -1.94243237e-01\n",
            " -4.76929545e-03 -5.77367023e-02  5.20618223e-02  8.74310806e-02\n",
            " -1.84203871e-03  1.20481402e-01 -1.77988455e-01  1.76688302e-02\n",
            " -1.94440875e-02 -1.23533823e-01  1.17107052e-02  2.09787369e-01\n",
            "  8.60030763e-03  9.83903557e-02 -1.11345276e-01 -5.86810634e-02\n",
            "  2.11945735e-02 -3.31301503e-02  9.68260393e-02 -6.02756962e-02\n",
            " -8.80886540e-02 -7.35073015e-02  6.03106022e-02  1.56559795e-01\n",
            "  8.12436864e-02 -1.29640147e-01  1.05163395e-01  1.74032703e-01\n",
            "  5.19921891e-02 -1.62538812e-02  3.12415324e-03  8.28333199e-02\n",
            "  3.80221829e-02 -1.01888612e-01  1.01425573e-01 -1.14957571e-01\n",
            " -8.38406757e-02 -1.81133151e-01  3.68453935e-02 -4.01622951e-02\n",
            " -5.00568524e-02  1.43865123e-01 -1.97333828e-01 -7.33005702e-02\n",
            "  1.74522147e-01  1.90932795e-01 -6.73148185e-02  1.34580478e-01\n",
            " -3.92486304e-02 -4.43917513e-03 -5.47224134e-02  1.52393728e-01\n",
            "  3.23617458e-03 -2.53895111e-02  6.84896335e-02  6.80674538e-02\n",
            " -5.80198616e-02 -1.11428499e-01 -5.28083257e-02  5.79050407e-02\n",
            " -5.81544638e-02 -2.40036882e-02 -1.15331069e-01 -3.04157864e-02\n",
            "  2.16317043e-01  1.92421883e-01  7.43133351e-02 -1.28453776e-01\n",
            "  1.76514000e-01 -1.96259283e-03  9.76249576e-02 -2.57491261e-01\n",
            " -3.78073603e-02  1.31214380e-01  2.05496952e-01  1.47837758e-01\n",
            "  2.28231661e-02 -3.45809869e-02 -1.18361518e-01  1.04492091e-01\n",
            "  5.43517992e-04 -9.30652991e-02 -1.08422004e-01 -1.44433528e-01\n",
            " -1.08081564e-01 -3.62958089e-02  1.87466204e-01  4.07065712e-02\n",
            " -1.66306034e-01  5.87029979e-02 -5.04914336e-02 -3.41506023e-03\n",
            "  2.15289928e-02 -5.55426627e-02  7.31450319e-02  1.28353923e-03\n",
            "  5.43244779e-02  3.49314548e-02 -1.95883960e-02 -1.82955593e-01\n",
            "  5.34740128e-02 -1.79026276e-04  2.10301876e-01 -1.23928532e-01\n",
            " -1.05748884e-02  1.28202304e-01 -2.77974270e-03  8.82267505e-02\n",
            "  3.86343822e-02  2.06360281e-01 -2.19363600e-01  5.65109961e-02\n",
            " -6.21242747e-02  1.67268291e-02 -1.88300181e-02  3.94466281e-01\n",
            "  8.13918039e-02  1.03844963e-02 -9.32342708e-02  4.64218408e-02\n",
            " -1.11341383e-02 -1.36018516e-02  5.35950623e-02  1.97186753e-01\n",
            "  1.43724069e-01  9.02153254e-02 -7.25260377e-02 -3.58866081e-02\n",
            " -2.32387260e-02 -7.43061155e-02 -2.43297040e-01 -1.30270153e-01\n",
            "  1.87758088e-01 -3.52262333e-03 -6.90913945e-03 -1.08854130e-01\n",
            "  4.55334671e-02  1.86865285e-01  2.82742202e-01 -1.20477855e-01\n",
            " -1.67039298e-02 -1.28993243e-02 -1.09485649e-02  1.21963307e-01\n",
            " -1.62001073e-01  1.66657656e-01  1.18698664e-01 -1.85318977e-01\n",
            " -3.36398929e-02  1.29399121e-01 -2.10997775e-01 -2.54987240e-01\n",
            "  9.65739861e-02 -7.79408738e-02  9.01549309e-02 -5.22573069e-02\n",
            "  2.16905221e-01 -4.53971326e-02 -1.05588749e-01  2.37849385e-01\n",
            " -3.05326402e-01  5.54553196e-02 -4.38281260e-02 -2.99974214e-02\n",
            " -1.42905325e-01  1.47352330e-02  2.24917047e-02  5.94927035e-02\n",
            " -9.28878710e-02 -1.69497102e-01  5.98442666e-02  5.53052574e-02\n",
            "  8.81166570e-03  9.21511650e-02 -5.93335554e-03 -7.97199905e-02\n",
            " -2.19115451e-01 -7.34528676e-02 -8.37849900e-02 -5.72875366e-02\n",
            " -2.05184706e-03 -4.05893549e-02  1.03495426e-01 -1.14047024e-02\n",
            "  1.52452230e-01  1.08030632e-01  1.64369550e-02  1.12269893e-02] \n",
            " \n",
            "\n",
            "The vector of word anak is [ 9.76612195e-02 -4.98065464e-02 -8.08498040e-02  1.44888878e-01\n",
            "  3.14107649e-02 -1.59987003e-01 -1.94163062e-02 -6.70783734e-03\n",
            " -4.32265215e-02 -1.40409917e-01 -2.35193074e-02  2.18658336e-02\n",
            "  4.19238470e-02 -1.48690119e-02 -6.55228794e-02 -1.00250449e-02\n",
            " -7.54116196e-03  3.12829912e-02  5.22206947e-02  6.87512904e-02\n",
            " -2.65286323e-02  1.67192183e-02  6.10518530e-02  1.30209029e-01\n",
            "  6.01377189e-02  7.79421180e-02 -2.21053138e-02  7.52581377e-03\n",
            "  1.36041448e-01  8.31929385e-04  1.33709922e-01 -5.67621216e-02\n",
            "  1.64912596e-01 -4.87471223e-02  2.18546093e-02  5.37452511e-02\n",
            " -1.34888142e-02 -8.22329335e-03  4.48499620e-02  4.45497632e-02\n",
            " -8.11074302e-03 -5.14005646e-02  2.83638947e-02  6.77171946e-02\n",
            "  6.48007616e-02  5.87227941e-02 -4.40918878e-02  5.30245528e-02\n",
            " -1.01378128e-01 -6.41719028e-02 -1.10293590e-01  3.27164074e-03\n",
            " -6.63668811e-02 -6.84297308e-02  2.38623619e-02  1.35707051e-01\n",
            " -9.02977120e-03 -3.28141525e-02 -4.17350680e-02 -2.30033360e-02\n",
            "  5.43236639e-03  2.03795582e-02  6.39431924e-02  3.69260572e-02\n",
            "  9.68442857e-02 -1.06766671e-01 -1.04967259e-01 -3.85306254e-02\n",
            "  5.53381816e-02  4.36536148e-02  3.53406626e-03  3.77604142e-02\n",
            "  4.90966812e-02 -1.28862390e-03 -3.00476095e-03 -9.13669467e-02\n",
            "  7.34643787e-02 -5.12095019e-02 -4.15592775e-04  5.04213572e-02\n",
            "  2.54009794e-02  3.78011875e-02  4.36390564e-02  4.98656929e-03\n",
            " -4.38112691e-02 -4.24583033e-02 -1.96688026e-01  4.07381877e-02\n",
            "  9.58038196e-02 -1.88100711e-02 -4.32361625e-02  5.79616688e-02\n",
            " -8.72953981e-02 -4.62975278e-02 -2.14490473e-01 -1.58497363e-01\n",
            "  6.29703552e-02  1.20741919e-01  6.06235676e-02 -1.58002116e-02\n",
            "  2.11257599e-02  1.05062528e-02  4.22616713e-02  1.31340604e-02\n",
            "  5.11336736e-02  3.78734097e-02  6.72585815e-02 -5.16256019e-02\n",
            "  3.88235115e-02 -2.05741487e-02  5.71454018e-02 -6.73957393e-02\n",
            "  1.24615207e-01 -3.65266055e-02 -1.53843537e-02  1.43745691e-02\n",
            "  4.64356653e-02 -6.66955300e-03  3.19761923e-03  1.27631035e-02\n",
            " -4.08223942e-02 -3.08275409e-03 -1.36876395e-02  1.03382524e-02\n",
            "  4.15512919e-02 -1.86540633e-02 -5.91305047e-02 -3.54113616e-02\n",
            " -1.00379318e-01  1.60138141e-02  7.47595355e-03  3.16758268e-02\n",
            " -1.72398183e-02  2.68875044e-02  7.61621892e-02  5.60695156e-02\n",
            " -1.23857241e-02 -1.52200861e-02 -6.38488904e-02 -3.39283794e-02\n",
            "  6.63358904e-03 -1.90482855e-01  1.42310215e-02 -3.29799205e-02\n",
            " -5.79367727e-02  2.95536425e-02 -6.73017045e-03 -2.42077522e-02\n",
            "  3.31800804e-02 -3.53902280e-02  2.86231190e-02 -2.86385641e-02\n",
            "  6.19129613e-02  2.63545122e-02  1.25490293e-01 -1.29446788e-02\n",
            " -4.05771956e-02 -5.69542013e-02 -7.24315643e-03  4.84037809e-02\n",
            "  1.17046133e-01 -4.48178649e-02 -1.71373487e-02  3.11869718e-02\n",
            " -5.25793945e-03  3.31392251e-02  9.46286172e-02  3.95551324e-03\n",
            "  6.62372261e-02 -5.68120591e-02  6.36327546e-03  4.30205129e-02\n",
            "  3.84406894e-02  1.06237968e-02 -1.00816321e-02 -8.55179410e-03\n",
            " -6.19301163e-02 -5.82365915e-02 -4.67368141e-02 -2.82350779e-02\n",
            " -7.91077502e-03 -2.39293575e-02  4.49826643e-02 -8.32664892e-02\n",
            "  4.94411960e-02 -6.56075729e-03  6.16932437e-02  1.76761359e-01\n",
            " -4.03499678e-02 -1.12248752e-02 -2.10929886e-02 -7.03046620e-02\n",
            " -1.56746209e-02 -2.45970534e-03 -3.27233002e-02 -2.17655301e-02\n",
            "  5.57526276e-02  3.82687710e-02  9.86169744e-03  1.89205751e-01\n",
            "  8.69334023e-03 -7.64175728e-02  4.72160541e-02 -7.11194426e-02\n",
            "  1.33473426e-03  1.21638160e-02  3.89513075e-02 -2.25033686e-02\n",
            " -1.43527193e-02 -3.28002870e-02  5.62636321e-03 -3.27671319e-02\n",
            "  9.72361192e-02  3.46974209e-02 -7.19153276e-03 -2.79449928e-03\n",
            " -4.12521884e-03  8.43316317e-02  3.93225476e-02 -4.97807078e-02\n",
            "  3.94861177e-02  7.19564781e-02 -4.57852595e-02  1.59890447e-02\n",
            " -2.78602801e-02 -2.87666824e-02  7.46045262e-03  5.67615330e-02\n",
            " -1.50050726e-02 -3.26823853e-02  5.40105216e-02  8.97461176e-03\n",
            "  1.96909141e-02 -1.16384285e-06 -4.53930758e-02  8.68665334e-03\n",
            "  3.55540141e-02  1.37293967e-03 -5.76493591e-02  3.04866061e-02\n",
            "  4.96624336e-02  2.19735354e-02  9.04599950e-03 -1.68927144e-02\n",
            " -4.33550682e-03 -3.82785755e-03 -3.80903706e-02  2.25601047e-02\n",
            " -3.84227652e-03 -3.78279835e-02  2.36292984e-02 -6.99731931e-02\n",
            " -7.95144401e-03 -4.71237600e-02  5.56141734e-02 -1.55421775e-02\n",
            "  1.10731013e-02  1.39710249e-03  4.33713458e-02 -8.50195140e-02\n",
            "  2.31428724e-02 -1.90568157e-02 -7.37600848e-02  1.74150169e-02\n",
            " -1.19582295e-01  9.79809929e-03 -1.75895579e-02  7.99350515e-02\n",
            "  3.86127271e-03  1.00189894e-02  3.57721858e-02 -3.09079769e-03\n",
            " -7.49389362e-03 -2.97465324e-02 -1.41784951e-01  1.40858501e-01\n",
            " -1.42420769e-01  2.59549320e-02  6.23599403e-02  1.90313682e-02\n",
            " -1.03423715e-01 -8.82895514e-02 -9.29260999e-02 -1.41338274e-01\n",
            "  9.27892476e-02  4.62429672e-02 -5.63822081e-03  4.39641587e-02\n",
            "  1.19716953e-02  2.59936955e-02 -5.91466241e-02 -7.64743835e-02\n",
            " -4.11660522e-02 -3.39521281e-02  3.51759139e-04  8.53778236e-03\n",
            "  4.16519642e-02 -6.96503371e-02 -3.47268283e-02  1.94897383e-01] \n",
            " \n",
            "\n",
            "The vector of word dtahan is [ 4.53315442e-05 -2.97476240e-02  4.26215120e-02  6.27065375e-02\n",
            "  1.84092913e-02 -9.78702009e-02 -1.81828886e-02 -1.68434177e-02\n",
            " -2.60470249e-03 -3.60513963e-02  3.44657246e-03 -4.57397550e-02\n",
            " -4.02420089e-02 -2.99911108e-02  6.23580534e-03  1.72027182e-02\n",
            "  6.39383774e-03 -8.84590950e-03 -8.57372489e-03  3.81487422e-03\n",
            " -3.38282250e-02  2.25274786e-02 -8.32646806e-03  7.69902579e-03\n",
            "  1.15121920e-02  1.51540693e-02 -1.86962076e-02 -1.34424167e-03\n",
            "  9.23213884e-02 -1.16527546e-03  2.48850882e-03 -2.05248296e-02\n",
            "  1.60754379e-02  7.88789243e-03  5.94236888e-03  6.34940946e-03\n",
            "  1.24089774e-02  2.54179761e-02 -3.28204129e-03  1.49618257e-02\n",
            "  1.80382989e-02 -1.92277934e-02  2.52663624e-04  2.82297358e-02\n",
            "  9.93645936e-03  6.32704869e-02  5.42164966e-03  1.04793282e-02\n",
            "  1.01329749e-02 -4.96527813e-02 -8.76344368e-03 -1.60507075e-02\n",
            " -2.39664572e-03 -1.84258539e-02  6.14634296e-03  2.00786330e-02\n",
            "  4.93409000e-02 -2.46442552e-03 -5.29054031e-02  4.46390826e-04\n",
            "  1.14545599e-02  2.83480715e-03 -6.06696494e-03  2.27152333e-02\n",
            "  1.59535073e-02  2.72818748e-02 -7.20385183e-03  4.28011175e-03\n",
            "  1.45148821e-02 -2.26922613e-02  2.82109194e-02  1.51677607e-02\n",
            "  7.04551442e-03 -9.06972028e-03 -5.37835155e-03 -1.00465510e-02\n",
            "  3.39110047e-02 -1.84284113e-02  1.57615952e-02  1.29751740e-02\n",
            "  1.63519140e-02 -1.28345005e-02 -1.69425029e-02  2.01015770e-02\n",
            " -2.74030678e-02  3.06058116e-02 -5.85054755e-02 -2.70977989e-02\n",
            " -3.39066098e-03  7.15159252e-03 -2.06849538e-02 -5.72897494e-03\n",
            " -9.86121148e-02 -5.88517543e-03  2.69108620e-02 -1.78177115e-02\n",
            "  1.59247741e-02 -1.48276053e-03  1.10020284e-02  3.79607826e-02\n",
            "  6.03269739e-03 -1.53797762e-02 -2.49265209e-02 -7.06707779e-03\n",
            "  8.58019479e-03 -1.59849636e-02  2.41804682e-03 -3.99730951e-02\n",
            "  1.09669473e-02 -2.23303679e-03 -4.66110483e-02 -1.43771823e-02\n",
            " -2.32461677e-03  9.41543095e-03 -4.08399105e-02 -3.10788723e-03\n",
            "  9.12612211e-03 -1.66247729e-02  1.49764959e-02  1.15562249e-02\n",
            " -1.02097169e-04 -1.55498786e-03  1.75871328e-02 -2.17280667e-02\n",
            " -9.87755880e-03 -1.73899326e-02  1.14817768e-02 -4.89360327e-03\n",
            "  3.53891030e-03 -2.61951294e-02  4.95333504e-03 -2.11911295e-02\n",
            "  3.06742098e-02 -2.72584576e-02  1.02539599e-01  8.36224388e-03\n",
            " -4.80009522e-03  3.94229777e-04 -6.07766956e-03 -1.29923504e-03\n",
            " -1.63482036e-02 -1.53789967e-01  1.25964852e-02 -2.95771454e-02\n",
            " -3.66962776e-02  1.76792406e-02 -2.53232643e-02 -3.46410125e-02\n",
            "  1.21189198e-02  3.01334336e-02  7.87534285e-03  5.23794675e-03\n",
            " -5.84762469e-02 -3.46547849e-02  8.30722079e-02 -2.16836482e-02\n",
            "  1.40494360e-02 -5.26583828e-02  5.11604222e-03  1.52580431e-02\n",
            " -6.27521798e-03  4.45209891e-02 -2.61642002e-02 -2.97105592e-03\n",
            "  1.70938612e-03 -6.38015103e-03  1.65719129e-02 -8.31829384e-05\n",
            "  2.27177050e-03  1.71152353e-02 -7.64360465e-03 -1.60017926e-02\n",
            "  6.67003263e-03  1.17936218e-02 -1.10707339e-02  1.20122563e-02\n",
            " -2.19910238e-02  4.10652859e-03  2.61789039e-02 -6.46290462e-03\n",
            "  1.23674329e-03 -1.35586597e-02 -1.39127299e-02 -2.88833492e-02\n",
            "  1.49183758e-02  1.33171119e-02  4.05948088e-02  6.23204336e-02\n",
            "  1.90333948e-02  2.93832412e-03  1.75772812e-02 -4.19464335e-03\n",
            "  2.05601435e-02  2.75592562e-02 -5.35338596e-02  3.65218520e-02\n",
            "  2.19778866e-02  3.49607063e-03  1.63058899e-02  4.71259691e-02\n",
            " -2.18583066e-02  1.74245890e-02  1.92251764e-02  1.31567344e-02\n",
            "  3.16035771e-03  2.80497856e-02 -5.54816127e-02 -2.52001304e-02\n",
            " -2.69775353e-02  8.64748005e-03 -5.70469722e-03 -9.22503695e-03\n",
            " -4.89596464e-03 -1.94041543e-02  1.27720153e-02 -3.79760340e-02\n",
            " -1.06635951e-02  4.67816032e-02 -6.73211217e-02 -3.05035561e-02\n",
            "  1.05146337e-02  2.07327958e-02  8.68056435e-03 -1.78408287e-02\n",
            " -3.26153673e-02  7.09105702e-03 -3.07009853e-02  4.40952294e-02\n",
            " -2.16500033e-02  3.25094233e-03 -1.33772735e-02 -5.08058146e-02\n",
            "  4.09779586e-02 -6.67609414e-03 -3.60158980e-02 -4.47332254e-03\n",
            " -2.98297517e-02  1.53629296e-03 -7.30925333e-03 -6.53672591e-03\n",
            "  1.00895859e-01  3.69672924e-02  6.56269118e-03 -4.83240634e-02\n",
            " -2.24061813e-02 -1.51473060e-02  2.09649839e-03 -9.11899004e-03\n",
            " -8.38393066e-03 -7.67656416e-03  1.87717751e-03 -2.12671794e-02\n",
            "  4.19827625e-02  3.29993330e-02  1.96827445e-02  1.73656270e-03\n",
            "  1.82387885e-03  4.34918422e-03 -6.28837757e-03  1.72184454e-03\n",
            "  1.11080045e-02 -6.12126337e-03  3.92304454e-03 -3.51150217e-03\n",
            "  1.85403451e-02 -3.04833222e-02 -2.50341371e-03  3.34051698e-02\n",
            " -1.94028411e-02  2.94985715e-02  1.79303363e-02 -5.43965539e-03\n",
            " -5.25603220e-02  9.10280040e-04 -3.14906612e-03  1.37741417e-02\n",
            " -9.97223556e-02  4.73156795e-02 -8.44630972e-02 -1.42087610e-02\n",
            " -1.02383438e-02 -1.47903832e-02 -1.44903120e-02 -2.12724693e-03\n",
            " -1.23453904e-02  4.96756611e-03  1.42427161e-03 -2.69564986e-03\n",
            "  2.91954912e-03  3.39991190e-02 -2.71268394e-02  9.05468361e-04\n",
            "  8.66370089e-03 -1.92380697e-02  2.52804402e-02  3.53033170e-02\n",
            "  1.32583231e-02 -4.52693328e-02 -1.22872368e-03  1.01151988e-02] \n",
            " \n",
            "\n",
            "The vector of word temu is [-0.04862673 -0.03412209  0.09974094  0.06240776 -0.116721   -0.02298027\n",
            " -0.02288393 -0.12611292 -0.02787279 -0.17009239 -0.04192586 -0.06389022\n",
            "  0.1341554  -0.01690672  0.09269127  0.14127691 -0.0453638  -0.0220063\n",
            " -0.05269102  0.04715368  0.02343494  0.01572742 -0.02742948  0.09992367\n",
            "  0.11131787  0.0115491   0.07856207  0.06578089  0.16111144  0.08419537\n",
            "  0.07477932 -0.03075841 -0.14084615 -0.01169597  0.01780483  0.01568608\n",
            "  0.05014534 -0.19521688 -0.02630812 -0.03324503  0.17054996 -0.06884331\n",
            " -0.11817396 -0.16389874 -0.02417129 -0.00881417  0.0086541  -0.19511542\n",
            "  0.07481717  0.00630216  0.05912101 -0.13208802  0.04027287 -0.03205685\n",
            "  0.05593482  0.1754341   0.02859186 -0.04677534 -0.07784924  0.04301054\n",
            " -0.01232558  0.00572416 -0.09673743  0.00106457 -0.13658905 -0.09171145\n",
            "  0.07995716 -0.14078544  0.0456952   0.01152154  0.11567625 -0.07329649\n",
            " -0.0615656  -0.00757031 -0.09645224 -0.10057979 -0.08038003 -0.09073927\n",
            "  0.05194713  0.11857697 -0.05711323  0.01104353  0.03258426 -0.10319667\n",
            " -0.00308625  0.06577788  0.00812571 -0.12755498  0.19537398  0.11959928\n",
            "  0.01079509  0.03020366 -0.0708083  -0.03290088  0.00494508 -0.1846901\n",
            " -0.01006815 -0.14757833  0.10556912 -0.0686636   0.05042394 -0.03578908\n",
            " -0.1975142  -0.09773334  0.09609947  0.06170825 -0.09300825 -0.1322905\n",
            "  0.06262556  0.0290818   0.05399423 -0.10824341  0.10914065 -0.08004464\n",
            " -0.15353891 -0.10736035 -0.10703482  0.1721438   0.00616485  0.12145783\n",
            " -0.10615033 -0.06201116 -0.11341231 -0.01772939 -0.0543422   0.01080068\n",
            " -0.04903822  0.0745675  -0.08013822  0.05728453  0.04255188  0.01119398\n",
            "  0.13161078 -0.14254555  0.12054559 -0.07079697  0.06605472 -0.06219774\n",
            "  0.04991585 -0.10871065  0.04019733 -0.2870285  -0.08696437  0.00836611\n",
            "  0.06716835  0.01262686  0.062509    0.07590024 -0.00419716  0.12157474\n",
            "  0.10404551  0.08730446 -0.06794292 -0.03407725  0.24646154  0.04391686\n",
            "  0.0183728  -0.16986471  0.00216083  0.07120074  0.14182743 -0.11738169\n",
            "  0.03716698 -0.08276761 -0.04336557  0.03244393 -0.0752175   0.13839373\n",
            "  0.11829153  0.04062495  0.13128883 -0.09685139  0.02774229 -0.11708152\n",
            " -0.02017044 -0.08278391 -0.0057109   0.07375753  0.22023563 -0.01764637\n",
            "  0.29410672  0.0421864  -0.142235    0.09354696 -0.14458191  0.0475838\n",
            "  0.00117074  0.1151865  -0.14089635  0.12025725  0.08537479  0.13167265\n",
            "  0.07010148 -0.16337118  0.06118895 -0.19066912  0.00448663  0.10478234\n",
            " -0.05842298  0.09716837  0.043997    0.02636394 -0.01496505 -0.17542163\n",
            "  0.02194241  0.05447955  0.13920668  0.00910781  0.13300748  0.02477603\n",
            " -0.15866831 -0.09059516 -0.06868523 -0.03905744  0.07506014  0.07537006\n",
            " -0.0720265   0.07018784  0.0622456   0.09804585 -0.13622189  0.02824058\n",
            "  0.09491909  0.10685083  0.23401111 -0.01991025 -0.1601315   0.00257803\n",
            "  0.25808161 -0.09678789 -0.07429745  0.0465859  -0.06716707 -0.117368\n",
            "  0.01689665 -0.23256908  0.00074236  0.09785564 -0.06358426 -0.04358635\n",
            "  0.010059   -0.18766493  0.00512417 -0.00242746 -0.04407062 -0.03020306\n",
            " -0.07969221  0.08307558 -0.02814801 -0.23210421 -0.09556586  0.01784255\n",
            " -0.00177908  0.0151278   0.18396339  0.11514737  0.04551762 -0.0689637\n",
            " -0.03019903  0.10465322  0.07598852  0.18327956  0.13222247 -0.01697814\n",
            " -0.04745065 -0.00339195 -0.09466805  0.018488   -0.02749367 -0.06098725\n",
            "  0.0067373  -0.04552158  0.00250247 -0.0733943  -0.09307465  0.03652628\n",
            " -0.16709971 -0.05573593 -0.0131142  -0.07308348 -0.03952163  0.06836146\n",
            "  0.13499942 -0.10562409 -0.14828351 -0.06447525 -0.12921372  0.02667682\n",
            " -0.01181686 -0.0423625  -0.13696232 -0.09663691  0.08784678 -0.09168243\n",
            " -0.06511576 -0.02176285  0.06291888 -0.12948819 -0.07447568 -0.12528273] \n",
            " \n",
            "\n",
            "The vector of word sdh is [ 0.13917917 -0.12635854  0.09590576  0.2936474  -0.07431784 -0.0332685\n",
            " -0.08870971  0.07364219 -0.01792423 -0.19889054  0.01696951  0.03227964\n",
            "  0.07385999 -0.12993002  0.06225927 -0.06159255 -0.06204823  0.17466645\n",
            "  0.0813311  -0.02527141  0.04625968 -0.08302819  0.31203374 -0.08162868\n",
            "  0.0168236  -0.07926629 -0.17609079 -0.11622527  0.19998585  0.0032558\n",
            "  0.21192837  0.03694882 -0.20933087  0.13446812  0.08693119 -0.03638855\n",
            "  0.06370875  0.07189624 -0.00538509  0.05327965 -0.09245789  0.01078765\n",
            "  0.06763618  0.15946364 -0.08811802  0.18837065  0.18035503  0.00755651\n",
            "  0.00458333 -0.00195884 -0.12736595 -0.14024948  0.00441454 -0.11600141\n",
            "  0.07900616  0.0400507   0.07819925  0.09644971 -0.09324766 -0.08725262\n",
            " -0.16134745 -0.11152525  0.00608488 -0.02547057  0.00517109  0.02764957\n",
            " -0.30839878  0.103387    0.1564606   0.04934282  0.06662881 -0.21954405\n",
            " -0.02817675  0.08033535 -0.05747442 -0.04074368 -0.02179559 -0.22308281\n",
            "  0.07838221  0.02001666  0.01848096 -0.18334627  0.29683733 -0.09243529\n",
            " -0.01738374  0.02860017 -0.20226474 -0.06281216  0.06535779 -0.01816337\n",
            "  0.06264447 -0.10959519 -0.02501654 -0.06612255 -0.18905888 -0.3454153\n",
            "  0.04138286  0.00656441  0.13546905  0.07910755 -0.15362069  0.07896009\n",
            "  0.08174324  0.18068689 -0.0226455  -0.01691009  0.04868783 -0.06544061\n",
            "  0.00830714  0.09367129  0.00893772  0.04648533  0.05492242 -0.03520665\n",
            "  0.2103895   0.13811363  0.02698337 -0.10745855 -0.10267083 -0.05494506\n",
            " -0.13404974 -0.08967559 -0.04674571 -0.17483734  0.0560117  -0.12134749\n",
            "  0.10470393  0.25307134  0.04650194 -0.03940563  0.15992215  0.08775705\n",
            " -0.03613042 -0.05520294  0.44414124 -0.015698    0.04284361 -0.04676725\n",
            "  0.19955839  0.079304    0.04853164 -0.3610177   0.16532645  0.02326435\n",
            " -0.22668377  0.07896452  0.11485375 -0.1883716   0.01081515  0.13900003\n",
            " -0.003928    0.01135841 -0.02921522 -0.00130649  0.23136656 -0.04248387\n",
            " -0.02028967 -0.00708762  0.07377593  0.07043879  0.40320235  0.3245228\n",
            "  0.05385158  0.0660052  -0.05741629  0.07783984  0.21247925  0.12426744\n",
            " -0.09307112 -0.050552    0.08503473  0.03142017 -0.10368593 -0.06168018\n",
            " -0.28494596  0.06346981  0.20350161  0.01079737 -0.0362878  -0.16427669\n",
            "  0.08231777  0.03668331  0.02802407 -0.17658734 -0.00472034  0.07664666\n",
            "  0.13209794 -0.00857962  0.24983418 -0.22315502  0.14054422 -0.01821563\n",
            "  0.13596147  0.0660558  -0.03125791 -0.10252448 -0.07890807 -0.09131995\n",
            " -0.05977643  0.08227415  0.02212758  0.03363888  0.13281262  0.01864798\n",
            "  0.31857267 -0.04727941  0.03543867 -0.12637319  0.07414374 -0.16107816\n",
            " -0.2086019  -0.16012357 -0.01758823  0.14041205 -0.07314643 -0.07816148\n",
            " -0.03877013 -0.00198618 -0.11002047 -0.01094483  0.22521505 -0.08350666\n",
            "  0.00254716  0.05448594 -0.0101776  -0.19699275  0.01636848  0.07383793\n",
            " -0.01363732 -0.05513531 -0.01844698  0.07085378  0.01131631 -0.03119454\n",
            " -0.05343636 -0.00796653 -0.3303737   0.12285649 -0.17426872 -0.07042865\n",
            " -0.08296472  0.05787087 -0.14067695 -0.10856545 -0.15832597 -0.13276364\n",
            "  0.11171314  0.09475999 -0.11332764 -0.04209441 -0.02363089 -0.04544669\n",
            "  0.1555535  -0.16025189 -0.03052589  0.16438478 -0.17008035 -0.02621349\n",
            " -0.01292892 -0.08879031  0.22740214  0.1887276   0.04485822 -0.09926896\n",
            " -0.04408339  0.16037479 -0.09504908  0.05155881 -0.07623493 -0.08381566\n",
            " -0.07825025  0.07048204 -0.19974938 -0.15179601  0.06919545 -0.02017533\n",
            "  0.15554386 -0.04634856  0.10034575  0.06403609 -0.11382455 -0.05993538\n",
            " -0.01516418 -0.08199589  0.13695428 -0.00365921 -0.01967836 -0.00232426\n",
            " -0.03432764 -0.08708078  0.12709683  0.00978946 -0.15696605  0.09424888\n",
            "  0.04848754 -0.04777501  0.06020718 -0.07540011 -0.16871092 -0.00102033] \n",
            " \n",
            "\n",
            "The vector of word bicara is [ 1.07975001e-03 -6.99702874e-02  1.80430356e-02  3.86692099e-02\n",
            "  3.47948559e-02  9.31696221e-02 -2.38534805e-04 -3.11219841e-02\n",
            " -1.41669214e-02 -8.76052603e-02  1.14737656e-02  1.10089337e-03\n",
            "  6.67981664e-03  4.67749797e-02 -3.59092131e-02 -5.95161691e-06\n",
            "  1.11785389e-01 -1.68067962e-02 -2.63607837e-02  1.67501587e-02\n",
            " -3.78998183e-02 -4.68004495e-02  4.96309660e-02  1.00297993e-02\n",
            "  6.27374202e-02  1.37134288e-02  5.01986705e-02  2.01538634e-02\n",
            " -2.71927174e-02 -1.19551234e-01  1.87528860e-02 -3.68605405e-02\n",
            " -3.29041108e-02  8.37243348e-03  6.02970831e-02 -1.08563080e-02\n",
            "  7.10472986e-02  8.13622307e-03  2.49697175e-02 -3.21567357e-02\n",
            " -1.90068260e-02 -2.07785666e-02 -3.47861461e-02 -3.01453890e-03\n",
            " -3.16044725e-02 -5.76904556e-03 -4.09099180e-03 -5.53266183e-02\n",
            " -2.42282301e-02 -5.97117655e-02 -5.56083210e-02  2.13617273e-02\n",
            "  1.02191421e-04 -6.79798871e-02  4.02254192e-03  1.58807654e-02\n",
            " -3.99677716e-02 -1.45649165e-02 -1.08518945e-02  1.70646589e-02\n",
            " -1.61930528e-02 -5.03414171e-03 -6.45667836e-02  4.84272353e-02\n",
            "  4.92223836e-02 -2.99734604e-02 -3.88088338e-02 -2.74232030e-02\n",
            "  2.92905215e-02 -2.77512968e-02  3.95641923e-02 -3.00750416e-02\n",
            "  4.24400270e-02 -8.06577038e-03 -9.25335754e-03 -2.48060245e-02\n",
            "  6.84674177e-03 -5.50040305e-02  2.40656827e-02  4.68733199e-02\n",
            "  4.56181169e-02 -2.70563271e-02 -4.80081700e-03  8.10389407e-03\n",
            " -8.83996580e-03  1.15031740e-02 -3.16794105e-02  7.49991834e-02\n",
            "  1.19353309e-02 -2.91568297e-03  1.79410621e-03  3.57548743e-02\n",
            " -1.38345091e-02 -7.14655453e-03 -5.80466501e-02 -1.79294392e-01\n",
            " -2.30807345e-02 -1.27035771e-02  5.20506017e-02  2.55268812e-02\n",
            "  1.77175719e-02  2.77634859e-02 -5.23759127e-02  2.46781614e-02\n",
            "  3.13937478e-02  2.10127812e-02 -1.47041157e-02 -1.18820444e-01\n",
            "  6.40737265e-02 -1.00809131e-02 -1.02246599e-02 -2.08180100e-02\n",
            " -2.12471467e-02  1.10845594e-02 -5.75287044e-02  5.04888408e-02\n",
            " -4.96325083e-02  2.63856346e-04  8.96784663e-02  5.62653132e-03\n",
            " -8.28271285e-02  8.76531824e-02 -5.42192273e-02  3.68781462e-02\n",
            "  1.15802912e-02 -4.55386080e-02 -3.97800142e-03  5.96221117e-03\n",
            " -2.46628709e-02  4.83989194e-02  3.06333154e-02  1.70636307e-02\n",
            "  3.79587151e-02  8.75861291e-03  1.17636718e-01  2.79343855e-02\n",
            " -8.72372091e-02  4.07679416e-02  4.28832173e-02 -1.87883023e-02\n",
            "  6.29444048e-03 -5.59614897e-02  5.83717301e-02 -1.03620468e-02\n",
            " -2.83227470e-02 -6.64833710e-02  5.75012863e-02  1.79168547e-03\n",
            " -6.51357174e-02  3.74898314e-03  1.02189079e-01  1.82910543e-02\n",
            " -5.52850142e-02  1.10507026e-01  1.96079407e-02  3.01365461e-02\n",
            "  3.55225392e-02 -2.95432657e-03  5.71221374e-02 -3.20280753e-02\n",
            "  2.22805273e-02  2.71497853e-02 -4.84997816e-02 -1.45613253e-02\n",
            " -7.86178559e-03 -5.64950816e-02  9.12702903e-02  2.72558127e-02\n",
            " -1.49921449e-02  1.84666030e-02  4.43058610e-02  3.68913710e-02\n",
            "  1.43106908e-01  2.53374223e-02 -3.79427783e-02 -4.31229360e-02\n",
            " -1.13383578e-02  6.33576885e-02  2.16160603e-02  9.03858989e-03\n",
            " -5.53610921e-02  1.78687647e-02 -1.26979798e-02 -1.49643598e-02\n",
            " -1.00684073e-02  2.13543940e-02  4.84331213e-02  3.78403673e-03\n",
            " -1.17644584e-02 -4.96724285e-02 -5.53018833e-03  5.18764630e-02\n",
            "  2.60113226e-03 -2.30028871e-02 -5.21071069e-02  3.07249911e-02\n",
            " -2.69777235e-02  4.15199697e-02  2.04651896e-02 -3.44251096e-03\n",
            " -3.74019556e-02  3.99886966e-02  4.28520143e-02 -2.80206650e-02\n",
            "  6.21560626e-02 -2.95988400e-03  2.10457575e-02  3.75148989e-02\n",
            "  2.34553274e-02  2.68889870e-02 -3.19572352e-02 -2.17493828e-02\n",
            "  4.91287671e-02  6.44169971e-02  2.36826446e-02  2.68469397e-02\n",
            "  3.78910941e-03  3.03704590e-02 -8.02253261e-02  6.57748729e-02\n",
            "  2.43865754e-02  1.24402586e-02 -8.88697337e-03  3.49062472e-03\n",
            "  8.65476858e-03  8.83212499e-03  5.48944175e-02 -1.42842429e-02\n",
            "  3.60037796e-02 -1.12503348e-02 -1.70262977e-02 -8.18367824e-02\n",
            "  1.92134455e-03 -4.78909630e-03  2.77294647e-02 -4.48652580e-02\n",
            "  1.27612818e-02 -5.58882169e-02 -7.84353167e-03  2.39240136e-02\n",
            " -3.00045013e-02 -9.49821714e-03 -1.67453326e-02 -7.23421499e-02\n",
            "  4.06648666e-02  4.41167615e-02  3.66414897e-02  3.06363162e-02\n",
            "  4.07031178e-02  1.70953255e-02 -2.27068700e-02 -2.78889835e-02\n",
            "  7.51371309e-02 -7.63558373e-02  7.62135088e-02  2.09082011e-02\n",
            " -1.68940965e-02 -2.71211918e-02  5.89638613e-02 -1.97385065e-02\n",
            " -1.09948674e-02  4.10857871e-02 -7.96955675e-02 -9.04332027e-02\n",
            " -4.47823778e-02 -1.19234875e-01  4.69230972e-02  2.31892057e-02\n",
            " -1.00104753e-02  3.66513282e-02 -8.36625602e-03  5.16626704e-03\n",
            " -2.57430220e-04 -1.24272658e-02 -1.42881274e-03  5.29169217e-02\n",
            " -5.79212084e-02  1.21812662e-02  9.87468101e-03 -6.03595488e-02\n",
            " -4.86973152e-02 -4.48985351e-03  3.51972878e-03  3.17628458e-02\n",
            "  5.08045219e-02 -8.62162933e-03 -2.92242952e-02  4.91400100e-02\n",
            "  3.20782810e-02  1.76699590e-02 -4.02211361e-02  4.04178910e-02\n",
            "  1.05893090e-01  2.55098734e-02  9.87630785e-02  7.66421407e-02\n",
            " -5.29735088e-02 -9.26172733e-03 -4.19836305e-02  3.58479097e-02] \n",
            " \n",
            "\n",
            "The vector of word baik2 is [ 6.28738552e-02  3.27656306e-02 -6.08511679e-02 -1.37076750e-02\n",
            " -8.90598148e-02 -4.19939235e-02  1.89712681e-02  3.14134769e-02\n",
            " -5.80009120e-03 -1.75292879e-01 -2.60558017e-02  2.07597166e-02\n",
            " -2.72802841e-02 -6.75537139e-02  1.10083304e-01  1.35054449e-02\n",
            "  1.13061421e-01  2.89545525e-02  5.70159256e-02  3.95869426e-02\n",
            " -8.73260200e-02 -3.12061273e-02 -2.14189738e-02  2.70086396e-02\n",
            " -3.29528339e-02  1.54270921e-02  6.38753474e-02 -4.60285284e-02\n",
            "  7.30642974e-02 -9.19836238e-02  5.89045919e-02  3.54677513e-02\n",
            " -3.81693989e-02 -6.35518357e-02 -1.56773217e-02  1.25116445e-02\n",
            "  4.88852486e-02  1.25087216e-01  3.70117370e-04  7.36159179e-03\n",
            "  3.04623488e-02 -4.95058745e-02 -4.79882993e-02  5.51151969e-02\n",
            "  4.64189127e-02  1.59826532e-01  5.78226782e-02 -4.32258844e-02\n",
            " -9.07089934e-03 -6.60270527e-02  9.80633125e-02 -7.68548250e-02\n",
            "  6.73187710e-03  8.50244462e-02 -7.30675384e-02  1.75681934e-02\n",
            " -2.33838856e-02  3.34385000e-02 -1.19728975e-01 -7.26534612e-03\n",
            " -3.33186164e-02  7.06969053e-02  1.65146943e-02 -1.64239202e-02\n",
            "  8.00970793e-02 -1.49023104e-02  4.76522138e-03  3.50783579e-02\n",
            " -1.04477722e-03 -9.43758562e-02 -6.02831803e-02 -1.03860587e-01\n",
            "  1.31599575e-01  4.99414001e-03 -9.93598998e-03  3.71707417e-03\n",
            "  1.27419718e-02 -1.06242202e-01  8.01802985e-03 -4.77392646e-03\n",
            "  3.85927334e-02 -1.97592765e-01  1.52455121e-01 -2.80509591e-02\n",
            " -2.73813251e-02  4.33555152e-03 -9.14878398e-02 -6.13911040e-02\n",
            " -4.17654961e-02 -8.44116360e-02  2.06474289e-02  1.04114555e-01\n",
            " -1.72017738e-02 -3.87696847e-02 -1.39925167e-01 -1.53404862e-01\n",
            " -7.05402717e-02  1.73751079e-02  6.59147650e-02  6.10592961e-02\n",
            " -3.58881503e-02  2.32714191e-02  2.79221945e-02 -6.79466352e-02\n",
            " -9.98530388e-02 -2.06478946e-02 -4.54833508e-02 -3.90481837e-02\n",
            "  4.29327711e-02 -5.08808941e-02  8.77791941e-02 -2.57507619e-03\n",
            "  4.02452126e-02 -1.02814667e-01  1.26611903e-01 -3.64208631e-02\n",
            " -1.06938314e-02  1.39394123e-03 -2.54233442e-02  1.15494132e-01\n",
            " -1.29280239e-01  1.02592371e-01  2.19875574e-02 -4.05408144e-02\n",
            "  8.71001258e-02  4.70765168e-03  1.14549376e-01  5.55580892e-02\n",
            "  4.75944299e-03 -3.23866084e-02  5.98570071e-02  2.14149691e-02\n",
            "  1.04247406e-03  3.19485776e-02  2.42125355e-02  5.73989041e-02\n",
            " -4.65873629e-02 -2.34452672e-02  3.21516395e-02  1.12768635e-03\n",
            "  2.54613869e-02 -1.47352070e-01  6.33389875e-02 -1.00401774e-01\n",
            "  2.79564969e-02  6.68513263e-03 -4.69754227e-02 -1.01995654e-01\n",
            "  1.35486163e-02 -1.33475410e-02 -2.61214841e-02  4.64577693e-04\n",
            "  2.76001915e-02  3.97345200e-02  8.45007598e-03 -3.04016806e-02\n",
            " -2.57321307e-03 -3.66936848e-02  7.34390318e-02  3.07702720e-02\n",
            "  1.88304894e-02  9.66175646e-02 -1.01099703e-02  4.03926782e-02\n",
            " -9.63056684e-02 -1.11137770e-01  1.03441700e-01  1.13389745e-01\n",
            "  8.08317661e-02 -3.91249843e-02 -2.79041529e-02 -1.49497334e-02\n",
            " -1.37609132e-02 -4.65497039e-02 -6.10153489e-02  1.01348661e-01\n",
            "  4.79832664e-02  8.27373005e-03  2.21709721e-02 -8.63629282e-02\n",
            " -1.07597865e-01  1.27646914e-02 -6.95621129e-03 -5.51605560e-02\n",
            "  1.04862884e-01  6.65723532e-02  1.88313514e-01 -2.06868090e-02\n",
            "  8.59478489e-02 -2.15870161e-02 -1.25366643e-01  2.81559974e-01\n",
            "  7.02792555e-02  9.24704894e-02 -6.26642779e-02 -1.63255576e-02\n",
            " -1.03400148e-01 -3.68346199e-02 -1.27922505e-01  5.57099245e-02\n",
            " -2.08076835e-03 -5.33545166e-02 -3.01923472e-02  3.40820253e-02\n",
            "  8.32967162e-02  1.82293747e-02  1.38734028e-01 -4.55917232e-02\n",
            "  5.70203625e-02  5.55895753e-02  6.55965656e-02 -2.02580914e-02\n",
            "  1.18246093e-01  1.17611270e-02  1.41168600e-02 -6.09870479e-02\n",
            "  2.52983645e-02 -5.71102910e-02 -2.72116698e-02 -9.07781869e-02\n",
            "  7.60985091e-02  6.20846031e-03 -6.22727498e-02  3.59253213e-02\n",
            "  3.98440938e-03 -4.27800566e-02 -1.16751187e-01 -6.08912073e-02\n",
            " -2.78541446e-03  6.60236403e-02 -3.40447240e-02  1.04096845e-01\n",
            " -2.82738023e-02 -5.27407564e-02 -5.07432595e-02 -4.44386154e-02\n",
            "  7.05099478e-03 -5.24078086e-02  4.01451886e-02  4.58304472e-02\n",
            " -4.51962650e-02 -7.56604820e-02  4.88421060e-02 -5.23944758e-03\n",
            " -9.36576426e-02 -3.06143109e-02  9.52610672e-02 -3.55890766e-02\n",
            "  1.28530478e-03 -1.26312554e-01  7.25003928e-02 -4.01712209e-02\n",
            "  6.36906996e-02 -4.21247482e-02  1.62447132e-02  1.45247206e-04\n",
            " -1.39588326e-01 -1.92628521e-02 -1.61806308e-03 -6.02045618e-02\n",
            "  1.80068552e-01  1.04466222e-01 -2.23708786e-02  7.82738551e-02\n",
            "  4.88374904e-02 -4.44868393e-03  4.71234135e-02 -7.18431361e-03\n",
            " -7.98240378e-02  1.14795696e-02 -6.70626760e-05 -6.39435947e-02\n",
            " -8.61667097e-02 -5.39685003e-02 -5.28230518e-02  7.26682097e-02\n",
            " -3.10831368e-02  3.81457955e-02 -1.05002224e-01  4.15597297e-02\n",
            " -1.30423874e-01 -1.40033895e-02  8.26892257e-02 -1.74293250e-01\n",
            "  5.17841689e-02 -2.15063080e-01  2.59373449e-02 -6.82732165e-02\n",
            "  2.00878512e-02  3.12132221e-02  4.25035432e-02 -2.95514963e-03\n",
            "  1.51003562e-02 -2.79246420e-02  1.66576616e-02  7.13618007e-04\n",
            "  1.02147385e-01  3.22644738e-03 -8.52652937e-02  1.37489617e-01] \n",
            " \n",
            "\n",
            "The vector of word dgn is [ 9.81520489e-02 -1.81131601e-01 -1.18355207e-01 -7.94546157e-02\n",
            " -2.42968649e-02 -2.02162698e-01 -5.11953980e-02 -5.86915202e-02\n",
            "  7.18966722e-02 -2.22167313e-01 -5.21685183e-03 -6.62737340e-02\n",
            "  7.61631504e-02  2.57392731e-02  1.98992252e-01 -1.54766086e-02\n",
            " -8.87330174e-02  8.64200741e-02 -3.06978934e-02  1.44685477e-01\n",
            " -2.95920730e-01 -1.62740529e-01  3.70415952e-03 -1.90561399e-01\n",
            "  8.89832154e-02 -3.91705893e-02 -1.80457741e-01 -1.09441578e-01\n",
            "  6.18589073e-02 -1.89201767e-03  1.32325858e-01  5.51917106e-02\n",
            " -1.13578886e-01  4.72953320e-02  2.69710064e-01 -1.22609928e-01\n",
            "  6.18541725e-02  1.11068018e-01  1.18574485e-01  5.39985560e-02\n",
            " -7.34299719e-02 -2.23415807e-01  2.20081322e-02  8.93699974e-02\n",
            " -7.06240982e-02  6.50310814e-02  2.50730366e-01  7.40355626e-03\n",
            "  5.48445135e-02 -7.45804459e-02 -8.56689885e-02  2.87663583e-02\n",
            " -3.98171842e-02  1.29323721e-01 -3.98137905e-02 -1.36596486e-01\n",
            " -8.24466795e-02 -6.46696612e-03 -1.67765200e-01  5.52755371e-02\n",
            " -9.01889354e-02  3.00757438e-02  1.90201215e-02 -7.56129920e-02\n",
            "  5.16558904e-03 -2.00453475e-02 -5.95554449e-02  7.59413466e-03\n",
            "  7.48116672e-02 -5.05229533e-02  4.68633696e-02 -1.65796161e-01\n",
            "  1.42679196e-02 -1.16630755e-02 -1.79106202e-02 -6.83582053e-02\n",
            " -8.32720846e-02 -1.84039652e-01  2.36649010e-02  6.45616204e-02\n",
            "  4.57532257e-02 -2.25427240e-01  2.22385138e-01  6.26988709e-03\n",
            " -8.94057751e-03  4.31892276e-02  9.62582976e-02 -2.69957542e-01\n",
            "  1.50111631e-01  3.59409451e-02  1.81332558e-01 -9.77439433e-02\n",
            " -4.65946533e-02 -1.12050578e-01 -1.51968285e-01 -2.50169098e-01\n",
            "  2.04432700e-02  1.16171964e-01  3.78051810e-02  1.75455660e-01\n",
            " -1.44168362e-01  1.24179944e-01  8.28535631e-02 -6.27805525e-03\n",
            "  2.82428078e-02 -5.59379682e-02 -8.42382908e-02 -6.84302673e-03\n",
            "  5.33964261e-02  5.77551052e-02 -1.44782126e-01 -1.60339341e-01\n",
            "  2.28697080e-02 -1.30610362e-01  2.34196872e-01  1.60806492e-01\n",
            "  2.97660027e-02 -6.87220767e-02 -8.23250599e-03 -4.72543761e-02\n",
            " -9.49480683e-02 -8.32791403e-02 -8.24997947e-02 -9.22470912e-02\n",
            "  1.04431935e-01  1.68512136e-01  2.97830403e-02  2.35428065e-01\n",
            "  1.59980178e-01 -7.15356171e-02  1.43121690e-01  1.97669670e-01\n",
            " -1.67899691e-02 -3.14421207e-02  8.18006992e-02 -1.52872846e-01\n",
            " -3.77332419e-02  3.84098999e-02  7.42524397e-03  1.68771565e-01\n",
            " -2.87024602e-02 -3.86536419e-02  2.02719033e-01  5.72683029e-02\n",
            " -9.70558748e-02 -9.15313363e-02  1.84769616e-01 -1.30609334e-01\n",
            "  5.36837429e-03  1.27489403e-01 -1.15492977e-02  2.90857963e-02\n",
            " -1.38521686e-01 -1.46543309e-02  6.02743387e-01  8.57345536e-02\n",
            "  1.25100598e-01  5.86558059e-02  7.70044848e-02 -4.49154526e-02\n",
            " -3.81633267e-02 -3.61677408e-02 -1.22656487e-01  1.39630511e-01\n",
            " -9.64080542e-02  1.07332453e-01  1.63491577e-01  3.64457928e-02\n",
            "  4.17988747e-03 -5.94919473e-02  4.96141613e-02  1.00781806e-01\n",
            "  6.13005310e-02 -9.57455114e-02 -2.37363070e-01  8.37887675e-02\n",
            "  1.54470801e-01  3.10793519e-04  6.32855892e-02 -1.09958239e-01\n",
            " -1.12588387e-02  1.47230700e-01 -1.29171219e-02 -7.60194063e-02\n",
            " -9.41776186e-02  6.84642326e-03  2.40006641e-01 -3.33737582e-02\n",
            "  1.85194314e-01 -1.22387320e-01 -1.23048909e-02  9.67354178e-02\n",
            "  1.03422567e-01  1.61365718e-01 -3.11723948e-01  5.23413196e-02\n",
            " -1.34869754e-01 -1.81695819e-03 -1.02150135e-01  2.08666325e-01\n",
            " -1.53700039e-02  4.00548689e-02  1.26082242e-01 -5.78615814e-02\n",
            "  1.10305987e-01 -1.33649886e-01  4.79768813e-02 -4.96867858e-03\n",
            "  1.46279514e-01 -1.65246233e-01 -1.03709415e-01  3.97733301e-02\n",
            "  6.15600869e-02  8.05950314e-02 -1.08442426e-01  9.22955573e-02\n",
            " -4.66083549e-02 -1.31193072e-01  6.05147965e-02 -9.36101824e-02\n",
            "  1.47842199e-01 -1.66296065e-02  3.18986513e-02  1.34834498e-02\n",
            " -4.44706827e-02 -3.90389673e-02 -3.75290103e-02  5.44727594e-02\n",
            "  7.01460242e-02 -1.01363465e-01  2.69319825e-02  7.09828734e-03\n",
            " -3.88295799e-02  2.63058441e-03 -1.12291127e-02 -1.03506401e-01\n",
            " -2.40166560e-02  2.29752231e-02 -1.16597392e-01 -5.76430745e-03\n",
            " -6.88559338e-02  2.73193438e-02 -3.38844284e-02 -5.69357425e-02\n",
            " -1.00576997e-01 -5.71117178e-02  7.83455223e-02  9.12617147e-02\n",
            " -6.29283488e-02  2.03680433e-02  4.64057662e-02  8.54218751e-02\n",
            "  1.01500139e-01 -7.59271011e-02  1.89248770e-02  1.23086289e-01\n",
            " -5.10220006e-02  3.53128947e-02  1.31342001e-02 -6.47290498e-02\n",
            "  1.67935193e-01  9.24064070e-02 -1.03133358e-01 -1.88343022e-02\n",
            " -1.73759907e-01  1.45408630e-01 -3.22880149e-02 -9.81825590e-02\n",
            " -6.32793158e-02 -1.80745751e-01 -1.29733816e-01 -6.97181374e-03\n",
            " -1.90712988e-01 -2.00958088e-01 -2.70435035e-01 -4.62258607e-03\n",
            " -2.26246491e-01  2.92111337e-02 -1.57722905e-01  1.05096571e-01\n",
            " -5.22224046e-02 -1.53111920e-01  1.17639273e-01 -1.89560026e-01\n",
            " -2.96286456e-02  4.87936251e-02  5.33795804e-02 -2.87339091e-04\n",
            " -2.18901746e-02 -8.99694562e-02 -2.37688012e-02  9.14957225e-02\n",
            " -7.56682009e-02  6.94277510e-02  1.68244556e-01 -9.76089910e-02\n",
            "  8.27032775e-02 -2.24954724e-01 -4.27464470e-02 -2.14621127e-02] \n",
            " \n",
            "\n",
            "The vector of word mantan is [ 1.39062917e-02 -7.75891182e-04  3.72574031e-02  5.57074919e-02\n",
            "  5.32620326e-02 -1.69926956e-01  8.32788646e-03 -3.87655720e-02\n",
            " -1.96273271e-02 -1.21374883e-01 -1.09724570e-02 -5.53935673e-03\n",
            " -1.64449196e-02  4.03518453e-02  4.04390804e-02 -2.06694584e-02\n",
            "  5.67982905e-02 -8.93197116e-03 -8.81296583e-03  1.18111251e-02\n",
            " -4.65455316e-02  6.61119027e-03 -1.03082424e-02  1.89161040e-02\n",
            " -3.88484225e-02 -2.00925786e-02 -1.24961929e-02  3.43125202e-02\n",
            "  8.06513876e-02  8.57031345e-03  5.48191443e-02 -8.76406301e-03\n",
            " -4.86174040e-02  1.19756171e-02 -4.78381179e-02  5.51848253e-03\n",
            " -1.61668437e-03 -1.32956076e-02  4.70288135e-02  3.42324823e-02\n",
            "  5.31225279e-03 -3.12414058e-02  2.76588704e-02  6.02429025e-02\n",
            " -4.07225750e-02  1.33756921e-02 -6.20685704e-02 -6.14282079e-02\n",
            " -5.65229496e-03  9.52996721e-04  4.14974336e-03 -2.04899330e-02\n",
            " -7.88151100e-03  2.50319834e-03  4.57094088e-02  3.66412066e-02\n",
            " -3.41825671e-02  1.41910492e-02 -3.28168646e-02 -2.72444785e-02\n",
            "  3.18201189e-03 -1.27606783e-02  3.75861302e-02  2.00652685e-02\n",
            "  1.32168820e-02  4.30299006e-02  1.47052412e-03  2.55407132e-02\n",
            " -4.93185446e-02 -5.31147532e-02  8.30915570e-02 -1.29535338e-02\n",
            " -3.37079577e-02 -3.14587094e-02 -1.10212937e-02 -5.24082296e-02\n",
            "  2.51905508e-02 -6.34882823e-02  5.63919917e-02  3.30936499e-02\n",
            " -7.54685281e-03 -1.36812916e-02 -3.49578783e-02  9.06808488e-03\n",
            " -2.43933890e-02  3.33541147e-02 -3.33078541e-02  7.86006004e-02\n",
            " -4.03895266e-02 -4.28834073e-02  7.02440888e-02  2.44048126e-02\n",
            " -3.42628956e-02 -3.95024233e-02 -7.09323809e-02 -1.02303326e-01\n",
            " -3.42938676e-02  2.96857022e-03  5.55273555e-02 -2.90128347e-02\n",
            "  1.60258971e-02  2.11051255e-02  5.55295497e-03 -3.93538140e-02\n",
            "  4.09773253e-02 -1.84073241e-03  2.18502432e-02 -4.26562279e-02\n",
            "  4.42494219e-03  1.57215749e-03 -3.95385213e-02 -6.96269795e-02\n",
            "  1.56632662e-02 -2.37584710e-02  9.59168747e-03 -3.26643251e-02\n",
            "  4.27811816e-02 -1.42894583e-02 -8.48501362e-03  5.10237031e-02\n",
            "  1.61581484e-04  2.01652013e-02 -2.73338109e-02  7.97406733e-02\n",
            "  4.58096229e-02  1.43252108e-02 -2.30976027e-02  2.74004936e-02\n",
            " -1.98115688e-02 -2.40650009e-02 -8.30104388e-03 -7.12466463e-02\n",
            " -1.31835071e-02 -2.87365052e-03 -2.26201341e-02  2.99165733e-02\n",
            " -2.49972772e-02  4.02160063e-02  6.44775257e-02 -1.94848180e-02\n",
            "  9.33796540e-03 -4.28501964e-02 -1.57699070e-03 -6.76006004e-02\n",
            "  2.46011536e-03 -7.00249597e-02  1.07604060e-02  3.95564586e-02\n",
            " -3.20125073e-02  2.66113430e-02  1.31327482e-02 -4.35591443e-03\n",
            "  3.29335220e-02  1.31786612e-04  6.42107427e-02 -1.78116541e-02\n",
            "  3.04044783e-02 -8.40277970e-03 -1.26269925e-02 -5.24196262e-03\n",
            "  1.20707907e-01 -5.09302095e-02  6.10250561e-03 -3.41544412e-02\n",
            " -2.04601381e-02  6.73164651e-02  8.50272365e-03  3.64529490e-02\n",
            "  1.51560726e-02 -2.91348845e-02  3.14743556e-02  1.67096984e-02\n",
            "  2.17760298e-02  4.26867567e-02  1.96709093e-02 -1.84370186e-02\n",
            "  1.38984863e-02  6.83587417e-02  2.19450835e-02 -2.26423144e-02\n",
            "  7.41939172e-02 -2.34432779e-02  2.77849450e-03 -5.21914624e-02\n",
            "  1.22332955e-02  3.01097315e-02  7.92006850e-02  1.39439031e-01\n",
            " -6.73222682e-03 -2.13671271e-02  4.55861166e-02 -2.89720893e-02\n",
            "  4.98782173e-02  8.18917342e-03 -2.39838623e-02 -6.73523638e-03\n",
            "  3.34812514e-02 -2.80255023e-02  4.34586070e-02  1.77156612e-01\n",
            "  1.33608971e-02  2.36777458e-02  4.78545949e-03  2.36116406e-02\n",
            " -2.52767038e-02  4.99016307e-02  1.42144207e-02 -2.59160083e-02\n",
            "  1.08533576e-02 -2.84233093e-02  1.21151032e-02  3.06030661e-02\n",
            " -3.32748927e-02 -3.08977012e-02  1.52283153e-02 -1.30680082e-02\n",
            "  1.30520379e-02 -4.73319506e-03 -4.05001380e-02 -1.08716033e-01\n",
            "  6.11715429e-02  4.50890474e-02 -6.91033155e-02  1.30376068e-03\n",
            " -8.20942502e-03  2.74694394e-02 -5.13717346e-02  6.22893833e-02\n",
            " -3.60390358e-02 -2.65264753e-02 -2.25200504e-03 -7.33346213e-03\n",
            "  3.73813584e-02 -2.77078152e-02 -2.59273238e-02  1.56926773e-02\n",
            "  1.39523476e-01 -1.26070771e-02  3.81775610e-02 -1.64563290e-03\n",
            "  2.05623340e-02 -7.00530130e-03  2.54123565e-02 -1.09270386e-01\n",
            " -8.76862742e-03  2.76329610e-02 -2.49190698e-03  1.85611974e-02\n",
            "  1.90872084e-02  5.34162633e-02  3.44715528e-02  1.98638509e-03\n",
            "  6.05244152e-02 -5.29096201e-02  6.14578538e-02 -1.98357198e-02\n",
            " -2.73167640e-02  2.26206686e-02  5.86920185e-03 -2.80656349e-02\n",
            " -7.24000623e-03  1.31714446e-02  1.08502284e-02 -1.09774070e-02\n",
            "  3.54437120e-02 -2.99943779e-02 -2.35269759e-02 -4.28954177e-02\n",
            "  1.22119552e-02  4.17930223e-02 -4.22190595e-03  2.51515396e-02\n",
            "  5.34726195e-02 -7.34710619e-02  4.37731482e-02  6.65931916e-03\n",
            " -1.45144597e-01  4.82256375e-02 -2.08851155e-02 -2.83126719e-02\n",
            " -5.47820423e-03 -2.26166304e-02  1.89558323e-02 -7.82743096e-02\n",
            "  8.83836579e-03 -4.02152650e-02  3.70958932e-02  9.30924551e-04\n",
            "  3.29266791e-03 -7.31374929e-03  1.93683933e-02 -1.60731375e-02\n",
            "  5.10917902e-02  6.76376093e-03  3.18918913e-03 -5.85560640e-03\n",
            "  2.29144674e-02 -5.15443049e-02  2.17804722e-02  1.26642525e-01] \n",
            " \n",
            "\n",
            "The vector of word suami is [ 0.02505007 -0.09280356  0.05430515  0.10763547  0.0151221  -0.1595039\n",
            " -0.12060435 -0.01598709 -0.00354659 -0.13995966  0.05080862  0.03516166\n",
            " -0.04240736  0.02942154  0.02308521  0.01763232  0.01397745  0.0089389\n",
            " -0.00800163  0.04800776 -0.03560083  0.05418302  0.03532548  0.05508328\n",
            "  0.00777694 -0.03169639 -0.01715511  0.01429896  0.10008495 -0.00099747\n",
            "  0.05576871  0.01501     0.05056324 -0.04900618 -0.01933525 -0.01110866\n",
            " -0.00350731  0.04075997  0.05273832  0.01200971  0.03192285  0.01423426\n",
            "  0.03034078  0.17200896  0.05429779  0.02011075 -0.07172085 -0.0065414\n",
            "  0.03027637 -0.04025728 -0.01560988 -0.03918383 -0.05620691  0.01183057\n",
            "  0.02345913  0.03735793  0.00387181 -0.0362012  -0.10246968 -0.04861805\n",
            " -0.0268394  -0.03364152  0.0768936   0.04269859  0.01403616  0.02470035\n",
            " -0.00172039 -0.01069783 -0.05795144 -0.0165951   0.03514222  0.04032226\n",
            "  0.04831838  0.06347053 -0.01454027 -0.10266545 -0.05026296 -0.05937254\n",
            "  0.0113332  -0.00719005 -0.04372238  0.02139089  0.04235459  0.03849978\n",
            " -0.10580438 -0.10276207 -0.04148079  0.06468539  0.03651973  0.0034063\n",
            "  0.04241161 -0.04294555 -0.06585097 -0.01875682 -0.10795209 -0.10958974\n",
            " -0.07927557  0.02485918  0.06402965 -0.02401754  0.01846055 -0.03300414\n",
            "  0.00237699 -0.02470108  0.1197238  -0.00445497  0.08111342 -0.03408059\n",
            "  0.07672743  0.04150021 -0.03818592 -0.08475743  0.05926994 -0.00395163\n",
            "  0.05123802  0.02323162  0.01028133  0.01652439 -0.12171146  0.03337951\n",
            " -0.07243621  0.08947795 -0.07280995  0.05099525  0.08525981 -0.00899944\n",
            "  0.02445749  0.02568335 -0.01767897  0.03668142  0.05427001 -0.01294463\n",
            "  0.01626336  0.02600509  0.00076772  0.0238946  -0.00465549 -0.00935062\n",
            "  0.01811105 -0.07712547  0.01023237 -0.09049689  0.02746917 -0.00862019\n",
            " -0.09146345  0.1242319  -0.01856119 -0.11032816 -0.02851551 -0.06091653\n",
            "  0.05034848  0.0074288  -0.04568591  0.0026739   0.07118524  0.00584643\n",
            "  0.01389321  0.04636502 -0.06199282  0.07249422  0.07685061 -0.03602329\n",
            " -0.06643637 -0.03691951 -0.00475857  0.06897721  0.16067836  0.01556263\n",
            "  0.1162762  -0.04412731 -0.01433764 -0.01547143  0.03378594  0.01262521\n",
            "  0.0129421  -0.0017833  -0.00675031 -0.00589645  0.04621159 -0.05834643\n",
            "  0.05693575  0.03235384  0.04415888 -0.02718439  0.03493256  0.01246407\n",
            "  0.04354114  0.10912114  0.01193943 -0.02100113  0.03255002 -0.03646982\n",
            "  0.03789145  0.02537294 -0.05017151  0.02679065  0.0284661  -0.06811221\n",
            " -0.01734915  0.10479316  0.01195511 -0.08125239 -0.03020068 -0.03773097\n",
            "  0.01492107 -0.0090145   0.0521482  -0.01644898 -0.08575229  0.05603343\n",
            " -0.00520337 -0.00163498 -0.0513745  -0.00924847  0.0222769  -0.04182087\n",
            " -0.03183729  0.0083211   0.00982114 -0.02245587  0.00499379  0.01851223\n",
            " -0.03951019 -0.02210167 -0.02477129  0.00877856 -0.07142317  0.04623848\n",
            "  0.07486854 -0.07680821 -0.03730838  0.01466374  0.05449593  0.09256245\n",
            "  0.03606816 -0.10029449  0.02555619  0.00793487 -0.025363    0.0546158\n",
            "  0.03406273 -0.00939102  0.04061038 -0.06654945 -0.030051    0.02380685\n",
            " -0.0237146   0.04559835  0.03429899 -0.03743359  0.05807099 -0.01180513\n",
            "  0.14221132 -0.09819188  0.055359    0.0032721  -0.04855125 -0.00556568\n",
            "  0.06601164  0.00905531  0.05542715  0.01845221  0.0168183   0.0239785\n",
            " -0.01070697  0.02352062  0.06908455  0.06957858  0.00359209  0.03168188\n",
            " -0.04406982  0.00267403 -0.02227617 -0.11344974 -0.08510485  0.06176831\n",
            " -0.03585578  0.05910283  0.03295299  0.03936554 -0.07669943 -0.04547775\n",
            " -0.05226614 -0.10881573  0.02142135 -0.0490879   0.0574747  -0.01660513\n",
            "  0.01401693 -0.03170172  0.0335457   0.00325418 -0.03068293 -0.04284818\n",
            "  0.00893099  0.01789219  0.03178102 -0.02938142 -0.06032868  0.11268306] \n",
            " \n",
            "\n",
            "The vector of word saran is [ 1.00217611e-01  5.92146553e-02  6.14310615e-02  2.23494992e-02\n",
            " -8.87975469e-02 -5.09645641e-02 -9.49460547e-03  8.04017261e-02\n",
            " -1.11688487e-02 -1.11138374e-01 -8.30336586e-02  1.08125061e-02\n",
            " -3.56771685e-02  3.89369056e-02  8.69370922e-02  2.38226391e-02\n",
            "  5.20063713e-02 -1.23998895e-03 -2.17545107e-02  3.16210017e-02\n",
            "  6.47510737e-02  1.63532663e-02  4.39822264e-02  7.94101879e-03\n",
            " -3.10146976e-02 -8.26852024e-02  2.00372934e-03 -2.09903158e-02\n",
            "  3.21239829e-02 -1.27563570e-02  1.13530576e-01  1.53264757e-02\n",
            "  8.85076746e-02  2.38057896e-02  3.83190550e-02 -2.35278979e-02\n",
            " -2.37615593e-02  3.76832485e-02 -1.31807122e-02  4.12559360e-02\n",
            " -5.42109180e-03 -9.60842147e-03  1.57228895e-02  2.91362274e-02\n",
            " -2.08688062e-03  7.19540864e-02 -4.93063871e-03 -7.06987828e-03\n",
            " -2.66468264e-02  1.99884437e-02 -2.44535264e-02 -4.74986807e-02\n",
            "  5.02541922e-02 -2.73296610e-03 -4.92426455e-02  4.12120372e-02\n",
            "  1.20841816e-01  4.62762304e-02  9.75843519e-04 -8.89110845e-03\n",
            " -6.00138027e-03 -1.39671369e-02  2.24062358e-03 -5.08589707e-02\n",
            "  2.50906125e-02 -2.97339377e-03 -5.00551499e-02  4.37195227e-02\n",
            "  2.36831605e-02  1.57333929e-02  2.06532493e-01 -6.30068108e-02\n",
            "  4.27651778e-02 -9.15006176e-03 -6.90821907e-05 -2.48155035e-02\n",
            "  1.03443349e-02 -4.02362160e-02 -2.52245143e-02  4.04812358e-02\n",
            "  6.51275069e-02 -6.42741844e-02  6.83803260e-02  4.78425343e-03\n",
            " -1.66188311e-02 -5.24768746e-03  1.73493788e-01 -1.15799360e-01\n",
            " -4.48233681e-03 -1.48199666e-02  1.09569095e-02 -2.64015719e-02\n",
            " -1.20470732e-01 -7.04498962e-06 -5.92640340e-02 -1.47784188e-01\n",
            " -1.20328064e-03  4.53681089e-02  5.55046871e-02  4.81418818e-02\n",
            "  4.06062678e-02  7.41356015e-02 -1.24731287e-02 -2.50478517e-02\n",
            "  5.25794737e-03  5.45608141e-02  5.71602769e-02 -1.40353188e-01\n",
            "  1.39657278e-02 -1.20123550e-02 -3.95978056e-02 -4.23494838e-02\n",
            "  1.88288819e-02  3.20773292e-03  4.50400561e-02 -1.37303472e-02\n",
            " -4.38046791e-02  2.81458162e-02  2.95834020e-02  3.94982565e-03\n",
            " -3.21406089e-02 -6.60074353e-02  2.32169889e-02 -1.64547861e-02\n",
            "  1.69697758e-02 -2.44779680e-02 -5.78628592e-02  5.13416808e-03\n",
            " -8.83976370e-02 -2.05470994e-02  1.55877341e-02  3.21539193e-02\n",
            " -4.77252156e-03 -2.77558193e-02  9.44518596e-02  3.16092856e-02\n",
            " -5.93585819e-02 -6.05659559e-03  1.31785944e-01  2.02311762e-02\n",
            "  6.52650297e-02 -1.64528742e-01 -1.74465720e-02 -3.57914064e-03\n",
            " -2.06219032e-02  7.35752657e-03  2.27093436e-02 -6.32976443e-02\n",
            "  3.17280740e-02 -8.23417902e-02  6.41185418e-02  8.57054908e-03\n",
            " -6.09315857e-02 -1.40219424e-02  1.68405399e-02  4.58154604e-02\n",
            "  7.63401464e-02  1.28740557e-02 -2.83804350e-02  6.81087673e-02\n",
            "  1.37188822e-01 -9.37389955e-02 -3.33406217e-02  1.80184916e-02\n",
            "  1.29517708e-02 -2.59387307e-02 -3.02904174e-02 -7.13219568e-02\n",
            " -5.90955000e-03  5.43908142e-02  4.53055650e-02 -3.57454419e-02\n",
            "  2.79682428e-02  3.47392559e-02 -1.19553059e-02 -2.47324780e-02\n",
            "  1.03021925e-03 -3.78823467e-02  3.16669345e-02 -4.58277483e-03\n",
            "  2.77314559e-02 -2.16629002e-02 -3.18252966e-02 -3.44610512e-02\n",
            "  4.15057763e-02  6.25005513e-02 -2.35290676e-02 -3.11164297e-02\n",
            "  1.53219094e-02  3.05832550e-02 -4.42860611e-02  6.30816743e-02\n",
            " -2.10981537e-02  4.20073941e-02  8.39028805e-02  8.49423558e-02\n",
            "  2.64288448e-02  3.14192474e-03 -3.98964845e-02  5.62076196e-02\n",
            " -2.08420865e-02  1.74637698e-03  2.96889283e-02 -2.20518336e-02\n",
            "  6.84697255e-02  1.50207840e-02  9.91608948e-04 -1.43952612e-02\n",
            " -2.16272976e-02  4.26400527e-02  8.24841857e-02 -3.61927822e-02\n",
            " -2.55351216e-02  1.11278575e-02  5.22522964e-02 -7.72119835e-02\n",
            "  2.44105142e-02  1.58520453e-02 -5.37180668e-03 -5.66022471e-02\n",
            "  4.31984812e-02  7.04648644e-02 -5.08158505e-02  6.54983148e-02\n",
            "  5.74099720e-02 -1.28681737e-03 -1.21523373e-01 -7.64314160e-02\n",
            "  1.73603185e-02 -4.10668477e-02 -5.65250926e-02  9.64875519e-02\n",
            "  1.76249892e-02  4.65158783e-02  2.49964558e-02  2.26038620e-02\n",
            "  3.48175429e-02 -1.51315136e-02 -3.89123969e-02  3.24998125e-02\n",
            "  4.36353534e-02  4.02837433e-02 -1.43515002e-02  1.71595216e-02\n",
            " -7.22186193e-02 -2.23376863e-02 -5.05796522e-02 -7.51922699e-03\n",
            " -8.77155829e-03 -4.28359583e-02  4.63810675e-02 -3.49992663e-02\n",
            "  5.17849699e-02 -1.91857308e-01 -1.03237107e-05  3.08014732e-02\n",
            " -2.93850936e-02  4.67858985e-02 -2.37545883e-03 -9.72009730e-03\n",
            " -6.40347898e-02  4.03175019e-02 -6.67806864e-02 -3.96604538e-02\n",
            " -2.51802336e-02 -7.19460323e-02  2.49005333e-02  4.64383848e-02\n",
            " -3.22156474e-02  4.62861620e-02  4.59564999e-02  4.80166934e-02\n",
            " -4.78268974e-02 -3.74070480e-02  4.13922034e-02 -1.34802181e-02\n",
            " -6.18621632e-02  2.76014179e-01 -5.45814857e-02 -2.16195825e-04\n",
            "  1.82405338e-02 -5.70376543e-03 -2.59519089e-02 -1.63368657e-01\n",
            " -9.38704088e-02 -8.18418860e-02 -8.85813087e-02  2.22727768e-02\n",
            "  2.14633369e-03 -1.95333883e-02  2.41265148e-02  1.25171850e-02\n",
            " -3.28031508e-03  4.78390008e-02  1.37504004e-02 -2.02291925e-02\n",
            "  2.59129144e-02 -1.82051942e-01  3.85338720e-03  2.70656466e-01] \n",
            " \n",
            "\n",
            "The vector of word terimakasih is [-0.01625532 -0.00569167 -0.02642816  0.03423095 -0.08111053  0.00019\n",
            " -0.01449332  0.01365842 -0.00737613 -0.06571379 -0.02676059  0.01858669\n",
            " -0.02042217 -0.01934304 -0.02180071 -0.0330272  -0.02120573 -0.02804982\n",
            " -0.00037236 -0.00120014 -0.01491243 -0.00821008  0.02165679 -0.03414185\n",
            " -0.02190023  0.01686792  0.00534532 -0.02787574  0.02141074  0.02521316\n",
            "  0.0432975   0.0059227  -0.01735223  0.01348199 -0.04626276 -0.01601456\n",
            "  0.01064047  0.00693354 -0.03918154  0.00873585 -0.00631651  0.0027537\n",
            " -0.00839348  0.02486721 -0.01586673  0.02586964  0.0668163  -0.04947506\n",
            " -0.03205431  0.02522394 -0.00077379 -0.01891623 -0.01539378  0.03071515\n",
            "  0.02470424  0.01777777 -0.01291769  0.02120962 -0.00207243 -0.00819124\n",
            "  0.00571473 -0.03938672 -0.00315745  0.00258221 -0.02643    -0.01702296\n",
            " -0.003349   -0.00233935 -0.01653123 -0.01829166  0.07220132 -0.00758514\n",
            "  0.01964459  0.00656702 -0.02055096 -0.0003231   0.01287645 -0.08478438\n",
            "  0.03734704  0.0288936  -0.01335583 -0.03951739  0.02167869 -0.03043967\n",
            "  0.03146337  0.03945753  0.05512597 -0.02274298 -0.03888099  0.01501322\n",
            "  0.03158874  0.01564249 -0.02931384  0.03996332 -0.01794969 -0.07860065\n",
            "  0.02644184 -0.0835729   0.01256976  0.00857939  0.01073024  0.0235006\n",
            "  0.00614919 -0.01901586  0.02843986 -0.02079077  0.06574012 -0.02421452\n",
            "  0.01961296  0.00835093 -0.00486579 -0.06053898 -0.00928422 -0.0155416\n",
            " -0.01765648 -0.02779126  0.00206636  0.01914827 -0.01040521  0.02594892\n",
            " -0.02206304  0.00450685  0.00865148 -0.0043797  -0.02806969 -0.02556854\n",
            "  0.00057576  0.01503775  0.00875036 -0.00804736  0.04675755  0.02590485\n",
            "  0.0045417  -0.00738674  0.02513344  0.01814817 -0.00402452 -0.04454009\n",
            "  0.03224574  0.03102877 -0.00851352 -0.02853401  0.04195366 -0.00133885\n",
            " -0.04338735  0.01825801 -0.03293969 -0.00591633  0.02090024  0.0002851\n",
            "  0.01532746  0.02471285  0.01661028  0.00176751  0.01224886 -0.00458055\n",
            "  0.02123426 -0.01714163 -0.03114974 -0.00429239 -0.01548003  0.03143661\n",
            " -0.03531274 -0.04692906 -0.01872625  0.0354069   0.0291833  -0.00053848\n",
            "  0.00997317  0.01255627  0.02507477 -0.01607006  0.0044154   0.02362471\n",
            " -0.01309853 -0.01159887  0.01471293 -0.01615462 -0.01300712 -0.02479998\n",
            "  0.06153155 -0.02682219 -0.02002391 -0.02242102 -0.00663724  0.06805612\n",
            "  0.04015886 -0.01135003  0.0107497  -0.01407713 -0.06934207  0.05540751\n",
            "  0.01928927 -0.02489849 -0.02075027  0.01006441  0.01017951 -0.00618983\n",
            "  0.0112357   0.04182568 -0.00338399 -0.03466549 -0.0257914   0.01465153\n",
            "  0.05438685 -0.0184384  -0.00087126 -0.02472371  0.00847307  0.0066267\n",
            "  0.01290267  0.03675767  0.00899128 -0.01482876  0.01633171 -0.01980886\n",
            "  0.02720732  0.00108757 -0.00026684 -0.04504279  0.00598344 -0.0354881\n",
            " -0.03351372 -0.02239963  0.04310139  0.00854241 -0.05260674  0.01291695\n",
            " -0.01307305 -0.01948572 -0.02233279  0.01669244  0.0198778  -0.00815946\n",
            " -0.01370982  0.02414292  0.014925    0.01660855 -0.03194254 -0.00332242\n",
            " -0.03558187  0.0120397  -0.04992549 -0.05350614  0.02928127  0.03026082\n",
            " -0.02354058 -0.021257    0.02142305 -0.03276598 -0.03278069 -0.00214556\n",
            " -0.00490737 -0.05983153 -0.00044301 -0.01845081  0.02298674  0.03703712\n",
            " -0.00465438  0.00657564 -0.00842473  0.02372556 -0.04208877 -0.04439463\n",
            "  0.00896236 -0.01958911  0.00263491 -0.00230385 -0.03964882  0.01410789\n",
            " -0.00155286  0.01758002 -0.04212056  0.03493959 -0.00346239 -0.03729383\n",
            "  0.00261069  0.06323585 -0.04968391 -0.06432491 -0.0527696  -0.0297184\n",
            " -0.04135099  0.00889386  0.01771413  0.02182066 -0.06788212  0.02622909\n",
            "  0.00641893 -0.02856958 -0.00101671  0.01068806 -0.03464584  0.03221852\n",
            "  0.01946747 -0.01962341  0.00785023 -0.01199396 -0.01008194  0.09395646] \n",
            " \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcjDwetOXiKb",
        "outputId": "b9220a89-dba1-4338-e035-043959b51ab5"
      },
      "source": [
        "print(model['terimakasih'].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(300,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QNDb4U7HNvV"
      },
      "source": [
        "# Fafa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2eh045riNbB"
      },
      "source": [
        "## **Langkah 1**: Install dependency & import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtvD5FIOczGz",
        "outputId": "1464f4c5-791e-44b0-f2b9-8add7dbe1bdd"
      },
      "source": [
        "!pip install PySastrawi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PySastrawi in /usr/local/lib/python3.7/dist-packages (1.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YUeGDj2umzw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c424042e-55c8-4b6b-cdac-217dff1e9ada"
      },
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQSTEQlU_1IP"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "96fg_sxf_Mki",
        "outputId": "33f769a9-2a65-4c32-b64e-4f0e64c210e2"
      },
      "source": [
        "#Datasetnya download yang baru di google spreadsheet, sudah aku update tadi dibenerin kolom2nya dikit\n",
        "import pandas as pd\n",
        "df = pd.read_csv('Sapa.AI Own Dataset - Sheet1.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Layanan Hukum</th>\n",
              "      <th>Layanan Medis</th>\n",
              "      <th>Layanan Psikologis</th>\n",
              "      <th>Rehabilitasi Sosial</th>\n",
              "      <th>Jaminan Keselamatan</th>\n",
              "      <th>Layanan Pendidikan</th>\n",
              "      <th>Pengasuhan Pengganti</th>\n",
              "      <th>Bantuan Sosial</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>selamat sore , saya adalah ibu dari satu anak ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mohon bantu pak saya sedang hamil pak diluar n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Saya seorang anak perempuan yang bersuami teta...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Jika anak perempuan tidak di beri nafkah orang...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pak saya status pernikahan sudah setahun pisah...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  ...  Bantuan Sosial\n",
              "0  selamat sore , saya adalah ibu dari satu anak ...  ...               0\n",
              "1  Mohon bantu pak saya sedang hamil pak diluar n...  ...               0\n",
              "2  Saya seorang anak perempuan yang bersuami teta...  ...               0\n",
              "3  Jika anak perempuan tidak di beri nafkah orang...  ...               0\n",
              "4  Pak saya status pernikahan sudah setahun pisah...  ...               0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_eRkkSdf1OG",
        "outputId": "24b9e158-f1aa-4d68-a5cf-dc1042e56554"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 41 entries, 0 to 40\n",
            "Data columns (total 9 columns):\n",
            " #   Column                Non-Null Count  Dtype \n",
            "---  ------                --------------  ----- \n",
            " 0   Text                  41 non-null     object\n",
            " 1   Layanan Hukum         41 non-null     int64 \n",
            " 2   Layanan Medis         41 non-null     int64 \n",
            " 3   Layanan Psikologis    41 non-null     int64 \n",
            " 4   Rehabilitasi Sosial   41 non-null     int64 \n",
            " 5   Jaminan Keselamatan   41 non-null     int64 \n",
            " 6   Layanan Pendidikan    41 non-null     int64 \n",
            " 7   Pengasuhan Pengganti  41 non-null     int64 \n",
            " 8   Bantuan Sosial        41 non-null     int64 \n",
            "dtypes: int64(8), object(1)\n",
            "memory usage: 3.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA5D6r8HiUdJ"
      },
      "source": [
        "## **Langkah 2**: Preprocess Feature (Teks Pengaduan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp-Q1ORPigfX"
      },
      "source": [
        "**Langkah 2.1**: Stem dengan Sastrawi untuk mengubah bentuk kata ke bentuk dasar, hapus angka, hapus simbol, hapus whitespace. Lalu remove stop word untuk menghilangkan kata yang kurang bermakna di dalam suatu kalimat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPxSOK_M-X9B",
        "outputId": "e92b5e22-1211-45e2-bc47-001cdb8ac768"
      },
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "factory = StopWordRemoverFactory()\n",
        "stopword = factory.create_stop_word_remover()\n",
        "\n",
        "x = []\n",
        "for i in df['Text']:\n",
        "  stemmed_sentence = stemmer.stem(str(i))\n",
        "  stop_word_removed = stopword.remove(stemmed_sentence)\n",
        "  output_token = nltk.tokenize.word_tokenize(stop_word_removed)\n",
        "  x.append(output_token)\n",
        "\n",
        "x[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['selamat',\n",
              " 'sore',\n",
              " 'anak',\n",
              " 'janda',\n",
              " 'mantan',\n",
              " 'suami',\n",
              " 'aniaya',\n",
              " 'tolong',\n",
              " 'bantuanya']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rG_W8lVPuBW3"
      },
      "source": [
        "**Langkah 2.2**: Padding kalimat pada dataset dengan whitespace agar masing-masing vektor kalimat memiliki jumlah kata yang sama."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McYl3mgFuIBw",
        "outputId": "e4b5b0d5-14a6-4bb4-b1d5-ffb77c942e1a"
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "for sentence in x:\n",
        "  if len(sentence) > max_len:\n",
        "    max_len = len(sentence)\n",
        "\n",
        "for sentence in x:\n",
        "  sentence += [''] * (max_len - len(sentence))\n",
        "\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['selamat',\n",
              "  'sore',\n",
              "  'anak',\n",
              "  'janda',\n",
              "  'mantan',\n",
              "  'suami',\n",
              "  'aniaya',\n",
              "  'tolong',\n",
              "  'bantuanya',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['bantu',\n",
              "  'hamil',\n",
              "  'nikah',\n",
              "  'anak',\n",
              "  'kembar',\n",
              "  'pacar',\n",
              "  'tanggung',\n",
              "  'anak',\n",
              "  'hindar',\n",
              "  'gak',\n",
              "  'tau',\n",
              "  'keluarga',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['anak',\n",
              "  'perempuan',\n",
              "  'suam',\n",
              "  'tua',\n",
              "  'campur',\n",
              "  'ayah',\n",
              "  'ancam',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['anak',\n",
              "  'perempuan',\n",
              "  'nafkah',\n",
              "  'tua',\n",
              "  'ayah',\n",
              "  'anak',\n",
              "  'sekolah',\n",
              "  'tolong',\n",
              "  'makasih',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['status',\n",
              "  'nikah',\n",
              "  'tahun',\n",
              "  'pisah',\n",
              "  'ranjang',\n",
              "  'nikah',\n",
              "  'pisah',\n",
              "  'ranjang',\n",
              "  'suka',\n",
              "  'antem',\n",
              "  'urus',\n",
              "  'cerai',\n",
              "  'anak',\n",
              "  'bawa',\n",
              "  'ambil',\n",
              "  'paksa',\n",
              "  'kampung',\n",
              "  'takut',\n",
              "  'anak',\n",
              "  'anak',\n",
              "  'keras',\n",
              "  'ngontrol',\n",
              "  'anak',\n",
              "  'bilang',\n",
              "  'ngurus',\n",
              "  'anak',\n",
              "  'kasih',\n",
              "  'nafkah',\n",
              "  'pisah',\n",
              "  'ranjang',\n",
              "  'rumah',\n",
              "  'tangga'],\n",
              " ['ayah',\n",
              "  'larang',\n",
              "  'anak',\n",
              "  'temu',\n",
              "  'larang',\n",
              "  'rawat',\n",
              "  'anak',\n",
              "  'adu',\n",
              "  'kpppa',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['cerai',\n",
              "  'tujuh',\n",
              "  'tahun',\n",
              "  'ngga',\n",
              "  'nafkah',\n",
              "  'anak',\n",
              "  'bantu',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['anak',\n",
              "  'telantar',\n",
              "  'rantau',\n",
              "  'tahun',\n",
              "  'suami',\n",
              "  'bawa',\n",
              "  'istri',\n",
              "  'bawa',\n",
              "  'surat',\n",
              "  'cerai',\n",
              "  'fitnah',\n",
              "  'keluarga',\n",
              "  'dukung',\n",
              "  'benci',\n",
              "  'tuduh',\n",
              "  'bantu',\n",
              "  'pptppa',\n",
              "  'gak',\n",
              "  'tanggap',\n",
              "  'salah',\n",
              "  'bantu',\n",
              "  'ketidak-adilan',\n",
              "  'anak',\n",
              "  'makasih',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['lindung',\n",
              "  'perempuan',\n",
              "  'suami',\n",
              "  'siksa',\n",
              "  'istri',\n",
              "  'samapai',\n",
              "  'darah',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['tolong',\n",
              "  'lindung',\n",
              "  'cemar',\n",
              "  'nama',\n",
              "  'anak',\n",
              "  'ancam',\n",
              "  'edar',\n",
              "  'narkoba',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['bukti',\n",
              "  'suami',\n",
              "  'papah',\n",
              "  'anak',\n",
              "  'binatang',\n",
              "  'pakai',\n",
              "  'balok',\n",
              "  'kepala',\n",
              "  'leher',\n",
              "  'anggota',\n",
              "  'tubuh',\n",
              "  'kaki',\n",
              "  'kaki',\n",
              "  'patah',\n",
              "  'aman',\n",
              "  'rumah',\n",
              "  'suami',\n",
              "  'papah',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['assalamualaikum',\n",
              "  'anak',\n",
              "  'tahan',\n",
              "  'temu',\n",
              "  'bicara',\n",
              "  'mantan',\n",
              "  'suami',\n",
              "  'saran',\n",
              "  'terimakasih',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['selamat',\n",
              "  'sore',\n",
              "  'perempuan',\n",
              "  'umur',\n",
              "  'puluh',\n",
              "  'tahun',\n",
              "  'siksa',\n",
              "  'laki',\n",
              "  'ditidurin',\n",
              "  'hamil',\n",
              "  'suruh',\n",
              "  'gugurin',\n",
              "  'beda',\n",
              "  'gak',\n",
              "  'gugur',\n",
              "  'telantar',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['selamat',\n",
              "  'sore',\n",
              "  'tolong',\n",
              "  'bantu',\n",
              "  'peras',\n",
              "  'mantan',\n",
              "  'kalo',\n",
              "  'ngancam',\n",
              "  'sebar',\n",
              "  'salah',\n",
              "  'aib',\n",
              "  'tolong',\n",
              "  'bantu',\n",
              "  'terimakasih',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['bantu',\n",
              "  'yg',\n",
              "  'nyebar',\n",
              "  'foto',\n",
              "  'seksi',\n",
              "  'gimana',\n",
              "  'laporin',\n",
              "  'terima',\n",
              "  'kasih',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['saran',\n",
              "  'adik',\n",
              "  'perempuan',\n",
              "  'siksa',\n",
              "  'dipukulin',\n",
              "  'kakak',\n",
              "  'kandung',\n",
              "  'lapor',\n",
              "  'wajib',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['siang',\n",
              "  'anak',\n",
              "  'umur',\n",
              "  'cerai',\n",
              "  'suami',\n",
              "  'suami',\n",
              "  'suka',\n",
              "  'keras',\n",
              "  'rumah',\n",
              "  'tangga',\n",
              "  'aju',\n",
              "  'gugat',\n",
              "  'cerai',\n",
              "  'suami',\n",
              "  'suruh',\n",
              "  'cabut',\n",
              "  'gugat',\n",
              "  'cabut',\n",
              "  'ancam',\n",
              "  'ketemu',\n",
              "  'anak',\n",
              "  'saran',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['bully',\n",
              "  'teman',\n",
              "  'kelas',\n",
              "  'tahun',\n",
              "  'lapor',\n",
              "  'saran',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['tolong',\n",
              "  'saran',\n",
              "  'teman',\n",
              "  'anak',\n",
              "  'angkat',\n",
              "  'keluarga',\n",
              "  'angkat',\n",
              "  'tekan',\n",
              "  'batin',\n",
              "  'usir',\n",
              "  'rumah',\n",
              "  'keluarga',\n",
              "  'angkat',\n",
              "  'bicara',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['kasar',\n",
              "  'ayah',\n",
              "  'ayah',\n",
              "  'penganguran',\n",
              "  'yg',\n",
              "  'nafkah',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['bantu',\n",
              "  'anak',\n",
              "  'toxic',\n",
              "  'parent',\n",
              "  'jujur',\n",
              "  'keras',\n",
              "  'sekian',\n",
              "  'terimakasih',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['bantu',\n",
              "  'korban',\n",
              "  'kdrt',\n",
              "  'lapor',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['gimana',\n",
              "  'lapor',\n",
              "  'tindak',\n",
              "  'keras',\n",
              "  'bekas',\n",
              "  'memar',\n",
              "  'bekas',\n",
              "  'psikologi',\n",
              "  'mental',\n",
              "  'sang',\n",
              "  'anak',\n",
              "  'calon',\n",
              "  'ayah',\n",
              "  'calon',\n",
              "  'sah',\n",
              "  'terimakasih',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['maaf',\n",
              "  'menindaklanjuti',\n",
              "  'jantan',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['paksa',\n",
              "  'tua',\n",
              "  'meni',\n",
              "  'suka',\n",
              "  'tolak',\n",
              "  'biaya',\n",
              "  'kuliah',\n",
              "  'abai',\n",
              "  'keluarga',\n",
              "  'tua',\n",
              "  'tua',\n",
              "  'paksa',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['selamat',\n",
              "  'pagi',\n",
              "  'bantu',\n",
              "  'tua',\n",
              "  'paksa',\n",
              "  'meni',\n",
              "  'anak',\n",
              "  'gadis',\n",
              "  'umur',\n",
              "  'puluh',\n",
              "  'tahun',\n",
              "  'calon',\n",
              "  'kenal',\n",
              "  'cari',\n",
              "  'buka',\n",
              "  'seleksi',\n",
              "  'lamar',\n",
              "  'si',\n",
              "  'anak',\n",
              "  'henti',\n",
              "  'kuliah',\n",
              "  'gaul',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['bantu',\n",
              "  'suami',\n",
              "  'tinggal',\n",
              "  'anak',\n",
              "  'perempuan',\n",
              "  'malam',\n",
              "  'suami',\n",
              "  'tinggal',\n",
              "  'hutang',\n",
              "  'gimana',\n",
              "  'bawa',\n",
              "  'surat',\n",
              "  'nikah',\n",
              "  'tahun',\n",
              "  'nafkah',\n",
              "  '3',\n",
              "  'anak',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['selamat',\n",
              "  'malam',\n",
              "  'istri',\n",
              "  'suami',\n",
              "  'lindung',\n",
              "  'kepala',\n",
              "  'sakit',\n",
              "  'keras',\n",
              "  'telinga',\n",
              "  'ganggungan',\n",
              "  'dengar',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['korban',\n",
              "  'cemar',\n",
              "  'nama',\n",
              "  'anak2',\n",
              "  'edar',\n",
              "  'narkoba',\n",
              "  'lindung',\n",
              "  'hukum',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['anak',\n",
              "  'perempuan',\n",
              "  'usia',\n",
              "  'tujuh',\n",
              "  'belas',\n",
              "  'tahun',\n",
              "  'paksa',\n",
              "  'nikah',\n",
              "  'kandung',\n",
              "  'cinta',\n",
              "  'tua',\n",
              "  'kandung',\n",
              "  'urus',\n",
              "  'rebut',\n",
              "  'uang',\n",
              "  'uang',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['ambil',\n",
              "  'anak',\n",
              "  'adopsi',\n",
              "  'tolong',\n",
              "  'bantu',\n",
              "  'bayi',\n",
              "  'pangku',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['istri',\n",
              "  'anak',\n",
              "  'nikah',\n",
              "  'anak',\n",
              "  'lahir',\n",
              "  'rampas',\n",
              "  'keluarga',\n",
              "  'istri',\n",
              "  'teman',\n",
              "  'keluarga',\n",
              "  'istri',\n",
              "  'istri',\n",
              "  'sulit',\n",
              "  'ambil',\n",
              "  'hak',\n",
              "  'asuh',\n",
              "  'anak',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['undang',\n",
              "  'undang',\n",
              "  'jual',\n",
              "  'anak',\n",
              "  'umur',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['saudara',\n",
              "  'perempuan',\n",
              "  'milik',\n",
              "  'mental',\n",
              "  'hamil',\n",
              "  'kppa',\n",
              "  'damping',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['anak',\n",
              "  'korban',\n",
              "  'leceh',\n",
              "  'sosial',\n",
              "  'media',\n",
              "  'ancam',\n",
              "  'lapor',\n",
              "  'tolong',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['anak',\n",
              "  'mantan',\n",
              "  'napi',\n",
              "  'karakter',\n",
              "  'kasar',\n",
              "  'rela',\n",
              "  'anak',\n",
              "  'asuh',\n",
              "  'upaya',\n",
              "  'tempuh',\n",
              "  'tahan',\n",
              "  'anak',\n",
              "  'hidup',\n",
              "  'tolong',\n",
              "  'kait',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['meni',\n",
              "  'sah',\n",
              "  'laki',\n",
              "  'beda',\n",
              "  'agama',\n",
              "  'hubung',\n",
              "  'laki',\n",
              "  'kasar',\n",
              "  'karunia',\n",
              "  'putra',\n",
              "  'umur',\n",
              "  'delapan',\n",
              "  'tahun',\n",
              "  'rebut',\n",
              "  'anak',\n",
              "  'tahan',\n",
              "  'anak',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['istri',\n",
              "  'pns',\n",
              "  'meni',\n",
              "  'puluh',\n",
              "  'delapan',\n",
              "  'tahun',\n",
              "  'salah',\n",
              "  'keluarga',\n",
              "  'pns',\n",
              "  'langgar',\n",
              "  'atur',\n",
              "  'sanksi',\n",
              "  'suami',\n",
              "  'udah',\n",
              "  'nikah',\n",
              "  'sirih',\n",
              "  'biar',\n",
              "  'adu',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['adik',\n",
              "  'suami',\n",
              "  'tengkar',\n",
              "  'cerai',\n",
              "  'si',\n",
              "  'suami',\n",
              "  'tua',\n",
              "  'ambil',\n",
              "  'hak',\n",
              "  'asuh',\n",
              "  'si',\n",
              "  'anak',\n",
              "  'si',\n",
              "  'anak',\n",
              "  'usia',\n",
              "  'koma',\n",
              "  'tindak',\n",
              "  'ambil',\n",
              "  'adik',\n",
              "  'tolong',\n",
              "  'bantu',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['kuli',\n",
              "  'proyek',\n",
              "  'kemarin',\n",
              "  'sengaja',\n",
              "  'luka',\n",
              "  'usaha',\n",
              "  'bantu',\n",
              "  'obat',\n",
              "  'adu',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  ''],\n",
              " ['tolong',\n",
              "  'candu',\n",
              "  'narkoba',\n",
              "  'tahun',\n",
              "  'henti',\n",
              "  'lingkung',\n",
              "  'dukung',\n",
              "  'tolong',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '',\n",
              "  '']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t065V9RjERz"
      },
      "source": [
        "**Langkah 2.3**: Ubah semua kata pada dataset ke dalam bentuk embedding dengan menggunakan Neural Network Language Model dari TFHub untuk bahasa Indonesia dengan 50 dimensi embedding dan nilainya dinormalisasi. Output neural network berdimensi (42, 32, 50) yang bermakna bahwa data kita terdiri atas 42 kalimat yang masing-masing terdiri atas 32 kata yang masing-masing dinyatakan dengan 50 dimensi embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq58rLUX_wH2",
        "outputId": "6d8f5395-faf2-4961-a34a-3da22d64d49d"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "embed = hub.load(\"https://tfhub.dev/google/nnlm-id-dim50-with-normalization/2\")\n",
        "\n",
        "@tf.function\n",
        "def embedding(sentence_list):\n",
        "  embedded_sentences = []\n",
        "  for sentence in sentence_list:\n",
        "    sentence_tensor = []\n",
        "    for word in sentence:\n",
        "      data_tensor = tf.convert_to_tensor(word, dtype=tf.string)\n",
        "      sentence_tensor.append(data_tensor)\n",
        "    embeddings = embed(sentence_tensor)\n",
        "    embedded_sentences.append(embeddings)\n",
        "  return embedded_sentences\n",
        "\n",
        "x = embedding(x)\n",
        "tf.shape(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([41, 32, 50], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e55-pXFsGBcW",
        "outputId": "81182364-f95d-49c8-e648-c31718fdf2f8"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array(x)\n",
        "type(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlZXTTozU69E",
        "outputId": "056de78a-8b59-4e5e-ac88-2be12b974579"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjPo-JOn3xjd"
      },
      "source": [
        "## **Langkah 3**: Preprocess Label (Kelas Pengaduan)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMFUfOjj39YR",
        "outputId": "9803fb54-217c-4cd3-aa52-ea359eb7b0ab"
      },
      "source": [
        "y = []\n",
        "for i in df.index:\n",
        "  a = []\n",
        "  a.append(int(df['Layanan Hukum'][i]))\n",
        "  a.append(int(df['Layanan Medis'][i]))\n",
        "  a.append(int(df['Layanan Psikologis'][i]))\n",
        "  a.append(int(df['Rehabilitasi Sosial'][i]))\n",
        "  a.append(int(df['Jaminan Keselamatan'][i]))\n",
        "  a.append(int(df['Layanan Pendidikan'][i]))\n",
        "  a.append(int(df['Pengasuhan Pengganti'][i]))\n",
        "  a.append(int(df['Bantuan Sosial'][i].astype(int)))\n",
        "  y.append(a)\n",
        "tf.shape(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([41,  8], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0qJDIwfGR9C",
        "outputId": "599de446-651a-4d15-ca19-46c4baa9d3a8"
      },
      "source": [
        "import numpy as np\n",
        "y = np.array(y)\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 1, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 1, 1, 0],\n",
              "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1],\n",
              "       [1, 0, 1, 0, 0, 0, 0, 0],\n",
              "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 1, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 1, 0, 0, 0, 1, 0],\n",
              "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 1, 1, 0, 0, 0, 0, 0],\n",
              "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 1, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 1, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1],\n",
              "       [1, 1, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 1, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 1, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 1, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 1, 0, 0, 0],\n",
              "       [0, 0, 0, 1, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZWOaiivM8H9"
      },
      "source": [
        "train_data_feature = x[:35]\n",
        "train_data_label = y[:35]\n",
        "test_data_feature= x[35:]\n",
        "test_data_label = y[35:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjVgj1KOyjxT"
      },
      "source": [
        "## **Langkah 4**: Neural Network Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2muDngSOTb2"
      },
      "source": [
        "embed = hub.load(\"https://tfhub.dev/google/nnlm-id-dim50-with-normalization/2\")\n",
        "hub_layer = hub.KerasLayer(embed, input_shape=[], dtype=tf.string, trainable=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "a3BHXLZHy1vE",
        "outputId": "fc4f297e-b774-4ca1-a092-bedef98740cb"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "                             hub_layer,\n",
        "                             tf.keras.layers.Conv1D(64, 5, activation='relu'),\n",
        "                             tf.keras.layers.GlobalMaxPooling1D(),\n",
        "                             tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "                             tf.keras.layers.Dense(8, activation = 'softmax')\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-1690b409aa40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                              \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGlobalMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                              \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                              \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m ])\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, name)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 952\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1091\u001b[0;31m             inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    860\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2683\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m       input_spec.assert_input_compatibility(\n\u001b[0;32m-> 2685\u001b[0;31m           self.input_spec, inputs, self.name)\n\u001b[0m\u001b[1;32m   2686\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    237\u001b[0m                          \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                          \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                          str(tuple(shape)))\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0;31m# Check dtype.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer conv1d is incompatible with the layer: : expected min_ndim=3, found ndim=2. Full shape received: (None, 50)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0hjMf8rE_n-"
      },
      "source": [
        "from tensorflow import keras\n",
        "optimizer = keras.optimizers.Adam(learning_rate = 0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liBHOh6hFaft"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history = model.fit(train_data_feature, train_data_label,\n",
        "                    epochs=100,\n",
        "                    validation_data = (test_data_feature, test_data_label))\n",
        "plt.plot(history.history['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8rDmX9gZF2z"
      },
      "source": [
        "#Icha"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etXTRFOMZKQ-"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"/content/Sapa.AI Own Dataset - Sheet1.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEixfnoOd6Ur",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "37ad40db-7eed-4301-dc74-60bd3c1cd3e2"
      },
      "source": [
        "data.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Layanan Hukum</th>\n",
              "      <th>Layanan Medis</th>\n",
              "      <th>Layanan Psikologis</th>\n",
              "      <th>Rehabilitasi Sosial</th>\n",
              "      <th>Jaminan Keselamatan</th>\n",
              "      <th>Layanan Pendidikan</th>\n",
              "      <th>Pengasuhan Pengganti</th>\n",
              "      <th>Bantuan Sosial</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>Saya seorang ibu yang tidak pernah menikah sec...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>saya istri PNS sudah menikah tiga puluh delapa...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>Adik saya dan suaminya sering bertengkar terus...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>Saya bekerja sebagai kuli proyek, kemarin saya...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>Tolong saya, saya kecanduan narkoba sudah lebi...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Text  ...  Bantuan Sosial\n",
              "36  Saya seorang ibu yang tidak pernah menikah sec...  ...             0.0\n",
              "37  saya istri PNS sudah menikah tiga puluh delapa...  ...             0.0\n",
              "38  Adik saya dan suaminya sering bertengkar terus...  ...             0.0\n",
              "39  Saya bekerja sebagai kuli proyek, kemarin saya...  ...             NaN\n",
              "40  Tolong saya, saya kecanduan narkoba sudah lebi...  ...             0.0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToUxxjbLd8hZ"
      },
      "source": [
        "def preprocessing_text(text) :\n",
        "  factory_ = StemmerFactory()\n",
        "  stemmer = factory_.create_stemmer()\n",
        "  stemmed_sentence = stemmer.stem(text)\n",
        "  factory = StopWordRemoverFactory()\n",
        "  stopword = factory.create_stop_word_remover()\n",
        "  stop_word_removed = stopword.remove(stemmed_sentence)\n",
        "  final_token = nltk.tokenize.word_tokenize(stop_word_removed)\n",
        "  return final_token\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFOw12Puvnum"
      },
      "source": [
        "data[\"text_preprocess\"] = data['Text'].apply(preprocessing_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xBpRjXxwER1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "outputId": "bbff79ed-0acf-4c02-a071-8bc55edbcd3c"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Layanan Hukum</th>\n",
              "      <th>Layanan Medis</th>\n",
              "      <th>Layanan Psikologis</th>\n",
              "      <th>Rehabilitasi Sosial</th>\n",
              "      <th>Jaminan Keselamatan</th>\n",
              "      <th>Layanan Pendidikan</th>\n",
              "      <th>Pengasuhan Pengganti</th>\n",
              "      <th>Bantuan Sosial</th>\n",
              "      <th>text_preprocess</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>selamat sore , saya adalah ibu dari satu anak ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[selamat, sore, anak, janda, mantan, suami, an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mohon bantu pak saya sedang hamil pak diluar n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[bantu, hamil, nikah, anak, kembar, pacar, tan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Saya seorang anak perempuan yang bersuami teta...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[anak, perempuan, suam, tua, campur, ayah, ancam]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Jika anak perempuan tidak di beri nafkah orang...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[anak, perempuan, nafkah, tua, ayah, anak, sek...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pak saya status pernikahan sudah setahun pisah...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[status, nikah, tahun, pisah, ranjang, nikah, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  ...                                    text_preprocess\n",
              "0  selamat sore , saya adalah ibu dari satu anak ...  ...  [selamat, sore, anak, janda, mantan, suami, an...\n",
              "1  Mohon bantu pak saya sedang hamil pak diluar n...  ...  [bantu, hamil, nikah, anak, kembar, pacar, tan...\n",
              "2  Saya seorang anak perempuan yang bersuami teta...  ...  [anak, perempuan, suam, tua, campur, ayah, ancam]\n",
              "3  Jika anak perempuan tidak di beri nafkah orang...  ...  [anak, perempuan, nafkah, tua, ayah, anak, sek...\n",
              "4  Pak saya status pernikahan sudah setahun pisah...  ...  [status, nikah, tahun, pisah, ranjang, nikah, ...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKxlW8c4xnJE"
      },
      "source": [
        "data = data.drop(columns = \"Text\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyK_6kcvyudg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4b7293f-4ff8-4ca1-977a-70cfa3a7ed8a"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 41 entries, 0 to 40\n",
            "Data columns (total 9 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   Layanan Hukum         41 non-null     int64  \n",
            " 1   Layanan Medis         41 non-null     int64  \n",
            " 2   Layanan Psikologis    41 non-null     int64  \n",
            " 3   Rehabilitasi Sosial   41 non-null     int64  \n",
            " 4   Jaminan Keselamatan   41 non-null     int64  \n",
            " 5   Layanan Pendidikan    41 non-null     int64  \n",
            " 6   Pengasuhan Pengganti  41 non-null     int64  \n",
            " 7   Bantuan Sosial        40 non-null     float64\n",
            " 8   text_preprocess       41 non-null     object \n",
            "dtypes: float64(1), int64(7), object(1)\n",
            "memory usage: 3.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wZYEqSj8g76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "dc762913-97bb-4d7e-c66a-9eecdb4ec780"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Layanan Hukum</th>\n",
              "      <th>Layanan Medis</th>\n",
              "      <th>Layanan Psikologis</th>\n",
              "      <th>Rehabilitasi Sosial</th>\n",
              "      <th>Jaminan Keselamatan</th>\n",
              "      <th>Layanan Pendidikan</th>\n",
              "      <th>Pengasuhan Pengganti</th>\n",
              "      <th>Bantuan Sosial</th>\n",
              "      <th>text_preprocess</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[selamat, sore, anak, janda, mantan, suami, an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[bantu, hamil, nikah, anak, kembar, pacar, tan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[anak, perempuan, suam, tua, campur, ayah, ancam]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[anak, perempuan, nafkah, tua, ayah, anak, sek...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[status, nikah, tahun, pisah, ranjang, nikah, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Layanan Hukum  ...                                    text_preprocess\n",
              "0              1  ...  [selamat, sore, anak, janda, mantan, suami, an...\n",
              "1              0  ...  [bantu, hamil, nikah, anak, kembar, pacar, tan...\n",
              "2              1  ...  [anak, perempuan, suam, tua, campur, ayah, ancam]\n",
              "3              0  ...  [anak, perempuan, nafkah, tua, ayah, anak, sek...\n",
              "4              1  ...  [status, nikah, tahun, pisah, ranjang, nikah, ...\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hTzzLaD9UMb"
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "\n",
        "for text in data[\"text_preprocess\"] :\n",
        "  if len(text) > max_len :\n",
        "    max_len = len(text)\n",
        "\n",
        "\n",
        "for text in data[\"text_preprocess\"] :\n",
        "  text += [' '] * (max_len - len(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEUPGL0N-I_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04458e52-1c81-4c39-991c-9b473e841b58"
      },
      "source": [
        "data.text_preprocess"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     [selamat, sore, anak, janda, mantan, suami, an...\n",
              "1     [bantu, hamil, nikah, anak, kembar, pacar, tan...\n",
              "2     [anak, perempuan, suam, tua, campur, ayah, anc...\n",
              "3     [anak, perempuan, nafkah, tua, ayah, anak, sek...\n",
              "4     [status, nikah, tahun, pisah, ranjang, nikah, ...\n",
              "5     [ayah, larang, anak, temu, larang, rawat, anak...\n",
              "6     [cerai, tujuh, tahun, ngga, nafkah, anak, bant...\n",
              "7     [anak, telantar, rantau, tahun, suami, bawa, i...\n",
              "8     [lindung, perempuan, suami, siksa, istri, sama...\n",
              "9     [tolong, lindung, cemar, nama, anak, ancam, ed...\n",
              "10    [bukti, suami, papah, anak, binatang, pakai, b...\n",
              "11    [assalamualaikum, anak, tahan, temu, bicara, m...\n",
              "12    [selamat, sore, perempuan, umur, puluh, tahun,...\n",
              "13    [selamat, sore, tolong, bantu, peras, mantan, ...\n",
              "14    [bantu, yg, nyebar, foto, seksi, gimana, lapor...\n",
              "15    [saran, adik, perempuan, siksa, dipukulin, kak...\n",
              "16    [siang, anak, umur, cerai, suami, suami, suka,...\n",
              "17    [bully, teman, kelas, tahun, lapor, saran,  , ...\n",
              "18    [tolong, saran, teman, anak, angkat, keluarga,...\n",
              "19    [kasar, ayah, ayah, penganguran, yg, nafkah,  ...\n",
              "20    [bantu, anak, toxic, parent, jujur, keras, sek...\n",
              "21    [bantu, korban, kdrt, lapor,  ,  ,  ,  ,  ,  ,...\n",
              "22    [gimana, lapor, tindak, keras, bekas, memar, b...\n",
              "23    [maaf, menindaklanjuti, jantan,  ,  ,  ,  ,  ,...\n",
              "24    [paksa, tua, meni, suka, tolak, biaya, kuliah,...\n",
              "25    [selamat, pagi, bantu, tua, paksa, meni, anak,...\n",
              "26    [bantu, suami, tinggal, anak, perempuan, malam...\n",
              "27    [selamat, malam, istri, suami, lindung, kepala...\n",
              "28    [korban, cemar, nama, anak2, edar, narkoba, li...\n",
              "29    [anak, perempuan, usia, tujuh, belas, tahun, p...\n",
              "30    [ambil, anak, adopsi, tolong, bantu, bayi, pan...\n",
              "31    [istri, anak, nikah, anak, lahir, rampas, kelu...\n",
              "32    [undang, undang, jual, anak, umur,  ,  ,  ,  ,...\n",
              "33    [saudara, perempuan, milik, mental, hamil, kpp...\n",
              "34    [anak, korban, leceh, sosial, media, ancam, la...\n",
              "35    [anak, mantan, napi, karakter, kasar, rela, an...\n",
              "36    [meni, sah, laki, beda, agama, hubung, laki, k...\n",
              "37    [istri, pns, meni, puluh, delapan, tahun, sala...\n",
              "38    [adik, suami, tengkar, cerai, si, suami, tua, ...\n",
              "39    [kuli, proyek, kemarin, sengaja, luka, usaha, ...\n",
              "40    [tolong, candu, narkoba, tahun, henti, lingkun...\n",
              "Name: text_preprocess, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezMoSp5y-Ot_"
      },
      "source": [
        "text_clean = data[\"text_preprocess\"].values\n",
        "label_clean = data.drop(columns = \"text_preprocess\").values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P1vjqr8_W3L"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "text_clean_latih, text_clean_test, label_clean_latih, label_clean_test = train_test_split(text_clean, label_clean, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecK8Ga4bEa_a"
      },
      "source": [
        "def embedding(sentence_list):\n",
        "  embedded_sentences = []\n",
        "  for sentence in sentence_list:\n",
        "    sentence_tensor = []\n",
        "    for word in sentence:\n",
        "      data_tensor = tf.convert_to_tensor(word, dtype=tf.string)\n",
        "  return data_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMi3_XesEfA6"
      },
      "source": [
        "text_clean_latih = embedding(text_clean_latih)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vx1d6GJYE1tL"
      },
      "source": [
        "text_clean_test = embedding(text_clean_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M9L-NzsCs_k"
      },
      "source": [
        "label_clean_test = np.array(label_clean_test)\n",
        "label_clean_latih = np.array(label_clean_latih)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4VmA12V_lmg"
      },
      "source": [
        "embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
        "\n",
        "hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB_PsV0t_8x7"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "        hub_layer,\n",
        "        tf.keras.layers.Dense(16, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjtlpPqX_-rH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "b67ad258-fe4c-4720-f1f6-4dfa6d3f5769"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(text_clean_latih, label_clean_latih,\n",
        "                    epochs=20,\n",
        "                    validation_data = (text_clean_test, label_clean_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-2a08bfd7ea2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m history = model.fit(text_clean_latih, label_clean_latih,\n\u001b[1;32m      6\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     validation_data = (text_clean_test, label_clean_test))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_x_y_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_x_y_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v2_behavior\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGe5zJmtFyS1"
      },
      "source": [
        "# Coba Pake Cara Tensorflow Biasa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeRJZ-VJF3gT"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/Sapa.AI Own Dataset - Sheet1.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHJFZDINGAfA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "416e6b6e-031a-4db8-d9ea-e9f8be493038"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Layanan Hukum</th>\n",
              "      <th>Layanan Medis</th>\n",
              "      <th>Layanan Psikologis</th>\n",
              "      <th>Rehabilitasi Sosial</th>\n",
              "      <th>Jaminan Keselamatan</th>\n",
              "      <th>Layanan Pendidikan</th>\n",
              "      <th>Pengasuhan Pengganti</th>\n",
              "      <th>Bantuan Sosial</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>selamat sore , saya adalah ibu dari satu anak ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mohon bantu pak saya sedang hamil pak diluar n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Saya seorang anak perempuan yang bersuami teta...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Jika anak perempuan tidak di beri nafkah orang...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pak saya status pernikahan sudah setahun pisah...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  ...  Bantuan Sosial\n",
              "0  selamat sore , saya adalah ibu dari satu anak ...  ...               0\n",
              "1  Mohon bantu pak saya sedang hamil pak diluar n...  ...               0\n",
              "2  Saya seorang anak perempuan yang bersuami teta...  ...               0\n",
              "3  Jika anak perempuan tidak di beri nafkah orang...  ...               0\n",
              "4  Pak saya status pernikahan sudah setahun pisah...  ...               0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "180gZ8K5GDf4"
      },
      "source": [
        "text = df[\"Text\"].values\n",
        "label = df.drop(columns = \"Text\").values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLt422ksGL_U"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "text_latih, text_test, label_latih, label_test = train_test_split(text, label, test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8h3py1cGX3e"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "tokenizer = Tokenizer(num_words=5000, oov_token='x')\n",
        "tokenizer.fit_on_texts(text_latih) \n",
        "tokenizer.fit_on_texts(text_test)\n",
        " \n",
        "sekuens_latih = tokenizer.texts_to_sequences(text_latih)\n",
        "sekuens_test = tokenizer.texts_to_sequences(text_test)\n",
        " \n",
        "padded_latih = pad_sequences(sekuens_latih) \n",
        "padded_test = pad_sequences(sekuens_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj38QnWuGhpx"
      },
      "source": [
        "import tensorflow as tf\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),\n",
        "    tf.keras.layers.LSTM(64),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(8, activation='softmax')\n",
        "])\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoe9QgEEGnVM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de22742-83ef-4391-c308-da4fda8f66b2"
      },
      "source": [
        "num_epochs = 50\n",
        "history = model.fit(padded_latih, label_latih, epochs=num_epochs, \n",
        "                    validation_data=(padded_test, label_test), verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - 3s 3s/step - loss: nan - accuracy: 0.0312 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 86ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 76ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 84ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 82ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 84ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 87ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 88ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 76ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 85ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 79ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 87ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 75ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 82ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 81ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 86ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 81ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 86ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 87ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 85ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 81ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 88ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 83ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 79ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 80ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 77ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 84ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 82ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 77ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 86ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 81ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 80ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 81ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 83ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 80ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 86ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 79ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 84ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 84ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 94ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 77ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 79ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 75ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 79ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 75ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 81ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 83ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 89ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 81ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 93ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.8889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SegDOOmhOLa4"
      },
      "source": [
        "# Dari Notebook Yang Satu Lagi (+ Edits)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mWnUUtsObtg",
        "outputId": "89600dfd-3a80-44e2-c8fe-3a759a8aaee3"
      },
      "source": [
        "!pip install tensorflow-hub\n",
        "!pip install tensorflow==2.3.0\n",
        "!pip install tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (3.12.4)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (1.18.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (56.1.0)\n",
            "Requirement already satisfied: tensorflow==2.3.0 in /usr/local/lib/python3.7/dist-packages (2.3.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.12.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.15.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (2.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (2.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.1.2)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.6.3)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.4.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.32.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.2.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.3.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.36.2)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (2.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow==2.3.0) (56.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.28.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2020.12.5)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.2.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.10.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.8)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.12.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQmcmgMNOPp1"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_addons as tfa\n",
        "from keras.regularizers import l2\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyNZCC2XOpdv"
      },
      "source": [
        "data = pd.read_csv(\"Sapa.AI Own Dataset - Sheet1.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "3wxBy7x7OruJ",
        "outputId": "2b26d5b7-02d7-4287-edb2-cff7b7fda922"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Layanan Hukum</th>\n",
              "      <th>Layanan Medis</th>\n",
              "      <th>Layanan Psikologis</th>\n",
              "      <th>Rehabilitasi Sosial</th>\n",
              "      <th>Jaminan Keselamatan</th>\n",
              "      <th>Layanan Pendidikan</th>\n",
              "      <th>Pengasuhan Pengganti</th>\n",
              "      <th>Bantuan Sosial</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>selamat sore , saya adalah ibu dari satu anak ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mohon bantu pak saya sedang hamil pak diluar n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Saya seorang anak perempuan yang bersuami teta...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Jika anak perempuan tidak di beri nafkah orang...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pak saya status pernikahan sudah setahun pisah...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  ...  Bantuan Sosial\n",
              "0  selamat sore , saya adalah ibu dari satu anak ...  ...               0\n",
              "1  Mohon bantu pak saya sedang hamil pak diluar n...  ...               0\n",
              "2  Saya seorang anak perempuan yang bersuami teta...  ...               0\n",
              "3  Jika anak perempuan tidak di beri nafkah orang...  ...               0\n",
              "4  Pak saya status pernikahan sudah setahun pisah...  ...               0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMMn_XUdOtSY",
        "outputId": "91317a87-688a-47f5-855c-6ef9b5b06e4b"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 41 entries, 0 to 40\n",
            "Data columns (total 9 columns):\n",
            " #   Column                Non-Null Count  Dtype \n",
            "---  ------                --------------  ----- \n",
            " 0   Text                  41 non-null     object\n",
            " 1   Layanan Hukum         41 non-null     int64 \n",
            " 2   Layanan Medis         41 non-null     int64 \n",
            " 3   Layanan Psikologis    41 non-null     int64 \n",
            " 4   Rehabilitasi Sosial   41 non-null     int64 \n",
            " 5   Jaminan Keselamatan   41 non-null     int64 \n",
            " 6   Layanan Pendidikan    41 non-null     int64 \n",
            " 7   Pengasuhan Pengganti  41 non-null     int64 \n",
            " 8   Bantuan Sosial        41 non-null     int64 \n",
            "dtypes: int64(8), object(1)\n",
            "memory usage: 3.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRv4iwzWOupw"
      },
      "source": [
        "features = data[\"Text\"]\n",
        "label = data.drop(columns = \"Text\")\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(features,\n",
        "                                                    label,\n",
        "                                                    test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3ANjlyBOwtk",
        "outputId": "6e77d45f-bc05-4b76-9da3-bd36ea1819ed"
      },
      "source": [
        "encoder = hub.load(\"https://tfhub.dev/google/nnlm-id-dim128-with-normalization/2\")\n",
        "\n",
        "#coba encoder\n",
        "encoder([\"Hello World\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 128), dtype=float32, numpy=\n",
              "array([[ 2.36742683e-02,  6.35085702e-02, -1.15481585e-01,\n",
              "        -1.45702913e-01,  1.04301488e-02, -1.74822628e-01,\n",
              "        -9.14966688e-02, -1.10140834e-02, -3.11713554e-02,\n",
              "         4.42826338e-02, -1.18015938e-01,  1.65684000e-01,\n",
              "         3.28270644e-02, -1.65709965e-02, -2.08926260e-01,\n",
              "        -6.39974847e-02,  1.11344308e-02, -1.77620471e-01,\n",
              "         9.67197642e-02,  5.14155701e-02, -2.61958130e-02,\n",
              "         4.59473617e-02, -9.79337990e-02,  1.42703667e-01,\n",
              "         8.68656021e-03,  6.42589107e-02, -4.82872576e-02,\n",
              "         5.63185662e-02, -1.99931618e-02, -1.77717619e-02,\n",
              "        -5.69315739e-02, -8.88189152e-02,  1.71472654e-02,\n",
              "        -2.29171421e-02,  3.65193412e-02,  1.13376379e-01,\n",
              "        -1.44323051e-01, -6.15268312e-02,  6.47685379e-02,\n",
              "         1.04529031e-01,  9.77970511e-02, -1.58518061e-01,\n",
              "         1.21136703e-01, -4.98565286e-02,  7.84816220e-02,\n",
              "         8.32099691e-02,  5.00144213e-02, -3.16916895e-03,\n",
              "        -3.43333296e-02,  1.64713711e-01,  2.31702805e-01,\n",
              "         7.91639239e-02,  1.13059379e-01,  8.17712992e-02,\n",
              "         7.26254731e-02, -5.31775430e-02,  3.76794413e-02,\n",
              "        -1.19780242e-01, -9.47449356e-02, -6.18526861e-02,\n",
              "        -1.10123403e-01,  2.43674070e-01,  1.90962330e-02,\n",
              "        -3.52120996e-02,  9.21682566e-02, -3.29064131e-02,\n",
              "         5.08862436e-02, -3.33181629e-03, -2.13269308e-01,\n",
              "        -1.14490017e-01,  3.09340842e-02,  1.48616761e-01,\n",
              "        -6.48055822e-02,  1.97822720e-01,  6.47367164e-02,\n",
              "        -4.40166257e-02,  6.04767760e-04,  8.95183459e-02,\n",
              "         9.27944630e-02, -3.13501656e-02, -4.66254987e-02,\n",
              "        -7.03167461e-05,  4.65048105e-02,  1.52915195e-01,\n",
              "         6.49038404e-02,  2.61299968e-01,  6.13905713e-02,\n",
              "         1.04573984e-02,  4.29929346e-02, -1.20397215e-03,\n",
              "         1.48121580e-01, -4.91156764e-02, -2.16861311e-02,\n",
              "        -1.95734173e-01, -4.25939038e-02, -4.29686010e-02,\n",
              "         7.79583529e-02, -2.29228269e-02, -4.26675081e-02,\n",
              "        -2.17963099e-01, -3.13717909e-02,  3.81008498e-02,\n",
              "        -1.27007976e-01, -9.07173827e-02, -8.86347145e-02,\n",
              "        -1.52369320e-01,  2.30633914e-01,  1.20986320e-01,\n",
              "        -3.41656767e-02, -8.80528912e-02, -6.80254847e-02,\n",
              "         8.39649525e-04,  4.24895212e-02, -1.24584690e-01,\n",
              "        -1.33455873e-01, -7.83608034e-02,  7.85712600e-02,\n",
              "         1.18587557e-02,  8.14007446e-02, -1.38191015e-01,\n",
              "        -2.48129740e-02,  5.91992028e-02,  1.16235949e-01,\n",
              "         6.09784611e-02,  8.83749127e-02,  1.59275439e-03,\n",
              "        -2.44132131e-02,  3.35503183e-02]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBNQvlEjO5lh",
        "outputId": "63c73c4d-8c24-4626-ab61-244cba611fad"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "                                    hub.KerasLayer(\"https://tfhub.dev/google/nnlm-id-dim128-with-normalization/2\",\n",
        "                                                   input_shape = [],\n",
        "                                                   dtype = tf.string,\n",
        "                                                   trainable = False),\n",
        "                                    tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)),\n",
        "                                    tf.keras.layers.Dropout(0.3),\n",
        "                                    tf.keras.layers.Dense(8, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdc12e2d710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7fdc12e2d710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abKTjnOSO-Y9"
      },
      "source": [
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer = opt,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=[tfa.metrics.F1Score(num_classes=8, threshold=0.09)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bG6LxukHO-3H",
        "outputId": "f708ca4b-4a0a-44bb-cf4d-83473c9d606c"
      },
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "          epochs = 500,\n",
        "          validation_data = (x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 0s 231ms/step - loss: 2.7843 - f1_score: 0.2706 - val_loss: 2.6258 - val_f1_score: 0.2146\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.6780 - f1_score: 0.2706 - val_loss: 2.5223 - val_f1_score: 0.2146\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 2.5752 - f1_score: 0.2706 - val_loss: 2.4225 - val_f1_score: 0.2146\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 2.4782 - f1_score: 0.2642 - val_loss: 2.3262 - val_f1_score: 0.2146\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.3853 - f1_score: 0.2779 - val_loss: 2.2335 - val_f1_score: 0.2021\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.2929 - f1_score: 0.2932 - val_loss: 2.1441 - val_f1_score: 0.2125\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 2.2063 - f1_score: 0.2636 - val_loss: 2.0583 - val_f1_score: 0.1813\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 2.1224 - f1_score: 0.2550 - val_loss: 1.9757 - val_f1_score: 0.1813\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 2.0407 - f1_score: 0.2696 - val_loss: 1.8965 - val_f1_score: 0.1813\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.9661 - f1_score: 0.2052 - val_loss: 1.8207 - val_f1_score: 0.1813\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.8931 - f1_score: 0.2721 - val_loss: 1.7483 - val_f1_score: 0.1813\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.8237 - f1_score: 0.2409 - val_loss: 1.6792 - val_f1_score: 0.1671\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 1.7582 - f1_score: 0.2081 - val_loss: 1.6133 - val_f1_score: 0.1688\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.6886 - f1_score: 0.2081 - val_loss: 1.5505 - val_f1_score: 0.1688\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.6293 - f1_score: 0.2081 - val_loss: 1.4907 - val_f1_score: 0.1688\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.5680 - f1_score: 0.2114 - val_loss: 1.4336 - val_f1_score: 0.1688\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.5109 - f1_score: 0.2133 - val_loss: 1.3793 - val_f1_score: 0.1688\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.4534 - f1_score: 0.2174 - val_loss: 1.3277 - val_f1_score: 0.1688\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.3989 - f1_score: 0.2114 - val_loss: 1.2784 - val_f1_score: 0.1688\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.3468 - f1_score: 0.2153 - val_loss: 1.2315 - val_f1_score: 0.1688\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 1.2994 - f1_score: 0.2114 - val_loss: 1.1867 - val_f1_score: 0.1671\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 1.2501 - f1_score: 0.2614 - val_loss: 1.1437 - val_f1_score: 0.1671\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.2055 - f1_score: 0.2081 - val_loss: 1.1028 - val_f1_score: 0.1836\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 1.1619 - f1_score: 0.2097 - val_loss: 1.0636 - val_f1_score: 0.1836\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.1206 - f1_score: 0.2483 - val_loss: 1.0260 - val_f1_score: 0.1836\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.0819 - f1_score: 0.2038 - val_loss: 0.9898 - val_f1_score: 0.1836\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.0382 - f1_score: 0.2468 - val_loss: 0.9551 - val_f1_score: 0.1836\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 1.0034 - f1_score: 0.2468 - val_loss: 0.9215 - val_f1_score: 0.1836\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.9697 - f1_score: 0.2766 - val_loss: 0.8891 - val_f1_score: 0.1836\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.9336 - f1_score: 0.2885 - val_loss: 0.8579 - val_f1_score: 0.1836\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.9022 - f1_score: 0.2899 - val_loss: 0.8279 - val_f1_score: 0.1836\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.8744 - f1_score: 0.2622 - val_loss: 0.7993 - val_f1_score: 0.1836\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.8427 - f1_score: 0.2780 - val_loss: 0.7720 - val_f1_score: 0.1836\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.8131 - f1_score: 0.3809 - val_loss: 0.7461 - val_f1_score: 0.1836\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.7893 - f1_score: 0.3539 - val_loss: 0.7214 - val_f1_score: 0.1836\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.7605 - f1_score: 0.3837 - val_loss: 0.6982 - val_f1_score: 0.1836\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7391 - f1_score: 0.3644 - val_loss: 0.6764 - val_f1_score: 0.1836\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.7157 - f1_score: 0.3645 - val_loss: 0.6559 - val_f1_score: 0.1671\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.6940 - f1_score: 0.3070 - val_loss: 0.6368 - val_f1_score: 0.1671\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.6726 - f1_score: 0.3885 - val_loss: 0.6187 - val_f1_score: 0.1671\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.6504 - f1_score: 0.3945 - val_loss: 0.6017 - val_f1_score: 0.1671\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6314 - f1_score: 0.3924 - val_loss: 0.5857 - val_f1_score: 0.1671\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6112 - f1_score: 0.3904 - val_loss: 0.5705 - val_f1_score: 0.1836\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.5953 - f1_score: 0.3841 - val_loss: 0.5560 - val_f1_score: 0.1836\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.5816 - f1_score: 0.3695 - val_loss: 0.5422 - val_f1_score: 0.1836\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.5638 - f1_score: 0.3904 - val_loss: 0.5287 - val_f1_score: 0.1836\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.5517 - f1_score: 0.3715 - val_loss: 0.5155 - val_f1_score: 0.1836\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.5361 - f1_score: 0.3945 - val_loss: 0.5032 - val_f1_score: 0.1836\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.5201 - f1_score: 0.3695 - val_loss: 0.4913 - val_f1_score: 0.1836\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.5105 - f1_score: 0.3945 - val_loss: 0.4802 - val_f1_score: 0.1671\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.4979 - f1_score: 0.3785 - val_loss: 0.4702 - val_f1_score: 0.1671\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4854 - f1_score: 0.3760 - val_loss: 0.4611 - val_f1_score: 0.1671\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.4784 - f1_score: 0.3656 - val_loss: 0.4530 - val_f1_score: 0.1836\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.4651 - f1_score: 0.3842 - val_loss: 0.4455 - val_f1_score: 0.1836\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.4555 - f1_score: 0.3572 - val_loss: 0.4388 - val_f1_score: 0.1836\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.4463 - f1_score: 0.3785 - val_loss: 0.4322 - val_f1_score: 0.1836\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.4356 - f1_score: 0.3708 - val_loss: 0.4259 - val_f1_score: 0.1836\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4273 - f1_score: 0.3812 - val_loss: 0.4199 - val_f1_score: 0.1864\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.4188 - f1_score: 0.4128 - val_loss: 0.4139 - val_f1_score: 0.1864\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4119 - f1_score: 0.3972 - val_loss: 0.4081 - val_f1_score: 0.1864\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.4058 - f1_score: 0.4083 - val_loss: 0.4024 - val_f1_score: 0.1688\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.3991 - f1_score: 0.4083 - val_loss: 0.3970 - val_f1_score: 0.1688\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.3898 - f1_score: 0.4217 - val_loss: 0.3921 - val_f1_score: 0.1688\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.3821 - f1_score: 0.4077 - val_loss: 0.3879 - val_f1_score: 0.1688\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.3803 - f1_score: 0.4029 - val_loss: 0.3840 - val_f1_score: 0.1688\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.3730 - f1_score: 0.4145 - val_loss: 0.3806 - val_f1_score: 0.1688\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.3697 - f1_score: 0.4062 - val_loss: 0.3778 - val_f1_score: 0.1688\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3631 - f1_score: 0.4161 - val_loss: 0.3750 - val_f1_score: 0.1688\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.3586 - f1_score: 0.4062 - val_loss: 0.3719 - val_f1_score: 0.1688\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.3553 - f1_score: 0.4062 - val_loss: 0.3687 - val_f1_score: 0.1688\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.3502 - f1_score: 0.4196 - val_loss: 0.3654 - val_f1_score: 0.1688\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.3467 - f1_score: 0.4094 - val_loss: 0.3625 - val_f1_score: 0.1688\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.3437 - f1_score: 0.4062 - val_loss: 0.3603 - val_f1_score: 0.1688\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.3371 - f1_score: 0.3946 - val_loss: 0.3589 - val_f1_score: 0.1864\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.3346 - f1_score: 0.5344 - val_loss: 0.3577 - val_f1_score: 0.1864\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.3308 - f1_score: 0.4145 - val_loss: 0.3566 - val_f1_score: 0.1864\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3311 - f1_score: 0.4094 - val_loss: 0.3555 - val_f1_score: 0.1864\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.3252 - f1_score: 0.3945 - val_loss: 0.3543 - val_f1_score: 0.1864\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3199 - f1_score: 0.4130 - val_loss: 0.3527 - val_f1_score: 0.1864\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3186 - f1_score: 0.3963 - val_loss: 0.3511 - val_f1_score: 0.1688\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.3175 - f1_score: 0.3981 - val_loss: 0.3495 - val_f1_score: 0.1688\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3124 - f1_score: 0.5271 - val_loss: 0.3481 - val_f1_score: 0.1688\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.3119 - f1_score: 0.5250 - val_loss: 0.3470 - val_f1_score: 0.1711\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.3116 - f1_score: 0.4039 - val_loss: 0.3462 - val_f1_score: 0.1730\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.3091 - f1_score: 0.4231 - val_loss: 0.3459 - val_f1_score: 0.1730\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.3050 - f1_score: 0.5231 - val_loss: 0.3458 - val_f1_score: 0.1730\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.3034 - f1_score: 0.5331 - val_loss: 0.3456 - val_f1_score: 0.1920\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.3009 - f1_score: 0.6529 - val_loss: 0.3456 - val_f1_score: 0.1920\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2988 - f1_score: 0.5315 - val_loss: 0.3451 - val_f1_score: 0.1920\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.2991 - f1_score: 0.5292 - val_loss: 0.3441 - val_f1_score: 0.1730\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.2965 - f1_score: 0.6387 - val_loss: 0.3436 - val_f1_score: 0.1730\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.2978 - f1_score: 0.5305 - val_loss: 0.3429 - val_f1_score: 0.1730\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2928 - f1_score: 0.6565 - val_loss: 0.3419 - val_f1_score: 0.1730\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.2920 - f1_score: 0.5173 - val_loss: 0.3416 - val_f1_score: 0.1920\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.2915 - f1_score: 0.7831 - val_loss: 0.3423 - val_f1_score: 0.1920\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2868 - f1_score: 0.7792 - val_loss: 0.3425 - val_f1_score: 0.1888\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.2885 - f1_score: 0.6561 - val_loss: 0.3424 - val_f1_score: 0.1920\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.2870 - f1_score: 0.6485 - val_loss: 0.3421 - val_f1_score: 0.1730\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.2841 - f1_score: 0.7981 - val_loss: 0.3416 - val_f1_score: 0.1730\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2837 - f1_score: 0.7923 - val_loss: 0.3407 - val_f1_score: 0.1730\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.2832 - f1_score: 0.6748 - val_loss: 0.3398 - val_f1_score: 0.1730\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2796 - f1_score: 0.8027 - val_loss: 0.3396 - val_f1_score: 0.1730\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2779 - f1_score: 0.7859 - val_loss: 0.3402 - val_f1_score: 0.1730\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2795 - f1_score: 0.7934 - val_loss: 0.3413 - val_f1_score: 0.1920\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2772 - f1_score: 0.7981 - val_loss: 0.3420 - val_f1_score: 0.1920\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2769 - f1_score: 0.7804 - val_loss: 0.3412 - val_f1_score: 0.1730\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2764 - f1_score: 0.7905 - val_loss: 0.3400 - val_f1_score: 0.1730\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2707 - f1_score: 0.7845 - val_loss: 0.3393 - val_f1_score: 0.1730\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.2742 - f1_score: 0.6831 - val_loss: 0.3394 - val_f1_score: 0.1730\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2707 - f1_score: 0.7998 - val_loss: 0.3402 - val_f1_score: 0.1730\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.2686 - f1_score: 0.8165 - val_loss: 0.3418 - val_f1_score: 0.1730\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.2694 - f1_score: 0.8141 - val_loss: 0.3430 - val_f1_score: 0.1920\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.2698 - f1_score: 0.7973 - val_loss: 0.3422 - val_f1_score: 0.1920\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2679 - f1_score: 0.8042 - val_loss: 0.3414 - val_f1_score: 0.1730\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.2688 - f1_score: 0.8061 - val_loss: 0.3414 - val_f1_score: 0.1730\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2675 - f1_score: 0.6831 - val_loss: 0.3411 - val_f1_score: 0.1730\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2671 - f1_score: 0.8196 - val_loss: 0.3416 - val_f1_score: 0.1730\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2641 - f1_score: 0.8082 - val_loss: 0.3423 - val_f1_score: 0.1730\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2651 - f1_score: 0.8042 - val_loss: 0.3430 - val_f1_score: 0.1730\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.2615 - f1_score: 0.8265 - val_loss: 0.3422 - val_f1_score: 0.1730\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2612 - f1_score: 0.8091 - val_loss: 0.3416 - val_f1_score: 0.1730\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.2582 - f1_score: 0.8305 - val_loss: 0.3417 - val_f1_score: 0.1730\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.2593 - f1_score: 0.8210 - val_loss: 0.3427 - val_f1_score: 0.1730\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.2618 - f1_score: 0.8077 - val_loss: 0.3433 - val_f1_score: 0.1730\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.2604 - f1_score: 0.8265 - val_loss: 0.3436 - val_f1_score: 0.1730\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2591 - f1_score: 0.8127 - val_loss: 0.3441 - val_f1_score: 0.1730\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2599 - f1_score: 0.8232 - val_loss: 0.3437 - val_f1_score: 0.1730\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2553 - f1_score: 0.8265 - val_loss: 0.3434 - val_f1_score: 0.1730\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2564 - f1_score: 0.8305 - val_loss: 0.3440 - val_f1_score: 0.1730\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2551 - f1_score: 0.8265 - val_loss: 0.3456 - val_f1_score: 0.1730\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2536 - f1_score: 0.8229 - val_loss: 0.3455 - val_f1_score: 0.1730\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.2512 - f1_score: 0.8288 - val_loss: 0.3446 - val_f1_score: 0.1730\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2535 - f1_score: 0.8186 - val_loss: 0.3427 - val_f1_score: 0.1730\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2510 - f1_score: 0.8367 - val_loss: 0.3422 - val_f1_score: 0.1730\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.2535 - f1_score: 0.7979 - val_loss: 0.3426 - val_f1_score: 0.1753\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.2496 - f1_score: 0.8248 - val_loss: 0.3440 - val_f1_score: 0.1781\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.2520 - f1_score: 0.8265 - val_loss: 0.3454 - val_f1_score: 0.1753\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.2514 - f1_score: 0.8292 - val_loss: 0.3464 - val_f1_score: 0.1730\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.2473 - f1_score: 0.8328 - val_loss: 0.3460 - val_f1_score: 0.1730\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.2493 - f1_score: 0.8265 - val_loss: 0.3443 - val_f1_score: 0.1730\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.2481 - f1_score: 0.8146 - val_loss: 0.3433 - val_f1_score: 0.1730\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.2493 - f1_score: 0.8494 - val_loss: 0.3435 - val_f1_score: 0.1730\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.2484 - f1_score: 0.8165 - val_loss: 0.3442 - val_f1_score: 0.1730\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2459 - f1_score: 0.8367 - val_loss: 0.3461 - val_f1_score: 0.1730\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2417 - f1_score: 0.8556 - val_loss: 0.3478 - val_f1_score: 0.1730\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.2455 - f1_score: 0.8229 - val_loss: 0.3476 - val_f1_score: 0.1730\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2426 - f1_score: 0.8418 - val_loss: 0.3472 - val_f1_score: 0.1753\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.2445 - f1_score: 0.8229 - val_loss: 0.3455 - val_f1_score: 0.1781\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.2426 - f1_score: 0.8367 - val_loss: 0.3451 - val_f1_score: 0.1781\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.2444 - f1_score: 0.8480 - val_loss: 0.3462 - val_f1_score: 0.1781\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.2401 - f1_score: 0.8390 - val_loss: 0.3477 - val_f1_score: 0.1753\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.2403 - f1_score: 0.7769 - val_loss: 0.3479 - val_f1_score: 0.1753\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.2392 - f1_score: 0.8352 - val_loss: 0.3473 - val_f1_score: 0.1780\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.2379 - f1_score: 0.8522 - val_loss: 0.3470 - val_f1_score: 0.1753\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2391 - f1_score: 0.8198 - val_loss: 0.3476 - val_f1_score: 0.1781\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.2393 - f1_score: 0.8339 - val_loss: 0.3479 - val_f1_score: 0.1781\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.2405 - f1_score: 0.8533 - val_loss: 0.3469 - val_f1_score: 0.1781\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.2391 - f1_score: 0.8553 - val_loss: 0.3484 - val_f1_score: 0.1781\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2363 - f1_score: 0.8533 - val_loss: 0.3502 - val_f1_score: 0.1781\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.2375 - f1_score: 0.8353 - val_loss: 0.3515 - val_f1_score: 0.1753\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2380 - f1_score: 0.8064 - val_loss: 0.3506 - val_f1_score: 0.1753\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.2360 - f1_score: 0.8140 - val_loss: 0.3472 - val_f1_score: 0.1780\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2347 - f1_score: 0.8433 - val_loss: 0.3446 - val_f1_score: 0.1808\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.2361 - f1_score: 0.8496 - val_loss: 0.3436 - val_f1_score: 0.1843\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.2327 - f1_score: 0.8678 - val_loss: 0.3461 - val_f1_score: 0.1843\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2333 - f1_score: 0.8957 - val_loss: 0.3507 - val_f1_score: 0.1781\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2313 - f1_score: 0.8358 - val_loss: 0.3526 - val_f1_score: 0.1781\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.2340 - f1_score: 0.8494 - val_loss: 0.3493 - val_f1_score: 0.1808\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.2289 - f1_score: 0.8775 - val_loss: 0.3454 - val_f1_score: 0.1808\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2312 - f1_score: 0.8279 - val_loss: 0.3451 - val_f1_score: 0.1808\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.2356 - f1_score: 0.8709 - val_loss: 0.3465 - val_f1_score: 0.1808\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2322 - f1_score: 0.8507 - val_loss: 0.3493 - val_f1_score: 0.1808\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2311 - f1_score: 0.8585 - val_loss: 0.3501 - val_f1_score: 0.1781\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.2287 - f1_score: 0.8556 - val_loss: 0.3480 - val_f1_score: 0.1815\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2302 - f1_score: 0.8270 - val_loss: 0.3469 - val_f1_score: 0.1808\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.2272 - f1_score: 0.8221 - val_loss: 0.3467 - val_f1_score: 0.1808\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.2254 - f1_score: 0.8221 - val_loss: 0.3467 - val_f1_score: 0.1808\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.2267 - f1_score: 0.8899 - val_loss: 0.3470 - val_f1_score: 0.1808\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2302 - f1_score: 0.8436 - val_loss: 0.3480 - val_f1_score: 0.1808\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.2266 - f1_score: 0.8605 - val_loss: 0.3503 - val_f1_score: 0.1808\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2267 - f1_score: 0.8188 - val_loss: 0.3511 - val_f1_score: 0.1781\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2249 - f1_score: 0.8775 - val_loss: 0.3487 - val_f1_score: 0.1808\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.2252 - f1_score: 0.8656 - val_loss: 0.3468 - val_f1_score: 0.1808\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.2260 - f1_score: 0.8175 - val_loss: 0.3455 - val_f1_score: 0.1808\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2263 - f1_score: 0.8637 - val_loss: 0.3469 - val_f1_score: 0.1808\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.2201 - f1_score: 0.8871 - val_loss: 0.3485 - val_f1_score: 0.1808\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.2223 - f1_score: 0.8628 - val_loss: 0.3492 - val_f1_score: 0.1808\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.2215 - f1_score: 0.8790 - val_loss: 0.3486 - val_f1_score: 0.1808\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.2261 - f1_score: 0.8742 - val_loss: 0.3479 - val_f1_score: 0.1808\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.2198 - f1_score: 0.8890 - val_loss: 0.3477 - val_f1_score: 0.1808\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2202 - f1_score: 0.8830 - val_loss: 0.3478 - val_f1_score: 0.1808\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.2206 - f1_score: 0.8116 - val_loss: 0.3496 - val_f1_score: 0.1808\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2212 - f1_score: 0.8244 - val_loss: 0.3517 - val_f1_score: 0.1808\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2202 - f1_score: 0.8616 - val_loss: 0.3504 - val_f1_score: 0.1808\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.2193 - f1_score: 0.8934 - val_loss: 0.3489 - val_f1_score: 0.1808\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.2183 - f1_score: 0.8875 - val_loss: 0.3471 - val_f1_score: 0.1808\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.2204 - f1_score: 0.8235 - val_loss: 0.3492 - val_f1_score: 0.1808\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2182 - f1_score: 0.8484 - val_loss: 0.3500 - val_f1_score: 0.1808\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2187 - f1_score: 0.8899 - val_loss: 0.3493 - val_f1_score: 0.1808\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.2197 - f1_score: 0.8651 - val_loss: 0.3486 - val_f1_score: 0.1808\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.2190 - f1_score: 0.9091 - val_loss: 0.3486 - val_f1_score: 0.1808\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.2176 - f1_score: 0.8378 - val_loss: 0.3492 - val_f1_score: 0.1808\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.2156 - f1_score: 0.8980 - val_loss: 0.3499 - val_f1_score: 0.1808\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.2142 - f1_score: 0.9115 - val_loss: 0.3496 - val_f1_score: 0.1808\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.2163 - f1_score: 0.8899 - val_loss: 0.3495 - val_f1_score: 0.1808\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2158 - f1_score: 0.9032 - val_loss: 0.3478 - val_f1_score: 0.1808\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2175 - f1_score: 0.9032 - val_loss: 0.3471 - val_f1_score: 0.1808\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2130 - f1_score: 0.8517 - val_loss: 0.3497 - val_f1_score: 0.1808\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.2120 - f1_score: 0.8929 - val_loss: 0.3534 - val_f1_score: 0.2030\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.2159 - f1_score: 0.8710 - val_loss: 0.3523 - val_f1_score: 0.1808\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.2120 - f1_score: 0.8698 - val_loss: 0.3481 - val_f1_score: 0.1808\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2123 - f1_score: 0.8508 - val_loss: 0.3472 - val_f1_score: 0.1843\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.2134 - f1_score: 0.8895 - val_loss: 0.3493 - val_f1_score: 0.1843\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.2151 - f1_score: 0.8176 - val_loss: 0.3525 - val_f1_score: 0.1808\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2148 - f1_score: 0.8890 - val_loss: 0.3541 - val_f1_score: 0.1808\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.2135 - f1_score: 0.8606 - val_loss: 0.3522 - val_f1_score: 0.1808\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.2111 - f1_score: 0.8506 - val_loss: 0.3488 - val_f1_score: 0.1808\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.2107 - f1_score: 0.8901 - val_loss: 0.3470 - val_f1_score: 0.1843\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.2101 - f1_score: 0.8740 - val_loss: 0.3490 - val_f1_score: 0.1808\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.2118 - f1_score: 0.8980 - val_loss: 0.3511 - val_f1_score: 0.1808\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.2118 - f1_score: 0.8794 - val_loss: 0.3518 - val_f1_score: 0.1808\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.2116 - f1_score: 0.8652 - val_loss: 0.3502 - val_f1_score: 0.1808\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2113 - f1_score: 0.8974 - val_loss: 0.3493 - val_f1_score: 0.1888\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.2104 - f1_score: 0.8484 - val_loss: 0.3518 - val_f1_score: 0.1888\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2135 - f1_score: 0.8552 - val_loss: 0.3561 - val_f1_score: 0.1808\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2081 - f1_score: 0.8677 - val_loss: 0.3561 - val_f1_score: 0.1808\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.2098 - f1_score: 0.8698 - val_loss: 0.3519 - val_f1_score: 0.1808\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.2048 - f1_score: 0.9140 - val_loss: 0.3453 - val_f1_score: 0.1840\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2085 - f1_score: 0.8764 - val_loss: 0.3460 - val_f1_score: 0.1808\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.2088 - f1_score: 0.9156 - val_loss: 0.3507 - val_f1_score: 0.1808\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2072 - f1_score: 0.8398 - val_loss: 0.3571 - val_f1_score: 0.1781\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.2066 - f1_score: 0.9082 - val_loss: 0.3568 - val_f1_score: 0.1781\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2096 - f1_score: 0.8790 - val_loss: 0.3500 - val_f1_score: 0.1808\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.2069 - f1_score: 0.8924 - val_loss: 0.3460 - val_f1_score: 0.1878\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2046 - f1_score: 0.9254 - val_loss: 0.3484 - val_f1_score: 0.1840\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.2058 - f1_score: 0.9279 - val_loss: 0.3541 - val_f1_score: 0.2030\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.2068 - f1_score: 0.9061 - val_loss: 0.3546 - val_f1_score: 0.1808\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2035 - f1_score: 0.8724 - val_loss: 0.3518 - val_f1_score: 0.1888\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.2040 - f1_score: 0.9235 - val_loss: 0.3471 - val_f1_score: 0.1888\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.2029 - f1_score: 0.9091 - val_loss: 0.3461 - val_f1_score: 0.1843\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.2044 - f1_score: 0.8830 - val_loss: 0.3483 - val_f1_score: 0.1840\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2033 - f1_score: 0.9006 - val_loss: 0.3511 - val_f1_score: 0.2017\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.2020 - f1_score: 0.9140 - val_loss: 0.3505 - val_f1_score: 0.1808\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.2043 - f1_score: 0.9140 - val_loss: 0.3492 - val_f1_score: 0.1843\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.1995 - f1_score: 0.9180 - val_loss: 0.3503 - val_f1_score: 0.1888\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.2038 - f1_score: 0.8593 - val_loss: 0.3517 - val_f1_score: 0.1888\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.2056 - f1_score: 0.9080 - val_loss: 0.3515 - val_f1_score: 0.1808\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.2009 - f1_score: 0.9206 - val_loss: 0.3531 - val_f1_score: 0.1808\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.2024 - f1_score: 0.8956 - val_loss: 0.3530 - val_f1_score: 0.1985\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.2023 - f1_score: 0.9194 - val_loss: 0.3502 - val_f1_score: 0.1808\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.2032 - f1_score: 0.8764 - val_loss: 0.3507 - val_f1_score: 0.1808\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2014 - f1_score: 0.9140 - val_loss: 0.3475 - val_f1_score: 0.1888\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.1995 - f1_score: 0.9115 - val_loss: 0.3471 - val_f1_score: 0.1808\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.2002 - f1_score: 0.9235 - val_loss: 0.3506 - val_f1_score: 0.1808\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.1982 - f1_score: 0.9249 - val_loss: 0.3509 - val_f1_score: 0.1808\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.2003 - f1_score: 0.9167 - val_loss: 0.3466 - val_f1_score: 0.1808\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1985 - f1_score: 0.9039 - val_loss: 0.3425 - val_f1_score: 0.1878\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1993 - f1_score: 0.9156 - val_loss: 0.3463 - val_f1_score: 0.1808\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1991 - f1_score: 0.9031 - val_loss: 0.3521 - val_f1_score: 0.1808\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1987 - f1_score: 0.9167 - val_loss: 0.3513 - val_f1_score: 0.1808\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.1967 - f1_score: 0.9206 - val_loss: 0.3483 - val_f1_score: 0.1843\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.1990 - f1_score: 0.9294 - val_loss: 0.3471 - val_f1_score: 0.1843\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1980 - f1_score: 0.9197 - val_loss: 0.3482 - val_f1_score: 0.1808\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.1972 - f1_score: 0.9180 - val_loss: 0.3537 - val_f1_score: 0.1808\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.1983 - f1_score: 0.9319 - val_loss: 0.3533 - val_f1_score: 0.1808\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.1996 - f1_score: 0.9345 - val_loss: 0.3506 - val_f1_score: 0.1808\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.1981 - f1_score: 0.9232 - val_loss: 0.3485 - val_f1_score: 0.1875\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1988 - f1_score: 0.8764 - val_loss: 0.3514 - val_f1_score: 0.1843\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1948 - f1_score: 0.9150 - val_loss: 0.3576 - val_f1_score: 0.1808\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.1978 - f1_score: 0.8806 - val_loss: 0.3545 - val_f1_score: 0.1808\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1960 - f1_score: 0.9428 - val_loss: 0.3493 - val_f1_score: 0.1808\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1975 - f1_score: 0.9115 - val_loss: 0.3476 - val_f1_score: 0.1840\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1937 - f1_score: 0.9467 - val_loss: 0.3477 - val_f1_score: 0.1840\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1944 - f1_score: 0.9345 - val_loss: 0.3492 - val_f1_score: 0.1808\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.1938 - f1_score: 0.9050 - val_loss: 0.3529 - val_f1_score: 0.1808\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.1945 - f1_score: 0.9287 - val_loss: 0.3592 - val_f1_score: 0.1843\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.1923 - f1_score: 0.9167 - val_loss: 0.3538 - val_f1_score: 0.1888\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1938 - f1_score: 0.8953 - val_loss: 0.3463 - val_f1_score: 0.1888\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.1956 - f1_score: 0.9433 - val_loss: 0.3447 - val_f1_score: 0.1875\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.1955 - f1_score: 0.9223 - val_loss: 0.3487 - val_f1_score: 0.1808\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1921 - f1_score: 0.9022 - val_loss: 0.3538 - val_f1_score: 0.1808\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1910 - f1_score: 0.8807 - val_loss: 0.3531 - val_f1_score: 0.1808\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1958 - f1_score: 0.9319 - val_loss: 0.3473 - val_f1_score: 0.1808\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1924 - f1_score: 0.9345 - val_loss: 0.3449 - val_f1_score: 0.1913\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1921 - f1_score: 0.9433 - val_loss: 0.3466 - val_f1_score: 0.1875\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1912 - f1_score: 0.9279 - val_loss: 0.3525 - val_f1_score: 0.1808\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1933 - f1_score: 0.9369 - val_loss: 0.3552 - val_f1_score: 0.1808\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1899 - f1_score: 0.9374 - val_loss: 0.3526 - val_f1_score: 0.1808\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1964 - f1_score: 0.9140 - val_loss: 0.3502 - val_f1_score: 0.1808\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1913 - f1_score: 0.9260 - val_loss: 0.3498 - val_f1_score: 0.1808\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1888 - f1_score: 0.9458 - val_loss: 0.3500 - val_f1_score: 0.1808\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1892 - f1_score: 0.9467 - val_loss: 0.3515 - val_f1_score: 0.1808\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1952 - f1_score: 0.9249 - val_loss: 0.3514 - val_f1_score: 0.1888\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1910 - f1_score: 0.9376 - val_loss: 0.3492 - val_f1_score: 0.1808\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1894 - f1_score: 0.9039 - val_loss: 0.3471 - val_f1_score: 0.1808\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1915 - f1_score: 0.9288 - val_loss: 0.3483 - val_f1_score: 0.1840\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1912 - f1_score: 0.9306 - val_loss: 0.3492 - val_f1_score: 0.1808\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1862 - f1_score: 0.9455 - val_loss: 0.3497 - val_f1_score: 0.1808\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1919 - f1_score: 0.9569 - val_loss: 0.3495 - val_f1_score: 0.1888\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1896 - f1_score: 0.9467 - val_loss: 0.3537 - val_f1_score: 0.1888\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1927 - f1_score: 0.9438 - val_loss: 0.3572 - val_f1_score: 0.1808\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1879 - f1_score: 0.9373 - val_loss: 0.3539 - val_f1_score: 0.1808\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1895 - f1_score: 0.9438 - val_loss: 0.3474 - val_f1_score: 0.1913\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1879 - f1_score: 0.9288 - val_loss: 0.3461 - val_f1_score: 0.1913\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1880 - f1_score: 0.9493 - val_loss: 0.3497 - val_f1_score: 0.1808\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1911 - f1_score: 0.9345 - val_loss: 0.3556 - val_f1_score: 0.1808\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1880 - f1_score: 0.8956 - val_loss: 0.3508 - val_f1_score: 0.1843\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1891 - f1_score: 0.9493 - val_loss: 0.3475 - val_f1_score: 0.1888\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1893 - f1_score: 0.9493 - val_loss: 0.3475 - val_f1_score: 0.1920\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1883 - f1_score: 0.9493 - val_loss: 0.3501 - val_f1_score: 0.1808\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1895 - f1_score: 0.9345 - val_loss: 0.3490 - val_f1_score: 0.1840\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1876 - f1_score: 0.9484 - val_loss: 0.3483 - val_f1_score: 0.1840\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.1840 - f1_score: 0.9521 - val_loss: 0.3479 - val_f1_score: 0.1840\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1879 - f1_score: 0.9261 - val_loss: 0.3508 - val_f1_score: 0.1808\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1843 - f1_score: 0.9411 - val_loss: 0.3498 - val_f1_score: 0.1808\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1851 - f1_score: 0.9467 - val_loss: 0.3487 - val_f1_score: 0.1808\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1878 - f1_score: 0.9140 - val_loss: 0.3521 - val_f1_score: 0.1808\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1829 - f1_score: 0.9411 - val_loss: 0.3526 - val_f1_score: 0.1840\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1876 - f1_score: 0.9336 - val_loss: 0.3471 - val_f1_score: 0.1878\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1851 - f1_score: 0.9594 - val_loss: 0.3472 - val_f1_score: 0.1840\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1878 - f1_score: 0.9195 - val_loss: 0.3485 - val_f1_score: 0.1843\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1855 - f1_score: 0.8815 - val_loss: 0.3556 - val_f1_score: 0.1843\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1840 - f1_score: 0.9402 - val_loss: 0.3517 - val_f1_score: 0.1888\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1849 - f1_score: 0.9428 - val_loss: 0.3435 - val_f1_score: 0.1840\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1856 - f1_score: 0.9623 - val_loss: 0.3433 - val_f1_score: 0.1924\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1840 - f1_score: 0.9467 - val_loss: 0.3462 - val_f1_score: 0.1840\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1838 - f1_score: 0.9550 - val_loss: 0.3515 - val_f1_score: 0.1808\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1872 - f1_score: 0.9260 - val_loss: 0.3471 - val_f1_score: 0.1888\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1837 - f1_score: 0.8994 - val_loss: 0.3441 - val_f1_score: 0.1920\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1852 - f1_score: 0.9335 - val_loss: 0.3458 - val_f1_score: 0.1920\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1831 - f1_score: 0.9384 - val_loss: 0.3519 - val_f1_score: 0.1808\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1820 - f1_score: 0.9467 - val_loss: 0.3535 - val_f1_score: 0.2030\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1850 - f1_score: 0.9039 - val_loss: 0.3491 - val_f1_score: 0.1840\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1828 - f1_score: 0.9402 - val_loss: 0.3457 - val_f1_score: 0.1920\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.1881 - f1_score: 0.8942 - val_loss: 0.3462 - val_f1_score: 0.1562\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1845 - f1_score: 0.9441 - val_loss: 0.3541 - val_f1_score: 0.1808\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1860 - f1_score: 0.9289 - val_loss: 0.3575 - val_f1_score: 0.2030\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1838 - f1_score: 0.9374 - val_loss: 0.3494 - val_f1_score: 0.1840\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1842 - f1_score: 0.9272 - val_loss: 0.3465 - val_f1_score: 0.1562\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1820 - f1_score: 0.9515 - val_loss: 0.3482 - val_f1_score: 0.1562\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1805 - f1_score: 0.8994 - val_loss: 0.3532 - val_f1_score: 0.1808\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1827 - f1_score: 0.9541 - val_loss: 0.3571 - val_f1_score: 0.1808\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1824 - f1_score: 0.9734 - val_loss: 0.3511 - val_f1_score: 0.1808\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1830 - f1_score: 0.9550 - val_loss: 0.3442 - val_f1_score: 0.2003\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.1839 - f1_score: 0.9433 - val_loss: 0.3457 - val_f1_score: 0.1888\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1804 - f1_score: 0.9467 - val_loss: 0.3504 - val_f1_score: 0.1808\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1832 - f1_score: 0.9230 - val_loss: 0.3540 - val_f1_score: 0.2030\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1825 - f1_score: 0.9254 - val_loss: 0.3487 - val_f1_score: 0.1840\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1803 - f1_score: 0.9580 - val_loss: 0.3447 - val_f1_score: 0.1924\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1784 - f1_score: 0.9521 - val_loss: 0.3448 - val_f1_score: 0.1958\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1794 - f1_score: 0.9592 - val_loss: 0.3494 - val_f1_score: 0.1888\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.1805 - f1_score: 0.9373 - val_loss: 0.3532 - val_f1_score: 0.1888\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1813 - f1_score: 0.9538 - val_loss: 0.3559 - val_f1_score: 0.1843\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1798 - f1_score: 0.9484 - val_loss: 0.3520 - val_f1_score: 0.1840\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1782 - f1_score: 0.9104 - val_loss: 0.3487 - val_f1_score: 0.1924\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1819 - f1_score: 0.9623 - val_loss: 0.3484 - val_f1_score: 0.1979\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1815 - f1_score: 0.9467 - val_loss: 0.3504 - val_f1_score: 0.1840\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1786 - f1_score: 0.8819 - val_loss: 0.3530 - val_f1_score: 0.1808\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1789 - f1_score: 0.9493 - val_loss: 0.3561 - val_f1_score: 0.1843\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1801 - f1_score: 0.9249 - val_loss: 0.3515 - val_f1_score: 0.1888\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1786 - f1_score: 0.9428 - val_loss: 0.3470 - val_f1_score: 0.2014\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.1782 - f1_score: 0.9521 - val_loss: 0.3485 - val_f1_score: 0.1979\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.1787 - f1_score: 0.9475 - val_loss: 0.3529 - val_f1_score: 0.1878\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1770 - f1_score: 0.9594 - val_loss: 0.3555 - val_f1_score: 0.1843\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1794 - f1_score: 0.9484 - val_loss: 0.3566 - val_f1_score: 0.1888\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1799 - f1_score: 0.8902 - val_loss: 0.3521 - val_f1_score: 0.1530\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1821 - f1_score: 0.9269 - val_loss: 0.3487 - val_f1_score: 0.1875\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.1779 - f1_score: 0.9458 - val_loss: 0.3548 - val_f1_score: 0.2100\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1805 - f1_score: 0.9550 - val_loss: 0.3558 - val_f1_score: 0.2062\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1790 - f1_score: 0.9133 - val_loss: 0.3479 - val_f1_score: 0.1875\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1786 - f1_score: 0.9521 - val_loss: 0.3486 - val_f1_score: 0.1562\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1779 - f1_score: 0.9441 - val_loss: 0.3524 - val_f1_score: 0.1530\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1794 - f1_score: 0.9343 - val_loss: 0.3536 - val_f1_score: 0.1808\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1832 - f1_score: 0.9373 - val_loss: 0.3539 - val_f1_score: 0.2062\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1764 - f1_score: 0.9654 - val_loss: 0.3489 - val_f1_score: 0.2049\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1802 - f1_score: 0.9873 - val_loss: 0.3473 - val_f1_score: 0.1979\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1783 - f1_score: 0.9873 - val_loss: 0.3513 - val_f1_score: 0.1840\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1752 - f1_score: 0.9580 - val_loss: 0.3584 - val_f1_score: 0.1808\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1784 - f1_score: 0.9515 - val_loss: 0.3490 - val_f1_score: 0.1808\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1758 - f1_score: 0.9467 - val_loss: 0.3426 - val_f1_score: 0.1924\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1782 - f1_score: 0.9484 - val_loss: 0.3429 - val_f1_score: 0.2003\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1802 - f1_score: 0.9540 - val_loss: 0.3530 - val_f1_score: 0.1808\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1801 - f1_score: 0.9348 - val_loss: 0.3557 - val_f1_score: 0.1843\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1756 - f1_score: 0.9830 - val_loss: 0.3499 - val_f1_score: 0.1888\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1763 - f1_score: 0.9540 - val_loss: 0.3455 - val_f1_score: 0.1701\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1766 - f1_score: 0.9611 - val_loss: 0.3474 - val_f1_score: 0.1878\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1750 - f1_score: 0.9634 - val_loss: 0.3561 - val_f1_score: 0.1808\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1754 - f1_score: 0.9484 - val_loss: 0.3578 - val_f1_score: 0.1808\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1757 - f1_score: 0.9580 - val_loss: 0.3493 - val_f1_score: 0.1562\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.1757 - f1_score: 0.9594 - val_loss: 0.3459 - val_f1_score: 0.1600\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1755 - f1_score: 0.9663 - val_loss: 0.3473 - val_f1_score: 0.1840\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1737 - f1_score: 0.9623 - val_loss: 0.3537 - val_f1_score: 0.1808\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1752 - f1_score: 0.9306 - val_loss: 0.3467 - val_f1_score: 0.1840\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1726 - f1_score: 0.9821 - val_loss: 0.3440 - val_f1_score: 0.1878\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1714 - f1_score: 0.9800 - val_loss: 0.3452 - val_f1_score: 0.1878\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1732 - f1_score: 0.9791 - val_loss: 0.3513 - val_f1_score: 0.1840\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1740 - f1_score: 0.9515 - val_loss: 0.3506 - val_f1_score: 0.1875\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1780 - f1_score: 0.9402 - val_loss: 0.3510 - val_f1_score: 0.1875\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1750 - f1_score: 0.9550 - val_loss: 0.3494 - val_f1_score: 0.1840\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1734 - f1_score: 0.9705 - val_loss: 0.3486 - val_f1_score: 0.1840\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1711 - f1_score: 0.9475 - val_loss: 0.3472 - val_f1_score: 0.1875\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1725 - f1_score: 0.9717 - val_loss: 0.3473 - val_f1_score: 0.1875\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1709 - f1_score: 0.9817 - val_loss: 0.3473 - val_f1_score: 0.1840\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1729 - f1_score: 0.9873 - val_loss: 0.3485 - val_f1_score: 0.1920\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1716 - f1_score: 0.9646 - val_loss: 0.3485 - val_f1_score: 0.1875\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.1693 - f1_score: 0.9873 - val_loss: 0.3475 - val_f1_score: 0.1840\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1733 - f1_score: 0.9873 - val_loss: 0.3484 - val_f1_score: 0.1840\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1740 - f1_score: 0.9594 - val_loss: 0.3491 - val_f1_score: 0.1808\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.1693 - f1_score: 0.9521 - val_loss: 0.3493 - val_f1_score: 0.1808\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1723 - f1_score: 0.9457 - val_loss: 0.3488 - val_f1_score: 0.1888\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1699 - f1_score: 0.9592 - val_loss: 0.3486 - val_f1_score: 0.1878\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1730 - f1_score: 0.9550 - val_loss: 0.3506 - val_f1_score: 0.1840\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1736 - f1_score: 0.9765 - val_loss: 0.3504 - val_f1_score: 0.1840\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1714 - f1_score: 0.9765 - val_loss: 0.3480 - val_f1_score: 0.1878\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1715 - f1_score: 0.9734 - val_loss: 0.3484 - val_f1_score: 0.1600\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1721 - f1_score: 0.9567 - val_loss: 0.3515 - val_f1_score: 0.1843\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1717 - f1_score: 0.9433 - val_loss: 0.3557 - val_f1_score: 0.1808\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1724 - f1_score: 0.9515 - val_loss: 0.3486 - val_f1_score: 0.1875\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1733 - f1_score: 0.9743 - val_loss: 0.3455 - val_f1_score: 0.1600\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1708 - f1_score: 0.9817 - val_loss: 0.3441 - val_f1_score: 0.1913\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.1714 - f1_score: 0.9844 - val_loss: 0.3464 - val_f1_score: 0.1840\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1739 - f1_score: 0.9791 - val_loss: 0.3500 - val_f1_score: 0.1840\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1711 - f1_score: 0.9623 - val_loss: 0.3527 - val_f1_score: 0.1843\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1703 - f1_score: 0.9515 - val_loss: 0.3520 - val_f1_score: 0.1562\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1704 - f1_score: 0.9873 - val_loss: 0.3488 - val_f1_score: 0.1562\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1675 - f1_score: 0.9567 - val_loss: 0.3476 - val_f1_score: 0.1840\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1699 - f1_score: 0.9654 - val_loss: 0.3519 - val_f1_score: 0.1840\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1699 - f1_score: 0.9748 - val_loss: 0.3531 - val_f1_score: 0.1843\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1712 - f1_score: 0.9831 - val_loss: 0.3452 - val_f1_score: 0.1562\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1716 - f1_score: 0.9690 - val_loss: 0.3448 - val_f1_score: 0.1562\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.1706 - f1_score: 0.9589 - val_loss: 0.3474 - val_f1_score: 0.1840\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1677 - f1_score: 1.0000 - val_loss: 0.3534 - val_f1_score: 0.2062\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1710 - f1_score: 1.0000 - val_loss: 0.3470 - val_f1_score: 0.1840\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1662 - f1_score: 0.9904 - val_loss: 0.3464 - val_f1_score: 0.1562\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.1701 - f1_score: 0.9654 - val_loss: 0.3507 - val_f1_score: 0.1530\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1652 - f1_score: 0.9861 - val_loss: 0.3572 - val_f1_score: 0.1808\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1704 - f1_score: 0.9765 - val_loss: 0.3509 - val_f1_score: 0.1840\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1705 - f1_score: 0.9484 - val_loss: 0.3461 - val_f1_score: 0.1701\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.1690 - f1_score: 0.9688 - val_loss: 0.3457 - val_f1_score: 0.1701\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1681 - f1_score: 0.9623 - val_loss: 0.3521 - val_f1_score: 0.1808\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1665 - f1_score: 0.9580 - val_loss: 0.3561 - val_f1_score: 0.1843\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1699 - f1_score: 0.9683 - val_loss: 0.3494 - val_f1_score: 0.1562\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1661 - f1_score: 0.9896 - val_loss: 0.3453 - val_f1_score: 0.1701\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1676 - f1_score: 0.9844 - val_loss: 0.3464 - val_f1_score: 0.1600\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1682 - f1_score: 0.9654 - val_loss: 0.3567 - val_f1_score: 0.1808\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.1689 - f1_score: 0.9580 - val_loss: 0.3504 - val_f1_score: 0.1808\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1681 - f1_score: 0.9515 - val_loss: 0.3438 - val_f1_score: 0.1840\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1693 - f1_score: 0.9688 - val_loss: 0.3427 - val_f1_score: 0.1646\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.1698 - f1_score: 0.9762 - val_loss: 0.3488 - val_f1_score: 0.1843\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1666 - f1_score: 0.9926 - val_loss: 0.3539 - val_f1_score: 0.1808\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1700 - f1_score: 0.9611 - val_loss: 0.3496 - val_f1_score: 0.1840\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1660 - f1_score: 0.9553 - val_loss: 0.3454 - val_f1_score: 0.1701\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1660 - f1_score: 0.9970 - val_loss: 0.3460 - val_f1_score: 0.1600\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.1679 - f1_score: 0.9904 - val_loss: 0.3520 - val_f1_score: 0.1843\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1694 - f1_score: 0.9802 - val_loss: 0.3569 - val_f1_score: 0.1843\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1710 - f1_score: 0.9765 - val_loss: 0.3436 - val_f1_score: 0.1878\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1679 - f1_score: 0.9844 - val_loss: 0.3424 - val_f1_score: 0.1979\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1664 - f1_score: 0.9817 - val_loss: 0.3483 - val_f1_score: 0.1840\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1674 - f1_score: 0.9873 - val_loss: 0.3564 - val_f1_score: 0.1888\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.1676 - f1_score: 0.9861 - val_loss: 0.3500 - val_f1_score: 0.1562\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.1668 - f1_score: 0.9839 - val_loss: 0.3459 - val_f1_score: 0.1920\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1679 - f1_score: 0.9636 - val_loss: 0.3455 - val_f1_score: 0.1979\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1666 - f1_score: 0.9904 - val_loss: 0.3474 - val_f1_score: 0.1840\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1671 - f1_score: 0.9720 - val_loss: 0.3508 - val_f1_score: 0.1808\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1650 - f1_score: 0.9830 - val_loss: 0.3501 - val_f1_score: 0.1530\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1660 - f1_score: 0.9926 - val_loss: 0.3472 - val_f1_score: 0.1562\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1668 - f1_score: 0.9867 - val_loss: 0.3439 - val_f1_score: 0.1562\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1662 - f1_score: 0.9844 - val_loss: 0.3454 - val_f1_score: 0.1924\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1643 - f1_score: 0.9904 - val_loss: 0.3527 - val_f1_score: 0.2100\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1655 - f1_score: 0.9904 - val_loss: 0.3545 - val_f1_score: 0.1840\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1634 - f1_score: 1.0000 - val_loss: 0.3525 - val_f1_score: 0.1875\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1671 - f1_score: 0.9318 - val_loss: 0.3520 - val_f1_score: 0.1562\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1647 - f1_score: 0.9580 - val_loss: 0.3492 - val_f1_score: 0.1878\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1639 - f1_score: 0.9821 - val_loss: 0.3511 - val_f1_score: 0.1878\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1665 - f1_score: 0.9873 - val_loss: 0.3540 - val_f1_score: 0.1840\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1660 - f1_score: 0.9830 - val_loss: 0.3466 - val_f1_score: 0.1840\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.1647 - f1_score: 0.9970 - val_loss: 0.3483 - val_f1_score: 0.1530\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1665 - f1_score: 0.9830 - val_loss: 0.3533 - val_f1_score: 0.1888\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.1654 - f1_score: 0.9800 - val_loss: 0.3506 - val_f1_score: 0.1808\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1671 - f1_score: 0.9904 - val_loss: 0.3440 - val_f1_score: 0.1924\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1643 - f1_score: 0.9750 - val_loss: 0.3425 - val_f1_score: 0.1979\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1684 - f1_score: 0.9904 - val_loss: 0.3483 - val_f1_score: 0.1840\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.1639 - f1_score: 0.9580 - val_loss: 0.3507 - val_f1_score: 0.1843\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1676 - f1_score: 0.9550 - val_loss: 0.3468 - val_f1_score: 0.1530\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1656 - f1_score: 0.9371 - val_loss: 0.3408 - val_f1_score: 0.1920\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1624 - f1_score: 0.9970 - val_loss: 0.3396 - val_f1_score: 0.1924\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1617 - f1_score: 0.9791 - val_loss: 0.3445 - val_f1_score: 0.1840\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1634 - f1_score: 1.0000 - val_loss: 0.3451 - val_f1_score: 0.1875\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.1612 - f1_score: 0.9861 - val_loss: 0.3449 - val_f1_score: 0.1600\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1624 - f1_score: 0.9913 - val_loss: 0.3457 - val_f1_score: 0.1600\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1635 - f1_score: 0.9904 - val_loss: 0.3496 - val_f1_score: 0.1530\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.1621 - f1_score: 0.9830 - val_loss: 0.3506 - val_f1_score: 0.1843\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1634 - f1_score: 0.9800 - val_loss: 0.3460 - val_f1_score: 0.1646\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1619 - f1_score: 0.9904 - val_loss: 0.3459 - val_f1_score: 0.1878\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.1622 - f1_score: 0.9904 - val_loss: 0.3492 - val_f1_score: 0.1840\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1616 - f1_score: 0.9676 - val_loss: 0.3504 - val_f1_score: 0.1808\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1627 - f1_score: 0.9580 - val_loss: 0.3446 - val_f1_score: 0.1562\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.1636 - f1_score: 0.9873 - val_loss: 0.3460 - val_f1_score: 0.1562\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.1649 - f1_score: 0.9589 - val_loss: 0.3480 - val_f1_score: 0.1875\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.1623 - f1_score: 0.9861 - val_loss: 0.3478 - val_f1_score: 0.1840\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "g3AB5mJePAJy",
        "outputId": "8183891a-e717-4990-a014-f4fd24186027"
      },
      "source": [
        "plt.plot(history.history['f1_score'], color='blue')\n",
        "plt.plot(history.history['val_f1_score'], color='green')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fdc12100150>,\n",
              " <matplotlib.lines.Line2D at 0x7fdc121002d0>,\n",
              " <matplotlib.lines.Line2D at 0x7fdc12100490>,\n",
              " <matplotlib.lines.Line2D at 0x7fdc12100650>,\n",
              " <matplotlib.lines.Line2D at 0x7fdc12100810>,\n",
              " <matplotlib.lines.Line2D at 0x7fdc12100a90>,\n",
              " <matplotlib.lines.Line2D at 0x7fdc12100bd0>,\n",
              " <matplotlib.lines.Line2D at 0x7fdc12100d90>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOxdeXgURfp+K5nJTUJCIAkEwn2piIKIKHigggqueC3qeizisa73sd6K7rqK66p4u/rzWmXRxQVRQVFAVOQUue8r5A65Mzkmc9Tvj0p1V/d09/RcySTp93nm6e7q6qrqnu63v37rq68IpRQWLFiwYKHjI6a9G2DBggULFsIDi9AtWLBgoZPAInQLFixY6CSwCN2CBQsWOgksQrdgwYKFTgJbe1WcmZlJ+/fv317VW7BgwUKHxK+//lpBKe2pta/dCL1///7YtGlTe1VvwYIFCx0ShJB8vX2W5GLBggULnQQWoVuwYMFCJ4FF6BYsWLDQSWARugULFix0EliEbsGCBQudBH4JnRDyHiGknBCyQ2c/IYS8Qgg5QAjZRgg5OfzNtGDBggUL/mDGQv8AwFSD/RcAGNL6uxnAm6E3y4IFCxYsBAq/fuiU0h8JIf0NsvwOwEeUxeFdRwjpTgjJoZSWhKmNHR7NzcCnnwLXXQcQwtLefx+gFJg4EXj9deDoUSA3FygtBZxOYP9+wO0G7HbA4QASEoCmJrnM2Fi2r7lZTktKAhobzbXJbmdtaWkxzkcIkJjI6omPl9uQmAikpADHjsl1NzUp25mVxdpZV8fOBWDn7PHI2wArp0cPoKQEsNmA5GRWBqWsXo9HWUevXkBDA8tHCCufEJY3IUG+5gkJLJ1Slr+lhaVnZLBj4uKAmBj2i4sDamtZ/ampLJ/Dwc4zJkZue0MDS3M4WP1paeyaNzQALhe7rgCry24HunUDqqvZdmws25ecLP+nMTGsXH7NbDbWZrFOr1dep1TO09TE/pO4OFY/P4+YGFafzcbyA+yeopS1QdxHCCvfZmPXOTZWvkdjY1lZzc3s/0pMZHk9HrbucrEy+D0XEyOX4/XK5dvt7HhKWR7+f/IyvF75HHk5/NfSIrdbvB78uvH2c3i9cvv5deLH8Py8Dfza8HZx6EUU5+3g/wNvK6/fbmf7m5vl/YTIbeBt9nqB0aOB337TricUhGNgUR8ABcJ2YWuaD6ETQm4Gs+LRr1+/MFTdMfDYY8A//8lIa9o0RoKzZrV3qyKPggL/eYLB0aOhHV9ba7y/tNR4f3U1W9bVsZeQEaqqfNMaGtjS7MvXCOILPRD4e5H7O0ar7R4PI2g1xJe3CIdDO50TvgiRtLW2RXAy1WqL2fL1oD5W3Na6piL5i3l37jRXX6Bo005RSum/KKVjKaVje/bUHLnaKcEfek4kwTxMAHDNNezm2LxZTnvkEZbWowfbPuUU+SbS+y1cKB//wQfGeZOTlW244w7guefY+sUXA8OHAyedpMwzZQqzgm+/Xf9cVqxg5d9zD7Ni9+3Tz/vjj8CYMcryA0VGBjBkiPa+667TP27qVPlaPPQQS+veXTsvtwxDwc8/+6YtW8a+2KINF17IvjBFpKREvt6hQyNfR6Rxww2RKTcchF4EoK+wnduaZiFKIX5exvi5A9T7+acwwCwgcVudx8jq4cfwfP7ymvkkNoLRefLPebPQs/zCgbg43zStaxwN4BKPiGC/GAJBYmLk64g0kpIiU244bpMlAK5r9XYZD6DW0s8jA24BipagOs2MlRguQne7uyah631hhcNC5xq8iGgmdLWcoievhBPx8ZGvI9KIFKH71dAJIf8BcBaATEJIIYAnAdgBgFL6FoClAC4EcABAI4A/RqapFsJF6CKB+SMz9f7YWDnN5VJuq/NoaZXqcnk+f3kDJV2tMvReBOGy0MNB6FoWemws8PXXoZcdbsTEAPX18nZCQttY6Hrae0dCuxE6pfQqP/spgD+HrUUWdBFtFnpHk1z0LOtQXxYckbTQ33479LLDDYdD7uAFWN+Cvw7lcKAt6ggG3JvFDNR9U+FCFH7IWYg02orQjazu9iB0PesxXHJGpCz0mBigpib0ssMN7q7JEej5T58eXL2i+240IZDztwjdQoeTXIxIWpRcuG+zUV6xDrMuZuoynE7fdEKi30KPjfXvatkeaGpSui+K5G4GaWnB1xuNCMTQsAjdQtRJLuHqFOVlGeUV6wiG0GNitAldbEOoCAeh19X5plGqnd7eaGhQfvUEqm13Bm8VEYEQeqTcOy1C70CIhIUeKcnFX6eomtCNXAH5iEWOYDwp9AhdHJWpBfEh9ffAhoPQR4zwTQvHAKRIgBM6P+9AX7SdwVslWESz26KFNkIkLPRISS7+LHRRcuFlGbU3VMmFD7tWI5ySS6RcC6OV0JuagvsvODqShZ6aGt7ytPpKwgGL0LsgokFyuf9+ZfmBSC560okRjF50RlPbNjQAw4ax9n75pXEdkdJ2Z8+OTLmhgodACPbLRByxHAhsBr554fhK0kK43TGNziEUWITegdDROkWNJJfly5Xl+5NcxDqC8fjQe3ERAqxcqX9cXR0LS/Dyy8CuXcZ1BON9YwYVFebyhduK9IdQz/fw4eCOy8jQ3+fP8g3maywx0dfl9YQTzJWdlMTCW6hhEbqFqOsUDcUPXV2+P8lFrMPoRREo/F2vQ4fCV1ekcc01+vuiaaRpqNo5j1ukBS0voVDrHjjQN02rU1OLpOPigMcfN5c3HIiiv9mCP0QbofuTXMwQbzCSSzCEHuynOPfcCOdLJFIwsj4j9fUQTNmhvFwIAfr21d/vr7PRqK16mr5WMLbt233TtAwYmw1YssQ3PVz9NmpYhN6B0NEkF38WuhjnOhDJJRgvF73rEk2Wa6h47TX9fW1F6Gbuv1D6GiiV5Tot+HMHNKpbb9/w4b5pWi6aWvdwRYV2ZMVQOpONQGgk/2kDjB07lm4y6o3Swd3f3I0tpVsi0KLIYfduoLyc3RhZWaxTb926wMvJyWGhQ5uagA0bWNqAAUC/fqw8pxNI6w6MPtG4HIcD+PVXtj56tPEAj02blANGBg1in627drGHNz0diIkFKo7JeXr3ZjE+7HbteOAcEyey0MIHDrBrs2ePdr7TTmN5+GQaNjvgDjDiod7kH4EM17YQPPr1AwoLQycyf/9Xt1Sg3sBnX6tvJy4eaHGyMM42m9zZCwD2OGD8qeze2bdPGbtm7FhWFp+oQqvsMWNYe7dvVxL+FRNH47NZLxufrA4IIb9SSsdq7YuQkmOhvWBGWdCy8M0WyGdgAeQZYHyKIOznjyjF2WX8kqpQSTAErGfVW1zeNkhKCo8HCokBqInxDUb71aQb09ouu91Xgyetx6Sk+H6Jxscr69M6P/7FoG5XxCQ8Smm7/MaMGUO7Cq6+mk2R8PHHbLugwN8UFNq/W29lx+/fL6c9+yxL69+fbZ9zjv/27NghH79unXHeMWOUbXj9dUoXL5a3L7mE0quuUua56y5KJ0yg9Nxzjc+ntJTSt99m63yp9Ssvp/Saa+Tt5OTArx0hwV1z6xfYLyZGO/3LLylNTdU/LjvbXPnp6cb7p07V32ezUZqT45s+dChbXnYZpTffrNzXu7f8LJx/vnKfy0Wp2y1v9+jhWzbHwIHK9CVLzPOHGgA2UarNq51IQew4oDS446KtU1RrW0zzZ4U4nW3n5RLsNY8GdKQBOHqSSmqq8b1pdhYif26JRl4s6gFqHNzjJD7e93jxvlFb7zab+T4YtVdLpMYsWITeDgiVXLQejEA+Z8M19F9rm6eZ6RRtbg5u6H9H8DgJJ9piWrdIw5+PvFl3Qn8vNz5JuBZstsAJXSufCLPPnfrYSI3+tTT0dkCoFrpRWlsO/dfaBlgHEP+4NMJrrwH5+WzdaJ7Vf/xDeV6hTAEXFxf8nK7thY5ioRt1WKalGd+bZs8xVELXMmC45a1F6GKb/fm4G0FN6IFGpjQLy0JvA6hv8s4uuSxfDhw86N+SfvVV2UfXaNTg3/9u7C0TCIwGpUQDtCzEth4BGiyMyDYnh3m66MGIiEX4m1veH6Fr+alnZzOyHjMGGDcOyM3VPl5vcnAtxMYCN96orFtEpL4yLULvQIg2P3StbQ6PJzAXNX+hV8Nl0aiHbMfEACefbO5YkWx79TJfZyCDSLKyfNOildDV7q5GZJuQwFxc9WA2+mCfPsb7jTR2u127nhNPZF9ts2cDF18MzJwZWhsB9nJ79115W03os2aZLysQWITeBlCTbGe30AFzGroIf/G+w2Wh5+X5ppnVqEWrKlLR8kQ/52iH+v/1d+8Z3fdmJRd/191oSL3Npi3Xme3Y9JfP6PzU7bKG/ncidKZOUT78Xw2HI7BZdn76yXi/GBgrlNGdAwYotwkxbwGLvuyRInRxUAtHpCIIhgq1bBAKoZu1fv0RoT9C14qaGC5CNzJgLELvxOhMnaIlJdpl1NaajxIIaBOZCDFkbihxMAYP9k0LZiq0SD2Q4YKRvBEuBBqCIVoJ3ez95C9fIIRuxXKx0C6Si7o8tYVeWwtUVmofG0zccj2EGsuFQx0q1+tlHbiBIpBzi8QXmRpqQoxkjBozrqZaaG9Ct9sja6EbdXSq2xWpry6L0NsB7a2hm5VcKNUYJq0i9Opq4D//0T4+0DkmjSC2IxSCfOst5TalwcXV4e6WZqBluZn16gCA887zn+eMM5Tbei/ZYKDuqOXue+r/gd8XalmL32/jxunXYXbSZJvNOB56WxH6pEm++8Vn77LLzLcrnLAIvR3Q1oR+002yTvzAA0BmpryPP2yvvsqOvfJKOV7LffcB33+vLOvyy1mwLA5/Ukk0Qe+h0uoo5TjxROD114FFi4A33wRWrGAz7axapcx3++3ARRdpl9GjB7B4sbz98MPA3r3AX/7CtsV5RF99FVi7Vtmn8Je/AEePytvXXAN88w1QUCCnzZ3L8vDIgFqeO1rzlQLGn/9PP80Co61ZI6eNHOmbLyaGeQFt3gz897/Kfby/4aqr2PKVV9hS9Boy6ph+8EG2TEhg/6G6fBF6//GsWUpCf/JJeV8wksuHH/ru51+Ozz4LvPOOuXaFG1GuBHZOtPUwdO4+VVcHvPCCch+3Op54gi3Fh4U/eEYIZvag9kKvXkBxsW96//76FndWFnDbbf7LHjAAKC3V3hcfD5x/vrx93XXMJ5tbmqKGP3QoMH48sH+/nGazsQiWHKmpwJQpyjri4liccE6MahfC+Hj9vgIjqSA1lZUpWsVaHcIJCYzQTjpJf0wBr4e3TZStjL5YeLubm9m10OrE5oOa9IgzNVWpoYshcYOx0LVeQLzsjAztMAFtActCbwe0pYUu1rVzp2+ZRjdzZxtir2cFGpGJntWthj9SEEmQSxj8GPE/4laeup9D7DfQknDU9asJJC8vtI5SraiCoqXOCR3wvW94e/l+revtb0CQuK5Fjtx61iNO9XFifcEQur/OVzNpkUCHs9BXrtSeASSawWOP//vfwMaNwcsUP/wA3H23cpDN//7HpkkrK2PbO3eyPByiFTRnjm+ZTz6pHyu8s0Fv6LZe/I7SUvMDiPyRQmwsk0hcLplYOQkFSuhaBoFaNlCf67JlwLffsmUg4HVpnd9NNzFL94ILlISufuFwgo8kofP2BUPowUguFqGHCTt2AB980N6tCAycLH/8kXXABWv57tjBPDLEB2b9emDLFnlASkGB7/XJymJtWL9eXuf5FyxgD0O3bqxcu511Ztrt7KbPzNS27Dsi9GZu1yN0rVGbejAiBW7RqoeUG1no6vICtdDVhJ6eHlhHLAdvu9b5xcXJ3ikJCfL1VbfP7WbnGElC18qrTrcs9CjEnXeyX0fCNdcA8+ezjpJrrmEzGGl1LPnDnXeyQFVlZSz+BMC2b78dGDWKBcW68ELgiy+My3G5ZAkgP9/4U/yCCzoPoeuFLA3HIKFg3ATNWuhiOqBN6P4sdHXESrMwstBjY5UkbTT/qtcr59V6gUaa0GNjlfvENqivnZ4kGoqFHim/czUsDb0dEE0Di0IhomiG1kOlJyu1F6Gb1dDFdCA4DV0vRIM/GBG6KAUZSS4A28f3a4Wx7WgWutEz0J4WukXo7YD29kMPldAjNew9XCBEuwNUj9DNxuI2QjAvubaUXPQmd/AHI8lFJPT4eP1OUYDt4+lapBwqofNr2FaEbvScRT2hE0KmEkL2EkIOEEIe0tjfjxCyihDyGyFkGyHkwvA31UK4CF3ME8xDHkpc6LaAHqHrxUGPtIWu958EK7kE0ykaaclFJPRgLHSje8pMdE+tvCLUdRpJLiLEax1KiICoIXRCSCyA1wFcAGAkgKsIIWoF+DEAn1FKTwIwE8Ab4W5oZ0J7W+gi2stCDyQUaTAwO/IQiLyFrvefaFno3IpVlydavW1poXPoWei8XQkJ8rpW+zweJaEHEtskEpKL+J+Ha+i/URuiqVN0HIADlNJDAEAIWQDgdwCE+HegALi7fxoAjeEbnQ9OJ+vcLCw0zsdv9NmzgT/8gQ0sCQWhRlsURyEmJrLA/X/5C/DII+zB4iPq9NwZAwm6pYdwxnlRIyZGO356fLx2vS++qF1OfDy7rvyYxER9D6XZs/WDMxUUaL80eP6tW+W0P/2JdXKr2yGS/n/+4ztakk8eweOrvKEyqYJ9CT/wAPCQ6pt8/Xq2vPFGuV3durHrFB+v/SWUnS2fb1ycb3uMXqpiXq1jATaYp6xM/zzVxxlJLuKgLHFSDrOErtWGtpIpzRB6HwDCIGMUAjhVlWcOgOWEkDsAJAM4V6sgQsjNAG4GgH5G05d0ENTUMB/w88/XnyRh9Wo2lBuQ3bo++ii4+sLVKfryy8rtmhpG5gAjLE5aofqmG/m3R3LQUloaG26vftGOHi2TkYgJE7TD9957LxuKzwm9qYkNQ+fX+Lnn5LwXXADs28eG9KvRrRvw5z/7pm/fDnz9NRv0c+utjGT4aFOnE3jpJbkdYn3XX89cKt98UybPP/2JvXA++ICVMXas9rkCjLDuvpu9yCorgRkzgKlT2Sjap55S5r3jDtklkd83OTks74UXMoMmI4PFLhk6lF0j/oKcNIkZMIWFcjt79gQGDmQvnM2bWTiCxEQ2EnbBAuD004FnnpFj7mzYwMp96il2z1x8Mft/33yTvbwcDlZGSgrwyy/A5MnsvD0edl9XVbH/5LLL2P/cty9zH+3RAzjuOObBpf46uPdeZuSkpCjj6Oh9Rfz2GwvvvHs3u1bjx/vmmT2bGUojRhjP3BQyKKWGPwCXA3hX2L4WwGuqPPcCuK91/TQw6z3GqNwxY8bQjo7iYjZz5ltv6ed54gk+u2bov0ceYWVWVsppb7/N0k4+mW1ffrn/dl96afjapPez2Sjt1i3y9Wj9cnIoHTfON/2007TzP/+8djqllPbtqzwnEWLe//yHXXutcvr10/4fPvqI7T/3XN99DoeyHWJ9W7aw7aFD5bTqapY2dizbfvRR/eszdCjLO3Ik2160SPucAEobGlh6ba2cNn48Wy5Y4NvupiY53/LlxvehHtau9T33SOCss1gdH39sLv9bb7H8hES2Xf4AYBOl2rxq5iOiCEBfYTu3NU3EjQA+a31BrAWQACATnRzcyjT6FAvnp1YkOkUjBZstsBmLwglKtUO7BjqwCPD93NeDv6iVgR5jZp/YHn9ui0btMqqLl6PntqiGqN0HG8I3kqF/Q6mP54tmt10zp7IRwBBCyABCSBxYp6d68P1RAJMBgBAyAozQj4WzodEIvQ4sEdFI6HoEE07YbO0bC0YrZrrevKRGLnPhIHQ9GN03RuXx48T2BOI1Ekg7jAhd67hA5qsNpj2RQKDeKx2a0CmlbgC3A/gWwG4wb5adhJCnCSEXt2a7D8BNhJCtAP4D4IbWT4NODW6BthWhc4TaKdoWaE8LnRBtQteLzx4OCz0YcjZD2kbHGVnoZgid3zNmvgb0vFyM0Fkt9GiercpU0yilSwEsVaU9IazvAnB6eJsW/WgvycUozQyxBzrTTDCw29uP0AHLQg+XhW7UHn/HBUvMloUePKyRoiGgI0ouX3zRNpNStKfkQmlghB5pC10PkdTQAyF0M8QbjIUeLPFZFnrwsAg9BAQquYQqi4RK6KtWAZdcwtyrRAQzSTKH3sNgt5vT6tURCMMFveHnWojGTlEjtGWnKIfWfdVVJRfLQu8AaGhgPrS1tWyAQlkZI2y+zn+iDtvWkguHkfSiHtRRWcnOo6lJngWHh87l6NsXQUPL5xYwb8VEInKmnoWuB6P/SNxnZPWGMvQ/UGhJLqFY6GbaoXUO/o7rrJ2i0WyhR3HT2gaLFgGXXipvx8QErv3GxrLBRd9952uNhYPQ+fRaWlb4rFnAuefKaZ9/7vvw3Xwz8NVX8vRratLv14/FWg8Gv/yinX7okLnjzQ7R1xvlqQWvV98qzsz0HemamKhfltkvrFCG/gcKLQtdjXB1ipppR7D7w31csLAkl04E9Yz1Xi8bDcilgF692Ki2N94AbrlFu4zYWEbmWoiE5KKGv3jlGzcyMj+1dXyvOLT5jTeAefPYaMb2gNb8kGr8+CMbsWgWXq++fv/73yu3e/c2lpzMvpDbW3JRI9ySi1E7gt0f7uOChSW5dCJoPWwzZ8oyRE4OG1b9pz8x/VkL7TmwCGDkZfSyOHKELU84gS1F623QIGDw4OBmswkHRELXO4eJEwPT+cVAUGp0767cTkwMT6d2KPHQA4WW5KJGuCWXYI6zJJe2R5cndC15xW6XP8PFz3G9T3OjGyIcoWb9dXzW1hoTOvdq4e0UvT30PD/aCuKXQbhejGI8Gn/lEGJcr/j/GXXyBiO5BPvFxttrdG9Zkot5mP0fOoLkEsVNaxtoPaQ2m0zeYpjXYAi9LTpFKyvN3ZSffcaWtbVyGg+e1R4+4/Hx5iWpQAld73y0PFqC6dAMpIxwI1waurq8YNvR1uWGG4H+x5bkEsXQevBtNpnIRRLXi+EdLiLyB716KivNeXVo+Z9XVbFlTU3w7QoWakIP15cOpfoWupa0FI4HtC1H6poZ4NIWkkuk/NCjlTCtgUUdAFoWuii5mLHQjT7F20JDN0voWigpYUs9Ahw3LrhyzUBN6MEOhVfDqFNU/X9QGh6L0OgeCHcQDN7eYL1uOELtFPWHjmKhm4VloXcA6EkugVjo4lD6SLktai05KiuDH5XJXRn1EEm9UD3pgFFdgVjAgUou0fyAaiHYofqhlBcMOsrQf7OwOkU7AMJhoYvWsdpSFgkrVEutro4tw2mh+yP0SD5cakIPV11G/QGBdoqahdELJ9xyTLBD9dUIdtpCs+goQ//NoiN0ikbppWs7hNtCVwe+CoeFzgnq22/ZMhBCP+ssttTTVNvTQrfbI0PogUpgZjtFoyWipRnJJRpI0ZJc2h5ReunaDv7cFkVC0yNFceIE9SjMcGroWnM1TpjAOjb1IiieeiqbzkvvZXT0KLBEHd1eQKQJXZRAwlVXoIQezQ+oFsy0NxrOqbN1inKuiGYLPYqb1jbQs9C1/jQ9i0iM7xIJC52DW+FiO1JT2bD99HR5n3hOc+cal1lfD/zud/r7I/lwqS30tnhQArXQoxFm2hsN59TZLHT+/EUzoUfppWs76LktBgIx2FVbE3q3bkxb5/XyfdOnyw/GpZeyCYybmpjfudZgopSU8LXTLGw2i9CDQbRMM6gHbgR0VkKP1i8IwCJ0TQs90D+Md1YCkZVc9AgdkAcIcYwZIw+r79sXGDKE+WAnJmrLL+1BADabUsaKxCAsNTqD5BLthM5fzJ1NcuGeZJaFHsXQIvRAHwaR0NUWejhuTt5GM4TO8/btK+8zEwfFbCTDcCI2VvlwWBZ654DRPKRmEK3/hyW5dADodaAF4mJoROjhtJS0fM05aau/DPLy5AfDTERDrQ7XSCMmRnl92ovQo8V7pbOAGzGhxqqJNnQEySWK3zWRxaJFLORtUZHvvlmzgG3b2PqGDcArr7B13vGoxkcfyeuff87KzcwEevQI759fUcFil4sTUrz9tjIPfxHddZd8bq+8IucLdHaiH38Mrq1msH8/u9Yc7UXoFsIL/j8GO9gtWgm9I0guoJS2y2/MmDG0PZGdTSmjP9+fzUZpZqb2vvvv1z9O7yfW1bMnpf36BV5Ge/wIMZcvOZnStDRKu3dn185mozQmhv1sNkrtdpYvLk7eZ7Oxa9G3L9uOjaX0gQcoTUxk6xkZ7LgLLmD/V2kpS7PZWLsSEthxmZksX2oqpUOGUJqUxI632dh17t6d5evTh9JRoyitraX0ooso/ec/KR0xgtJvv6XU46F00CD2P+XmUpqeTulHH7F6d+5k6T17UrpqlfIemjeP5R00iNL6ekq3bKE0L4/Sd9+ldOxYSh9/nF2XZcu078HaWkoHDqR03Trt/ddfz8rgeO89SidNkrcPHGDnd8klctrq1ew6NDSwvOefT+l551Haowdr25AhlC5fzvJ+/z2lw4ZR2tQkH//BB5SecQal48ezfSKuvZbSOXMo/eknSgcPptTh0G73kiWUnngipS6X9n4zGDeO0vnzgz/eDNauZde/rs5c/spKSvv3Z/9zewLAJqrDq12W0BMT9Qnq+OMp3bBBex+l8vrDD8vrgwfrl3f11crjv/8+ciQcF6edPnIkpZMna5Pxtdf6niNHRgalEyca1zltWtv+dxYsdGUYEXqUftxEHkZD5QkxF61O/PTSk2O0QKn5vIFC7zM3JUU7dEFiovEUbImJ2lEaRUSzpmjBQldClyV0vZGVgHlCFzt9OgKha4WOTUgwnq0oIcE/oUe1pmjBQhdClyV0IxCiT1IiYYqdNxkZ+uWJBM6FirZGcnLwhO4vVnq0dmJZsNDVYD2KGoiJ0bfQ9aQaIwtdJPCmpuDbFQpCkVzUI0v5teHn3B6zHVmwYMEXFqFrwEhyEaUa0TI1InTRqnc42ocAQ5Fc1OBfL3zCZSP5yoIFC22HLkno/mKHmyV0EUaELo7CbGgAjh0zrj8SiI/XJ+dACZ33HXBCb49BSRYsWPBFlyR0f8PcjTR0kdDFTlEjDV2Ms+Jw+O9kjAQI0SfuQAmdg7/Egp1cw4IFC+FFlyR0MX65Fows9GA0dLE+h0OemDjR1H0AACAASURBVLmtoaWVE+JfQ9cDP2dLcrFgITpgitAJIVMJIXsJIQcIIQ/p5LmSELKLELKTEDI/vM0ML/x1TIZbclETentY6IC2tU2pnG6zATNn+j+Gw9LQLViILvj1ICaExAJ4HcB5AAoBbCSELKGU7hLyDAHwMIDTKaXVhJBekWpwOODPQjfyctGTXAIh9MpK/22MBPxJLn37Av/5j7ljAMtCt2Ah2mDGQh8H4ACl9BCltAXAAgDqOW5uAvA6pbQaACil5eFtZnhhRnIxo6GL7ohGhF5YKK+/+SawYoX/NkYCoUgu4lRxHJaGbsFCdMEMofcBUCBsF7amiRgKYCghZA0hZB0hZKpWQYSQmwkhmwghm461h6tHK8xILnqhP91u4IEH2Lroftirl34kQ3FgznffAaWl5tvqD1z24IiJ0Sbfyy7TThclFy3wfVqTYliSiwUL0YVwdYraAAwBcBaAqwC8Qwjprs5EKf0XpXQspXRsz549w1R14DBjoevB5QKef54RIfcv93oZ8ZWXs7IpZa58w4cDfYRX3/PPs+X06YG116g9w4axJSfc6dOBO+5Q5lm/HjjzTP2YK2YIXXwZ8C8Ti9AtWIgumCH0IgBCBG7ktqaJKASwhFLqopQeBrAPjOCjEqESOofXq5ykIS5OJj67nenlYrx1Ppl0Xl7gbdbDaaexJZ8TdNAgX+I2Imx/+/k+rWtiaegWLEQXzBD6RgBDCCEDCCFxAGYCWKLKsxjMOgchJBNMgjkUxnaGFWY6RfUgkpfHY5y3tla5zaWXHj2M61fDKPZLZiZb8naoZwECZB1ci5RFDV1rv9Y+vs4lJktDt2AhOuCX0CmlbgC3A/gWwG4An1FKdxJCniaEXNya7VsAlYSQXQBWAXiAUtpOvhz+YUZD14NIXh6Pvozh9QL19cq0igq2NBqEFCw46RPi+wLgbdR6MVBqvJ9b6OoAY4A8/Z1loVuwEB0wFfiUUroUwFJV2hPCOgVwb+sv6hFOyYWTodcL3HIL6/AsK5PlFRFffcWW4SR00TIHjH3otV4+cXFKyUgN0UddTGtqkmUea05OCxaiA10ykrW/2CNmCV2UXIqLgXffZet9+wIFBb7H9u4N/PGP2h4jZjB2LLBpE6uTe9iMGQPceSdw6BB7YRDCvHAqK4GbbgI+/RQYMIDlnT4duP12pvPPmMHmP737btbeBx9Uzu/JcdZZLO/FF7MBUWPGMO38v/8F+vcHHn0UuPrq4M7HggUL4UWXJHRxslct/TcQQudWb36+nH799cBzz/mWfdNNwL33Av/7X3DtvvRSRuhDhwJ79rC02Fhg3jzg97+X86WmAm+8wdZPOklOj4sDXn1V3j79dHn9uee06+zfX7u9J5zAln/7W8CnYcGChQihS8Zy4YSu5ZcNmNfQRclFJPS8PO2BRlx7DnaCC95BOXSob1vVSwsWLHQ9dElC56SsR37BeLkcOSKn9+sn+2jzjkMRoRI69z0HfM/BInQLFrouuiSh6827yWFEik4n8MQTrOPT42Gdg/n5SkLPyZEJXRxYFKyFzjs5ExKA114Dbr3Vt62WhW7BgoUuTeh65GdEirt2AX/9K9PDjx1jE1bccINM6BdfzCzod94BzjuP5R04kO0L1jLnsg4hwJ//LJcnttUidAsWLHTJTlF/kosRKXIPGYdDtpw9HjYi9IorgM8+Y2knnggsX87WFy1iXiihauhGsIjcggULXdpC14PZTtG6OrZMTWWyS//+xuUGS+jioCE1LAvdggULHBaha8CoU1Scvo6PBK2tZZa7HqGrCTxYC92I0I3yWLBgoWugS0suejAiRT4/6LZt8kQVP//MlpGy0I2sb8tCt2DBAkeXtdBjY5XxzEUYkSKPA6OedahHD+Ccc4zrjYR2zmERuQULFixC14AZCx1gZXDf8Gee0Q9Dy8sLtVPUstAtWLBghC5L6DabvpZuRIpiYK+UFPmlMHiw+fojQehGHacWLFjoGuiShO52M+s6GELfsEFedzrlMgYN0j9GbZlbnaIWOgsqGivw4HcPwu21guJHA7okoWtJLmJwK+7l8v77wPjxwCmnsB+glFy6dQOuuQaYOZMN9/cHS3Kx0NnwwHcP4PlfnsfiPYvbuykW0EW9XLjkIhL6J5+wCShWrJBJ8YYb2A9g84VmZSnL+ec/gWuvNV9vJDtFLVhoD3gpe4jqnfV+clpoC3RJQueSiwhCjK3c8nLfNDHqoRF4eSUlwNNPA1VV5tuqVY5RmmWhW2hLJNlYcP9GV6OfnBbaAl2S0LWmjvNH6GVlvmlDApwG+9//ZiEDtELrmoHVKWoh2pBktwg9mtBlCd2mOnNClNO4qcEt9MxMYNIkIDfX/FRynGz5tHTV1YG3Wa9dloVuoT2RaGd+u01uPxP1WmgTdElCV0suastca+g/J/Q9e9ggIn/Yvp1FZKyvZ4G7QoE1UtRCtCLR1kroLovQowFdktDVkosZUiwvZ8eYlUueeAJYv943fdIkVv7q1YG12YKFaIQ9loUctSSX6ECndVusqmLEySduFqGWXNREvmgRm/yYEGDNGjaX59//DvTs6Wu9l5XJ3jLHjrGynU59S/n554HrrgvsXMxEW7Q0dAvtAe5/bkku0YFOS+gHDrAlnyxZhD/JpbmZETgALFwoxzVXuy2WlADZ2WzYf10d0KsX8Je/sBfAokXa7erXzxpYZKHzwOVhczJaFnp0oNMSem0tW2pNBK0nuWhp54cOsVmJAEbYIn76iS3feYdZ8gDwyivA0qVsPSGBkf6FF8rHZGVZhG6h88DlZYTuaHG0c0ssAB1QQ1+0CPjwQ//5CgrYct06YMIEJRmvW6eMyeLxAJdcAmzc6FsOt84BYPdulo9jzx62PHYMeOABti6G5rXbmQWfkiKnGcVa14PVKWqhLTFv3TyM6zMOp/U9DS+tfQmbSzfj9QtfR2p8qk9eLrnUOesU6Y2uRjz8/cP42zl/Q7d4jZnS2wDP/fwc9lTswRsXvYH52+djQPcBmDxwsrS/uL4YL697Gc9OfhbVzdV45sdnMPe8uXC6nXh81eN4dvKzkhePEZbuX4qKxgpcd2KAWmoE0OEIvbZWOSGzHsSBQGvXsinhOBobGfEOGMAsaKeTldmo8dUoEn9Dg7LuhATg9NNZSF2Ph704nE6mzyclAY88wvJxi5yH17VGjFqIZtz97d0AAPokxb3L7wUAzBo9C2cPONsnL5dcGlwNivQ3Nr6BVza8gtT4VPz1nL9GuMXaeHjFwwCA2065DTd9eRMAdk4cs5fMxrIDy3DB4Avw4dYP8eHWDzE+dzy2lW3DvPXzkJeWh3tOu8dvPRfNvwgALEIPBuJwfCPcfz8bms+xZYu8fv75zJ1w7VpmWb/+Ots/Ywaw2CAkxbRpwEcfBd5mTuDjxyu3Az3e6hS1EGk0u5s10/U6Pbnk4nQ7Fek8JIBeeW0J/tJRo8XDJgh2eV1SHwAFldI7YsCxTquha43s5BA1dEplEhRju2hJI8Fa1la0RQsdBdVN2qPe9Do9OVmqidsWw2xFTvjtCU7QasTGMBLweD3wUBY2NZbEgqLjfkJ3WkLfv1+5LZK16LYoEroop/zhD0Dv3iyi4rx5QHIycNddwbUlVEK3NHQLbYWqJu1AQ3qEzq1Yp0dpoXNCby8r1+OVY2PrEXoMYfTnpV7pi4KndVR0OMnFDFpalBILwKaM69mTrbvdsveLnoWu7ni9887Q22Vp5xaiHZzQE2zK6bf0RoLqSS72GDbgSCTWtoT4gtH7SoglrRY69Ujt5Fa7WfAXQbSgY7+OdLB9O+ucFDF3rryully4vOKK0NehJblY6CjghJ5kTwIVblRdyYUTepRZ6OILRq/t3BpvdjdLxKynt+uhtrk2yBZGBqYInRAylRCylxBygBDykEG+ywghlBAyNnxNDBzcnfCll+Sp4T7+WN6vJ7nwCaBHjQpve9QEbk1BZyFaUd3MNPRke7LCsvWnoastdNJ6I7aXhi6+YPRitXNrvNHVKGnoTo8TBOYfIn69AChegO0Fv4ROCIkF8DqACwCMBHAVIWSkRr5uAO4CoBHBpG1x9Chb3nwz09LnzGGdpNxqF0eKer2+hH7aaeFtj2WhW+go4BZ6clyyQnsOVEPnRB8NFrreoCcuuTS6GiULXf1i8gexzyEaOoDNWOjjAByglB6ilLYAWADgdxr5/gpgLoB291M6epRFRExioZqxeTNbDhokzwOq5eXC/dDDTYptQehdoVP05XUv45wPzwn6+HpnPXJfzMVP+T+FsVXRgas+vwpzfpgjbb+x8Q2c8d4ZAICPtn6E0W+NlvYdrj6M7Bey8eGWD9HvpX4Y/MpgXP7Z5QBkgoqPjVfID9xt8d3N7yL7hWws2bsEAPDDkR8AMOLeV7kPA+YNwJ+++pNEbtzybQu8tuE1THp/EgCVhd6itND/8L8/4NEVj0oWepOrSdLQxeM4yb+3+T1kzM3A/O3zfeoUCd3pduK5n5/DJQvk0YfbyrZh4vsT22wkrRlC7wOgQNgubE2TQAg5GUBfSunXRgURQm4mhGwihGw6duxYwI01i6NHlXN8LmH3HoqKWIwXLcnl2DF5iH+kSDEKvsg6NO759h6sOrIq6OO3lG5BUX0RHlv1WBhbFR1YsGMBnlr9lLT956V/xpoCFo/i+sXXY2vZVkkS+HDrhyhrKMP9392PgroCHKw+iM93fw5AHvHZ4mnRtNDXFKxBWUMZNhSx2dJFyWFzyWYcqTmCt359q118ue9Ydgd+Ospe1qKlLUouXurFJ9s/wd9//rukoSskF7dTclvk5P7d4e9Q3VyNa/53jU+dIlE7PU48vOJhfLH3CyltQ9EG/Hz0Z+yt2Buu0zREyJ2ihJAYAC8CuM9fXkrpvyilYymlY3tyl5MwY/lyYNcuoG9f7f1HjiglF06yoldLMMPzjWBJLhaiAWY8MjhBOT1OTULnaVqugDXNNdJ6ew/O0bPQRX95bpU3uhoVFrokI7W+FIzmSxXL05Jr+P6yBoOBMWGEGeoqAiDSY25rGkc3AMcD+IEQcgTAeABL2qNjdP9+YMoURtrHHaed59AhX8mlqoqNGOVpkZJc9LbNwuoUDQ3c8oqGzqv2gD+N10u9MqG7tQmdyzAuj8vnOmoReqBeI+GCwkIXCF3sC+Ckr7bQ+bF8v5FcIrpzii8Rfm34/lJHaXAnEiDM+KFvBDCEEDIAjMhnAria76SU1gLI5NuEkB8A3E8p3RTepvrHvn1suXChMogWR2KiTOhcchF9z7t1A2pqOqaGbpTHQueGka+3SLr+rGWn2ynFZGl2NysInWvo/KXg8rp8LH4tQm+vof9ivaKFLRI6z9PkbpJIvNndLBEzTzMidLEe9XqiPVFKixpCp5S6CSG3A/gWQCyA9yilOwkhTwPYRCldEulGmsWhQ2x5xhm+k0ADwMCBwOHDTHJpaQH69FEG8UpLi05Ct0aKhgeBuKN1JBjFIhc7JTmh612HJneTQnLRclsULXT1C0L0yW7POOle6tWVXMSXDifsRlej1E6nxymRsCkLXYhxo/asSbQnSvujhtABgFK6FMBSVdoTOnnPCr1ZweHwYebZoo5bzjFwoGyhFxezH9fLs7JYtMT8fKtTNNrhpd4OP0Q7nDAiHFHy8Cd/NLub/UoukobubfGRcGqcNT7524PQXR6XbqdoeYNswYmSi0TobqdsoZsgdIWG7lESes/knlGpoXcI3H8/sGABC4mrR8gDBrBRpBUVwG+/sVjlfI7QJ55g4XCBjqmhG+XpbAhGly2pL8GLa19UpO06tguPrHhEkiWcbidu+/o2VDZWSnmONRzDbV/fFpJ08Namt/DVvq9M5a1qqsIV/70CL/zygmG+zSWb8djKx0ApNSQc0Yr2J7k0uWQL3eV1SeecZE+SCI8Toj8Lva6Fecs0uhrR5GrCrV/divKGcrz/2/u4ZMElOFR9yLAt+TX5uHPZnVh5eCWmzZ+GH478gPWF6zFt/jQs27/M8Fi3161roSsIXcdClzT01mVDixwaeNXhVZg2fxp+zP8RgEpDF14ic9fMxaWfXiqdZ1tZ6J2C0A8eZKFyk5OB2bP18w0cyJYtLcySf+ghNgkFwCx0o5mLQkGokosVPleJYDwnlh9cjsV7lbGRJ380Gc/+/CwqmxiBz98+H29uelOKow0Aj658FG9uehP/3fnfoNs7d81cvLFRYy5EDWwo2oCFuxbige8eMMz3ybZP8MxPz6DOWacgdLX3iWhF+/MNFyUXQCayjMQMKX1r2VapLPWLVZQzuPujo8WBDUUb8Pavb+ObA9/gqdVP4Yu9X+CbA98Ynt9tS2/DqxtexU1f3oSv93+Nz3Z+hgU7FuDr/V/jg60fGB7r8rp0BxaJxCpa4KLUpLbQxa+Mf2/7N77e/zU+2fYJAKXkIr703/71bSzaswjfH/rep95IosMF5/r2W+Dzz5VpBw+y5bJl8lB/LWRksKXNBvz+9yx64guthhAhkdOhrU7R8CIYQhdn1OHD0tVuZpzoxPI5QYYyQKbOWYfCusKA22mEwnpWXmFdoQ8JxyXGSdtaFjonHrWXiii5ALJlm5mUiaO1RxV5m1xNvha6U7DQW89DPPfCukKpzJL6EsPz45Iat3DrnHUSwRbXFxse62OhC5KLeCz//yubKiViVni5uJkLY4tXfkmWOFi7ix2sHJHExfPn4J3MZY62kVw6HKEfPAh8pfH1On26MZkDzKVx7Fg2lRzvND35ZKC0FDjrLPZCAKJPQzfTnq5A5BzBDLE2IspI+kpTSlHnrDMdlU9sJ6VUevmoUVDLxvoV1BUo0htcDUhPTJe2tTR0LhOo28Qll+4J3VHTXCMRYWZSJnaU71C8AOqd9b4aumChc/mlwdWA/Np8AMCBqgPSyEp/pJwSl6LYrnPW4VjjMVPHihp6gi1BIblwQgZky7uoTvbCVlvootwi1s2XooUuSnVq1Dpr0eRqMjWlXSjocJLLbbfJHZrib4kJX5tevdi8oV6v7LaYns7inmdkyFKLZaFHN0K10DmkAFKtRMe9P7S8QIINA9vsbobb60ZNc42p4d9iO9XTuokQrV6xXHUdhhY6fP3IvdSLHok9AMgWeo/EHnB73Yr2NLgafP4HLckFYH0VALC+SA7zxC1cPSTbkxXbdc466SVWXF9sOJZAtNC7xXVTvLjElwG3qEXCFy109ReLeDxfNrubpfuFS3d6aIuO0Q5H6OGAXiyXSEsuettmj7c0dIZgOkXVlq8IvQkQRAQbi0OsV7QE9aDoWNT5qvB4PRKh+CN0LQ2dW5X8OsTFMImGW8A9kloJXbDQAWBf5T6pLC1CF7VmUX7ghM6XKXEp/q1slfVf1VSFEkcJku3JaHY3K0IOaB3LSVlt6Yv1anngKCx0t9PnelY0ViCGxKDMUQa3140mVxPSEtIAGFvoQNvILh1OcgkF777LvFtaWrTjobdbp+jUu4HM3cC2a4FtfwBO/BAYuRCo7w3s/D1QygJS/VL5JZ6f/xaePutpAGMAAJ/t/z/8vPYLxNrmARjQJQjdrIX+6IpHMWPEDIztPVbyuuDweD3S57QRoXMrVk2ujhYH7v32Xvx98t+RbE/GrCWzkJeWh+fOfQ4A8MIvL2BU1igM6D5AOqagrgDDMocpyllXuA5L9y/F02c/7VNPnbMOu4/txoIdC1DaUAoCgnlT5yEuNk7S9AtqCyTCBYBNxZsUMV1uWHyDtH7rV7di5vEzJQudW66p8amoaKpARWMFAEgWOm8LL58TMsACfC3cuVD3umlZ6Bzj+ozDmqNrMOXjKZrHzho9y+d6763cCy/14tTcUyXPl/TEdLi9bvRM6qn4Orhz2Z0Y2ZMFhE2OU1r6/l4kq4+sVsRyeW7Ncz55xvYeiw1FG/DsT8+i2d0sSVQVTRWaZfZK7oXyhnLM/nI2enfrDQC4Y9wdmDZ0mmFbgkGXIvSbbpLXzz+fLdvSQtckdOIBxs9rXfcyQj/1VaD3ryxt7L9AnmcHfFf2MVaVL8X4PuPBCf3lLXNQ0liI0xMvR1chdDMaek1zDf7+898xb/08OB5x+BBEfm2+ZIkV1RdhRM8RmuVwK059/EdbP8I7m99BSlwKLh95ORbsWAAAePrsp2GPsUteKptukgdMa3WMnvZ/LFbzg6c/iOS4ZB9CP/ff5yryXzjkQpyUfZJcZn0h+qTKsfLe3fwufi35VdrmAboAJnmUOkpxXC8WF4MTV0ZSBiqaKpBfw7TuIRlDsAzLUNrAPDN6JrG4SyIxt3ha8NE23xnT7TF2uLwuxUtSHVr3rlPvQrO7WfMLZNexXXB5XJIcdlb/s2CPseO7Q98BAGYeNxMrD6/E2sK1ICDI6ZbjQ9Jf7/8aZQ1lSE9IR3wsm5osLjYO6QnpPrJHXlqepPGL690TuqOysRLfHvhWkf+s/mfh4TMexpSPp+DTnZ+iR1IPZKdk40jNEew+thsAMCJzBEb2HImtZVtxoOoABqYPRHlDOXaU70BNcw1yU3MjFhKhS0ouubnAmWeydTEeeqRHW2pKLbGCdRjb+ie7EzQyApTPqiIQmpsya9VL2j8Wc1vBjIXOyYnrvj6E3rofkDsYtSB6a4gQdVaxrMK6QoWWKh5n5OnCvUjELwl1nbYYG/Jr8qVyBqYPRGFdIfJr85EWzz77RTIHgBS7LDkk2ZMUEg230HNScgAwKxgAJuWxELT7K9nEvFoWeveE7lJ+gGnVABQdsqLcMTCd+QynJ6Tj4mEXY82sNVh741qf34VDLkR+bT7qnHW4YPAFWHX9Kpycc7JUzqm5p+LJM58EAGSnZOOe8fcA8J0LdFPxJuR1z0NcLJOT+qX1Q/eE7lDjhKwTfNaHZw7HPePvQYmjBCWOElw36jopz6rrV+H8QefjrlPvQn5tPpNc4tOQnZItXfvVN6zGwisXIiORudX1795fOv7RiY9i7Y1rMWPEDJ+2hANdktBFSUW00NulUzRWIOIYl3IJAC3yJ6ObysOu1fCitWOvK1joJqwbbmlxiOTY4mlR7OeeIlLwLqGzkGvaapc0Ttouj0tR1tHaowqCF48zInReRp2zDom2REXdHH1T+yK/Vib08bnjGaHX5OP4XsdLpC7ipBzZmh+VNQoe6pFcAbkVnZWSBUAm9DP6sTjq+6sYoXNNfeexnVJZ6rk387rnAWCEzcGlG95WAMhNzdW9BgCzkgtqC1DdVI3U+FQAkJb8+Ly0PKkNfF2vLHusXVoXy+E4odcJPusJtgRFuZnJmT7H5aXlwdHiQImjBIn2RCl/oi1RegHy+pJsSYrjIokuSehinJd27xQVyZuTu0jy9b2l/B5O6IKFzpvalQg9EAudo85ZJ0187PQ4Fft5Z6WWJ4uehc4JuKi+SFFWfk2+guB5R1hqfKqPi6HYOcvLqG2ulUiPd1Jy5HVnkkBBXQESbAk4MetE1DnrsL18O/K650mkKkIkrON7Hg9AfrHw65iVnCWlp8anIislC1nJWZKmzgnqQNUByaNDbRH3S2MTEIgWuqjtM5nQHKG7vC4crD6oSejpCelSXfyaqJFkT5LKEi10Xo74otEidPF81PnV+wvrCpFgS5C2+6X1k+QiXp844bZWe8OJLqWhc+hZ6IES+pK9SzB/+3wMzhiMv53zN918+bkvAJdvwnfdgZkLgRTPrQDOYjv9Wej1vaVVQwudRI7QF+9ZDEqp4jOxprkG9317HxpcDchNzcW0odPw9q9vg1KKeFs87hx3J17d8KrUAZcSl4L42Hi/rl1m8OD3DyrIQgvby7dL6//e+m/UOeskrfNQ9SF8vP1jEBBQUCzdvxQzF86UJIYfjvyAmQtnApCJe1PxJikNAFbnrwbAOjW3l2/HyJ4jsevYLry07iVpgmQAmLee9Y8c1/M4rCtcpyhDHKz02sbXsOrIKuyp2IMxvcdgf9V+vLL+FcU55aXl4fPdn6O4vhi5qbnom8qiWlc1VSEvLQ/1znpsK9uG43sdjx3lO1i9veQ40ryjUA3xWnILMq97nqQ3a11rtWsnP0600BWEbtZCFwhPTeixJBaEEEUZWhZvSlwKGl2NyOueh10Vu6R83DNmWOYw/FLwi7TOMSBd7sAW25GRkGHYzkRbokT64ouAt1v0PY+0hW4RegheLi/88oI0Q8ojEx+RLAM18vP+CrhiUGnLwue7D+IEmgCZ0DU09NgWoCkdSKwGXElSu7iFruWV4SUtinMIJ2Z8yoicPilblCsPr8R7W95DZlImKhorsOrIKuws34l+af2wv2o/dpTvwOaSzRicMRhurxtHao4AYHqt1qdvINhXuc/UyMuTc07G5pLN+Ofaf6KysRIn9DoBR2qOwOVxIZbEIt4Wj2Z3MygotpRukQa9VDVVYUvpFgDsAe2R2AMVjRVSGgCkxaehf/f+qGmuAaUU15/IZgX6tZjpqKOyRiGWxKLR1Ygpg6bgyuOuxPNrnleUwfPZYmxoaGnAltItyE7JxtXHX40EWwIOVh3E6OzRSLIn4fFJj6PZ3YwNRRvgpV5cedyVmNB3Ak7OORlOtxNTB0/FsB7DcKj6EB6b+Bhmfs5eHCKB9E3ri8kDJqOwrhA9k3tiQ9EGtHhakJ6YjitGXoFtZdsw83h23MzjZqK2uRbDMochJyUHUwdPxeHqw5LrYgyJwfDM4dhTwWZknz50Olbnr8bM42fC7XXjaO1RXDbiMthj7XC0ODAqaxRuGH0DLhmuEddawCm9T8EpvU+Bo8WByQMmA2BeMaOzR2P60OkAgEEZgzBj+Aw8MOEB9EruhZnHz8RNJ9+EyR9NltqysXgjzhlwDuJi41BSX4LzB52P3NRc7K/cj3vG3wO3143jeh6HoT2GYkLfCeiX1g8nZZ+EaUOn4a9n/xX90vrh/EHng1Kq+OrgGNlzJCb2m4hjjcdw3sDzkJ2SjWUHluGKytxm7QAAIABJREFUkVdIeaYOmooNRRtwdv+zcXb/s7Fw90Ifr5uwg1LaLr8xY8bQtgKjbfk3fLi87/LLKR0xgq3fdRfbP2eOuXJzXsihmAOKOaDVTdW6+WIeT6A47wE6ezalQ14ZQkc9dZXcnvQDUhm4YwhLu2sAxYw/UNx4GsW159KkJJb3hJcnUMwBvfGLG6Xje83NppgDOvrOpylA6QcfhHChNOD1eqX2eb1eKX3uz3Mp5oB+ufdLaf8FH19APV4PTfhbAsUc0Li/xlG3x01L60ulPKsOrwq6LbyMlYdWmj7mli9vkY77/X9/TzEHdOJ7EymllGbMzaCYA7p492JKKaXP/fQcxRzQWYtnBd3GaAE/52/2fyOtL9y5UJFnxGsjKOaAvrLuFVNluj1uqaysf2TRr/d9LW1HA3hblh9YHtZy52+bH13nycKWa/Jql9TQ9SQXDjNWbkNLA0ocJZJfqVFHnZe4AE8cKEWrD7GGbu6xKyUXr52lCZKM2+uroXvhleuIAEQ3LzHA0IGqA+iZ1BNjcsZIaYPSByGGxGBQ+iAAwIDuAxAbE4teyb0UeUJFICNFh2QMkda1vBwA32nV1CMoOzKMoi1yrdfs9RS/DFs8Le02vZw/hLtd0XqeWug0kovLBaxYwaae05xPtPcmIJV9pjtygcXsSxFF3YD6Pmz7WGx/AKNBCCMsrkMCTCebPHAySh2l2FS8SfrkH5E5AsX1xbqDU7zUC8R4GDkDsMfa0YwWwNYEDFwBpLdGFvPYgfh6YPhiIM4BdCsCEmqA2Ba4By8GWoBaF+ukOlx9mOUD0NTqJ11t3wkMX4xfG4C0PSFcSBXE0YEfbf1I0hw3Fm/E4IzByE7JlvYPzhgsLXce2ylti/FIRJ/pYGHkh76xaCMoKMb1GadoEwDJC2R3xW6sPLxS6pQsri/GV/u+kganuLwubCvbhlFZo4Jq3y8FvyDRlqjwMFHD4/VgR/kOnJh9IgAWrGpzyWZMHjhZ6kSjlOKHIz8gOyVb108eYO6EA9MHIsGWgN9KflN0yopujPy6eakXKw+vlP5bl9eFraVbUd9SL3m4AMwH/2jtUQzPHA6n2ylFWuTHaBkxxxqOSTJWsj1Z8pDxh8rGSjS4GiQNel/lPvTp1icoiSKYWD8cO8p3IMGWgB6JPSSpRa+8hpYGFNcXY0gP2WjYUroFo7NHB11/qOg0hP7ll8BllwHjxgHr16t22pqBGydI1u5RADM+bd3Xn/1mfArYuiUCaAAhBFf89wofvXPp1Uvx2sbXsHS/PNfHmJwxWHF4he6fLt30goXeABcw9i1g6r1yxrhG9pvZ2vE4eLm0q+VSllbcGgfop6M/ATOZdt/QajwUpPwPmPk/vHoMePVTRAQPrXhIsX3LmFtACMGorFEKAhyVNQpf7P1CQYi8oy4cE1PoWUwNLQ0Y9y4j8rqH6tAtvpvkW5xsT5Y6KysaKyS9FQDuXX4vvNSLWMLcnz7e9jE+3vYxtt26TeGnbAbF9cU4/b3TAQCux12KDlIRz/z0DJ784Un8dstvGJ09Gjd/dTO+2vcV3pn+DmafzGJAbyvbhnM+Ogf2GDtaHtc2GCobK3HcG8fhj6P/iHemv4PT/u80xUCeJ394Ulrn121D0Qac9+/zpPQWTwtGv81IKP/ufIlUr/78anyx9ws0P9qMWUtmYf72+QCYcaMVDx0Acv6ZI3X29kjsgYq/aI+eVGPoa0NR1VQF+iSTDoa9NgznDjwX3137nanjRQRrUR+sOogT3mT/d8+knih/oNywvBmfzsB3h76D9wkvCCFYtHsRLv3sUnw842NcM+qaoNoQKjoNode3xtcp0BojYmtiZL7mfmD7NRg2jE2GAQD33AsUFQJTHvo/vLbxNSDWhZiYONQ21+LCIRfimXOewZGaI5jx6QzUOmtR21yLcX3G4e1pbyM1PhVrC9YC0B8+LqV77aCUjaRzowXI3grUZwPfzQUuvR6o7QvE1wIfrAZmnQ7snsG+KLqVIuGrBWhuBrLvvgSlzfk4M+9MrH74ZQCA/bZT4fK2oH/9TBz55EE89RRw8cXhvLKy54I6fsaITGY1rr5hNcobyiV54/FJj+OKkVdgeOZwKe+G2RtC+nTl1jSg/4CJPt+1zlp0i++GgekDcejOQ0iOS/aZ4IKDD7BRh8gtqi8KmNDFeC2VjZWSj7ca6wrXAWBub6OzR+NYA3NR5K6CgOwfb2Rx8gkbfj76MxwtDp9RmSK4cSHWAUCqm7eHE/rX+7+W8i/avUjKk2RPQnVztWa7xGsYiEcT75AG5NG5PJZ4oAh2FKboViq6jOrdb7x9Te4mJNmTcLCafW3/WvKrReihwtN6H2nq39yTpGYAUDoaKX2A0a1KQVojUF0nuCHFtoCQOLR4WpCdnI3R2aMlr4wWTwtaPC3ITMqUPqu4V4NfQhc1dLSw2C3lx7M2AUBTBpBYBZSOZhJNfR/A3gwkH0NM+WigUZYukuxJLB9kXT2OdgdKRyMvXj63cCMP2i5X3RO6K/Rpe6zdhwhDDRsqEoXeAyvGvRbXuTtaW2ihYp9DWUOZLqGrwdsmtlsM5uTxenwG8wAyOSfaExVRA83WAbAXl1ad/CVa1lCmCBObHJeMyqZKn3jy4UCzu9nveWjBzAvfH8TZh0To3W9J9iQ0uBpQ76xHkj1J8nLTK6ct0OEI/falt2PhLhYUKNmeLOlcFRUA/piEmppTcP9yFauf3TooZNhiIP0QirKB+1sVje05QH0KsHT/ZpZw7oNY5klEVVMVfi35Ffcvv196AD7e9jGO1BxBVVMV7l9+PwCmtQNsVho+QEOEFE956JfYmH4UDdWHUUWdQK9a4Lcb5U5PV5LfTlGfgUXEI3Xg8YFFnRXiQ6X3wBpFHVSXYQbBhMwVpzgT1/XAvw54e8V2i8c3uBo03T15niR7kt+IkPy+UecTCV2rzeo0HtpWJHlqELs9EJQ3lJuKfqmGeE8Eq6HrzX+qd79xQne0OJCFLInQG91tP48qR4cj9NVHVisD7NQIO/OAxt6b8NYmlU47qjUecv/VQL9fUB4DvNUaM6m5J0AzgTVHW2+C0e9jvScGLWjCzmM7caDqgPT2X31kNVxeF6qaqvDWprcAyH/2wp0LNS0oKRZz3o/YG7MWtK4ZHlDA2w04eL789dCSzMibeIAYL+CJY6Qe45JGinJrXLrhBbLnfuidFWYeWNGy07LyArXcggmZGyih8xc+b6/e/Jf1znq/hC6Gu1XLKoBgoauujRjcygyhS8QlEKCHemAjodNJeUO51JcRCAKZO1UPeoSud7/x68Cvp9Z1aWt0OLfF7bdtB32S4tYxt6JXci/WifIkxexUZrX3XLwejkccih/+1drTv/hD4O8OnLZK3jd1swOjlzrw5rQ3WZ7X9+CviQ4k2hJx96l3w/GIA0X3MQtm7nlzMTB9IGYeP1M6/qur2fRJ3133nU+9jkcc2P6n1hGLS97F1Ucc+N3w36EXjgOeqwH2TZOtclcSQChga/2M9Wi4LapHigojSju9he4Ng4UeoOXWFoSutswVFnpjuU8+vfoSbAlSHr1RtPy+8bHQ6wK00Fs9T0Ti0vr60Roy7w/lDeVBXXdF3PcgNfRALXQuI/L2cu8ki9CDQIItAQ0tDcivyUdjI9DsYGEyaayGrhcr69iAth86j/nANHRmBfM0vuQaupQXrJOT79OCVqeoBxqjQ12to0ztDVJ+eOKAGJfUL+Djh66w0NuH0PdW7JXChoooqS8J640tPlTF9cVYW7BWkkSO1ByBx+tRzofpDL+FXlBbgE3Fm6Qvts0lm33m2ixvKMeA7gNgi7FJRFhUV8TcKTXCbTpaHKCUSnXVt9RjY9FGFNUVKYhUi+QopfjmIJtsWZxdh4e7VUNPQ/dQDwgIBmcMll4iLZ4Wqd9CPTEDl1zE/1fr2hq5LB6pOYLfSn6D0+3Ez0d/ltK1CJ1SisPVh5Ffk4/i+mI4Whw4WntUIu7CukLFVHHqthytPeqTxgOUAcyb6FD1IYWEJEJ8QYhx17lF7mhxsCBtrfF4HC0OaWR0SX2Jct7R5lq/E2GEgo5N6K4G9J/XH+PPLcHHHzBCdyNwQo+JURI6iAce6jFF6OI+Leh2inJIFnqrv6299SHx2BmpC6EBXJStSzeYsK89CL3R1Yjhrw/HyDdGorpJ6QHT+8XemDY/fAH8xYfqyR+exIT3JmDxnsUoqC3AgHkD8OQPTyo7RTUkF9GKU0cm1JIz1GWM+dcYnPLOKdhYvBFbS7dizL/G4Pg3jlfk4R2hvZJ7SQOxJrw3AePeHacgLrGOJneTJM0tP7gc494dh3HvjkOZo0xql9b5/Jj/o+QtU++sV0zqrAU9yYUfk5OSI7VZ9HzhURc5uGUqdv6pY6AbtQMAhr82HCf/62Tc9vVtmPj+RCm91FHq0753N7+Lga8MRP95/dHnxT7IfTEXeS/n4cHvH4TT7UTfl/rixiU3+pwnwF6meS/n4clVsvvm+sL1GPTKIPzr13+hsK4QJ751Iga9MsiUhZ79guxxIEkuznr88Ys/4s5v7gTA/pcB8wagorECvV/sjUsWyOEO8l7OQ+Y/jOMQhYIOTegc2/fXAB5G6F7in9DFaIs8HrpI6JwceVosiQUBMSR0vc88iUQ88svBI8ojags9rkHO72+kqCC50HYgdNE65RYJIEcmXHVkVdjq0rIA91ftl9rw1b6v/EouYhmiS6XWtroMj9cjubLtr9wvDcqpb6lXEFlhXSFyU3ORm5qLovoiUEqlNorEyK1BR4tDUQ8n9uL6YhTUFUiuoVrnw9vQN7WvohxdyUWnUxSA3OZW+UWMlSPOBQpAmjRCdJF0e90+5YrPqIhGV6N07NIDSxX7iuqKfMoRB/gBsnvqmoI1kv7/7UF5Igrxxb27gn09ritaJ6Xxl9aSvUsUIYrV8YH4F6BYnnjOPMyxo8WBT7Z/4nOe/L4Q26YOwxxudApChycOcIdPcuEdjDyNEIK42DhtySXWpOTi0ZNchE5RQLbQvXapU5Q1QvZo0eoUVZTZRhAfAL31cEFL/y6sK1TM4q7oFNWQXMSXrhlCV7gQCh3xhXWFinPkpEIpRUFtAXK7MXIsqC1Q+O6Lx/Cy6531mm0F2Oc9b5dWnsK6QhAQnNn/TNS3yOX4lVw0LHRO6IV1hew8Wn2yh/YYqgj5AMjPhSgluDwuXylHx0tIvA7qsgvqCnzKyemWo1lOTkqOVJZ4zmp5jufl4FJQcX2xwh1WHBUNyC8+Iy8XQPt6AlBMTq1GJFw+gQ5M6PG2eHmDxkgWOsJA6G6qJHS+bmihBzBSVNGBGaOy0EXJRbTQBfLW6hRtDwtdnOlHfEiNZgAKFloPlZpYHS2sMzvZnuzXQhdjvABAdrKv877DJZehfmFpvcBqnbVSOOHcbrk++cTrInWCuhyG2reRhV5YV4jslGxkJGQoLHQ97VrsFFXXxQnd6XGiorFCavdpuaf5lMMtdJHQtSx09UAtsd16UE967aVeRSwgEQpCT5bPR3xx8xeGSOi8L6PUUap46fDIkRy8HXpf3+pOUTWM3C9FV9FwosMSusJCj3EHZKFrTXAhEjq3dtXE7XQ74fK6QusUjdXrFOUWuqpTlFBQ4lGQd7R0iirIShhlJ64H41OsBS1CL6grkEiy0dWIemc9UuJSkBKX4tdtcVCGMkiYenZ4QGkVi2RcUFegPN/WfXzZN60v+qb1RX1LPXaW71QcJ5XdIljoretaliiP4aJ1PgV1Beib1pedb2s5/IWmBbFTVIzBAzDZhsdX59eVT6KhBjem1ISubqOehWr0wi+oK1CUI3Z2qtEruZd0TUWZSfyfS+rZFxyfDk7cX+IoUertKpLl7dCz0HlYB70vLCMrPBJGD9AB/dA5lITukSx0GtvsmznATlE9QufzU0amU1RDQ/fa5fZ7hc5RrU7RAN0WWzwtuOWrW/DwGQ9jaI+hPvtf+OUFfLH3C5zS+xS8OOVFrDy8Ek+tfgpe6oU9xo6XpryEgroC9EruhQRbAt777T0pNrxI9JPenyTJUqFAbQWd0vsUbC3bKtV1qPqQNNw/hsRg8Z7Fkn7KIWqxYtAuAOgW382nzlVHVkkddtzSO6X3KVh5eCViSAzG9h6LTcWb8Niqx/DGpjckPTY3NVciMx7/Zlwf1inKy+OeI6vzV0tDxnNScrCtbJtULiBLQa+sfwWL9sjD7wHgt5LfMGXwFHSL7wYP9eCT7Z+gW3w33bj8To8TV39+NXYe2+ljeXMLHQCuW3QdjjUeYySf5hvpjt/zop585cIrfZ6BXwp+wcT3J6J3t964dtS1eH7N86Cgkk5/Qq8TFBORZCRmoLyhHB9tlSefPu/f5/lo6BzVzdV4+senAciGFcAGYV322WXISs6Swg94qRcLdy3EvPXzsLdCngvVqJ/nukXX4c2L3vT5+p70/iRQUKmTW2uybEDJCRPfn4jRWaPRI7EHKpsqIyJLAp2F0IlHttBjQrfQOTmqiZuTSqidosyapgCIvobOJReAkX6rNW6Pscs3SoxLKDMwS3hj0UZ8sOUDHKw6iB//+KPP/lc3vIqjtUextmAt5p47F/O3z8eGog2Y0HcCVh5eiS/2foEjNUeQl5aH3x/3e0Xn1sD0gRjbeywaXY0KKy4UZCRmYNrQaUiyJ4GA4I+j/4gX1r4AL/VK06V5qAdTBk1BDInBsgPLfMo4Oedk9E/rj+yUbJyUfRLuHHcnXF4Xpg6eikHpgzB18FRkJWehxFGCrOQshcXWL60fzul/Ds4bdB7e3MTGLMw+aTbWFKyRXhw9k3vishGXYXT2aAxMH4hpQ6eh0dWIcweci+nDpuPVDa9K5Z094GzkpORIdcwYPgN3jLsDFBS3jLkFy/YvQ4u3BYPSB+HOcXdixzFfUjs191Rcf+L16JfWD6uOrILb68akfpNwxXFX4Mf8H7G1bCs2Fm+U8pc1lGHxnsUY2mMobjvlNpycczIqGitQ3lCOswecjcykTFx53JWoaKxAVkoWfjfsd5iUNwnTh05HVVMV1hSsASAQeqsFetGQi6RO3t8N+x26J3THD0d+QH5tvkR6TrcTawvXYlLeJAxIH4CLh12MMTlj8Mn2T/DYpMfwf7/9H2486Ub87ce/weV14cz+Z6K2uRY7yndIhpQaYvRH8eVyuOYwFu9h0UgvHHIhACb/fL77cx9PIx6v5qTsk5CemI6Vh1dK+zYWb8TKwyt9LPRfCn5ReEUNzxyuGJzFIRL6z0d/xvrC9RiWOQyVTZWaBkQ4YIrQCSFTAcwDEAvgXUrpc6r99wKYDcAN4BiAWZTSfJ+Cwgh9Cz0wDV3Ly0VPQ9ci9GA6RVkj3K1uiToaOu8UBRhxt5J3kj3JR3JJsicFLLnwG1BL/3O6nSioLUBeGpvDMr82HwerD+Kk7JOw4roV6PtSXxysPogDVQdwau6puG/Cfbhvwn0B1R8OTBk8RXff/RPu93v8vAvmKbaXXeP7EtDC5SMvl9avOuEqzTxJ9iR8edWXijR/s/UAjOgB4NIRl+q2Uwvf/uFbxfY7F7+DzSWbMeZfcrx6TsD3jr8Xl464VFEHx6eX+4bqXHLVEizdvxQXzb8IgNJC75HYQxpcJ2L6f6Yrwvg6PU5kJmVixXUrFPmuPfFaAPLE1N/84RvF/oe+fwhz18xVpD18xsN44ZcXFJKG3jrXyD1ej2YnLc/7j/P+gckDJyP26Vh4qRdzzpyDOavnwEOZC/PA9IEY2XMkvtr3FdIT0zG291h8c+AbjMgcgRXXrcD7v72PWUtmAQBenvIy7v72bh9O8FAPEmwJuGDwBbh4WJgj6LXCr4ZOCIkF8DqACwCMBHAVIUQ9OeFvAMZSSkcBWAjg+XA3VA09C92M26LfTlEdycXQQg+gU5TVo+rYdLcGr+IauscutRexLgV5qztFgyF0HoNGq91Hao6AgmLKoClS3gNVBxTxzncd24X82nwMTh/sc7yF6IB6CD23YvVC+potS5QZtcJdaNbtdgZVr9Yxsf/f3rnHylHdd/zzm13fe7GNsSF2YgXM81IgEXXMFeFV4fIolLQpbVKFvECtJau0qGlV1Y1rQRpUhPpQ06SFChqshPRBFB4CUauEElRVakm4BEjsgsFQl2JjfHnZgB/X9+7pHzNn7pnZM7Ozu7N37879faSVZ2dnds4Z7/3td7+/3zlHatSCWtOCG/G1HLVu90+baa8Xbl+3/bAJU5snmGpMMdWYCq8Z9ake1ON22fPc+5Al8hqmEb5Xxj0rgyJJ0XOBHcaYl40xk8A9wK+4BxhjHjfG2Kr8J4D8lWBLoBuF3jIp2o5C7yApmmhT7UgyeKfr0AETTMbHLxpaFAVhM7NvwaK2PXTr2+7av6tpBKMN9lYB37P1Hnbt3xWvNnTqslMZ3z1OwzSakovK3CE997xVo50EFPcc13LJCtJN154+3NEcLb5zakEYXN3Andieat4/3Zj2Vt3Y19PXsX/XVtnXg/pM8HaCu+2n7wvPN5Xx5PRkR/ehKEUC+ocBNyX7arQvi3WA97eriKwXkXERGZ+YmPAd0pL334c9e+DA/lSVSxz8ulfoWUnR7iyX5KjTmYA+mfTLfZZLbcZyiSsYgqlYtS8aWtS2h25rbt8+9HZi7meYGQBz0aqLWLl4Jd9+9tsYTLwCz5qVa+Jj+7k6i5JPOnCXpdDdgUVZwanp2iUqdKuQMy0XT3C3SjuNfd1ex84YWQ/qBBLMKPSgFh+TUOiOarfE98dT5dLpfShKqe8sIl8AxoCLfa8bY+4E7gQYGxvraOHG22+HDRuAD43Ab9kLTwMCU8MdBfR0lUs7lksgAfWgXigpmrBc0lPlNlIB3QnyRpKWS9jOIx1bLsYYtk1sY9Uxq3hl3ytsf2N7otZ3295tLF+4nBWLVvD8Dc/zxoE3GK4Nx8vHXT92PZ8Y/QQj9ZHC830rs4/P9vDtL4KruK0dkRecfHZPt78M3PeuBW0qdJPhoU/7f7XYXwHWQ68H9bhPtaDWZLX4fsH4RF6n96EoRRT6LsCtXTo+2pdARC4DNgGfNMb0ZhgUcPnlsGkTMJWyXCAM6G1aLr6kaDuWC6QqT1JkJkWbLJcchZ5Kisb73KRoG5bLnvf28NbBt/jUmZ8CYPub2xOvb53YykdWfAQI5zg5ZdkpibVARYQTl56owXyOkw4ctuKoI4Xus1ymcwJ6DxW6DbZFPHQb3KcbLTz01BdQTUJFnvDQA4+HLrWm8/MCeq8tlyJ3+ElgVEROJgzk1wCfcw8QkY8BdwBXGmNazxnaBatXw9FHwy23pZKiANM5Ct0INCLfq0PLxVeHbp93lBS9+Ktw5n0zg4gAfubB6PiZfZO//AUg7GMc0K+9LFxEOtp3JNgP6y7gT1+DO+7yNiXGfjFdNXoVtz95Ozf/+81sfnpz/PpTu59i/Tnr899EmfNkJUU7UsrdJkWnD2fO7VL0uu6+WlBLTA7Wyn6xSjtNVl7BqvCEh+4Eb1etp8/Ps2EPT/VWobcM6MaYKRG5AXiEsGxxszFmm4jcDIwbYx4C/gJYDHwv8qBeMcb0pi4HWLKE9hX69BAQ+mO+gO4q5yzLxbcN4X9g20nR+kG44C/hwHJ4+jfhzdNh62fCZegOHAdvnQYHj4PnrkZG3sc04PI1p7PhwnBB4395aRIOLeMzP3cu68fWMf70YXZPNhgWWDzkbUrM4qHFnLn8TC444QI2XrQxri+2XHrKpXzx7C/mv4ky58lKipam0NtIih6aOtTR/Oh5Ct0G60CChCp3xz7YbZ9CFyQzKWqtVNdD91W55CVFMy2XPit0jDFbgC2pfTc525eV3K5cmgJ6EYU+PRPlfFUutaCGmBomR6H7tu3z3IDu/DqIz12xFYbfgy1/C89eF+67957kye99CL77ACML4cABuO1GGF0FD3/uYeTz4SF3fxOGhuCyPZdw93fgD78F17URi7+y9iutD1IGkqykaLceekKhZyVFZ7FsMf3357VcPB66e16TQhePh55nuTjn+2ajtMwFy2XOMTxMXHcOhBUfEAb2s/+J87/5shXj4YDM1c8nArovKQpQY4ipDA/dHb7uC+gPbn+QbXdtI80r+15BGgswSNJyuSwcEs7usZb9tVWFviUb7T7P2gnKPMcNHIEE3Sl0T5WLDXTe44OZsr6GaZSaFLUB1U6vMFIfiZW420+YCapWadv22PMOH/bfE1vVYs8bqg0las/Tgdw9Pz2S1r0PDdPoaZXLwE7OxdQIvHhluG0tl2U7ARAJWDK8hCXDS3j5nZfhqHdaKnSAwAzleui+bQirPtasXBNf0318dMVHOfaF34+vdc7Kczh18tdg4iz48Tp4o3na1izyArqipEmrxq48dPe9nJlOW3nobglf2UnRuD214cR2lkKfakwlj3X74UmK1oJa64FFnqSofc391eBet68e+txF4LFbYfRfZyyXiC3XPMrSRWHy8I8e/WP+/D9vTUxk5QZBW+UCoUJn6U72NEK5mwjiQXZA33DhBjZcuCGzpWfeAm8SBvTli5Zz1bv38Tf/0EZPCwRtDexKmnSpoZ1vpayRonnvZa9tr2sw5Q0scipO0u1x+wnEawjYgUXu6+556SBrPXRr1TQNLHJ+gaTPt9uJgO5ct98Di+YukS8dK/RD4YQ5w8HMbHNrV10abhyYSci4wc9V6CPmWDjjQf5j8hsM1YYSs9a5028uHVnaUXO7tUVUoSvtkKViOwqsGQG0leXiXrdnCr2eVOg+vAo9557Ya7RKirpBPv1eqtDbxUQ3xir0r/8PANPOPFEXn3AJ/P0TsN8/G4Eb0H/1ve+z+YGXuP634Xd/Y2U8gT3AjRffyBWnXcGykWXxNKOFm2nynxdFA7rSDlk2SSeB1ZcUhewvh9hyKWDP5JE3sMjXnsTCNw62/NB9PU+h2zp0q+wTCt310D3amP+hAAAMC0lEQVRJUa/lkmPvlMlgB/S0Qj8YquiGM6++MQK7Pp75Fm5AX2JWwc5VjNbhjNSyjAsXLGTtSWs7aqYN4Ol/20UDutIOWQp9NiyXWVXoGarbxSp0txY+10MPPAOLiir0aJ/r5atCz+D112H37rAypdGImh8k60unHUu9kb2sH5CsculVpYgGdKUf+IbrQ/dKOU/Zpq/drTL1neMG1PQ1shT6VGOKaTPNwtqMjZr3ZRNIEA4scjx0NxGa9tDd822b3SqXbn8hFWXgPPS774Y1a6JAnbZcItoJ6G5StFeUFdAVpR18tdEwSwpdeqjQU5ZLIYUeDSzKCv5ZlkvCQ3csl3gyr6g+2psUbWR46Gq5zHD11XB6tGLa1demLJeIVgHdDaiu5TLX1e5cb58yt8hKHHY7sMinRpuuHQU11+LoleVSyEOPlLYbWPNyAfHkXI3mybkEidtlq2haJUXd+6CWi8PoaPgAMhW6G8SLWC6DotA1oCvt4AbhbgNrQoF6aq6bju9TUjRrvhifQnfXIW2l0OsykxQVkbh/di2BImWL7nv3ioGzXBKkk6IRrRR6Vtlir9CArvQDcT4w3SblfEk/aF2H7gbNXij0PPvFxQ7hz7oPvrlcXA89/asgT6H3c2DRwAb0sjz0QUqKKkqndJuUywp+mSNFPUPiezGwyLVCIL9sMa3QE23zzLaYUOhO2aIgTUvW+e5PVlJUFbqHfftwFHp3VS6q0JWq0/XAojYVum8ln54o9KCYQreBOes+eIf+i3/ov69dc6VscWAD+t69QFy2WE6Vy1wPmHO9fcrcpcyBRe0kRbtW6DmTc6W3BUlMoucSJ0U99yGQILanXMWdHljk64u1XHyvZXnoWrboYWICTYoqSkG69tDbTIrGHnqtBx66Y7mkF5vI+tKILRfPffCdYz10X9mi267CSdFZKlsc7IDeYVLURQO6Mh8osw49L5mY3t/rpGjWPOVpbFLUV+3j9scq9Xjov2lesUgkv2zRlxSdrbLFgQ3oe/dSWlJ0UAK6onRKmUnRLPvFd3xe4rHQdVskRZsqXjK+YOxSkD6l7L2Gb+i/JynqHp9+X1XobVCmQg9m6S6oQlf6RZEJtfJILynnS3r66HVS1FXOeQrdJih9X2yZte5RUjQ9sMg9N7ZcfEnRrCoXVejNTEwASLi8W6rKJTk5V/77zObQ/6znRdGArnRKEd87j6aA7ihkHzbQJQbvlJQUbfLQXbWe0R4bXNtV6FbZu++dGFhEs4du71VmlYsO/Z9h0ya4917YuTPa0ahX3nLJW4IufYyi+OjW+vC93+T0ZOaXg6/6oyyFnlXl0qlC951jk6L2vPR7pxW6r4yxH1UuAxfQ778fXngh3B4ZgUONmiZFFaUFRRKZbb1fjrqFmUAnIggSrlhUlofegeXiLihtaWW51IN6Yh1Wn63iS4ra13WkaAGeeAJefRX27IF33yVMjA6YQm/VpjS6BJ3SLW6gkxI+LL6kZztt6OacplJFTwmjiyDeBbLzyhbtl0a8DqvUcpPB6SBtf8FYdIGLDI45JnzEeBR6u3XovR76XxYatJVO6VUQyfTQI+VqB+0YY8pNinqG/rsKPZCAhgn/+IdqQ3Fw9dkmRRW6RWienMuXND4wfSB+rmWLRRlAhT7XvziU6tEr3zbTQ7eWCzN/XKUmRT2Tc7mBPj2l7pHGkaY25NlGgQRJhZ6qVbcB3H5xtWp3t8nholQjoM/xof8a0JV+U3ZAz0oGxq97At1sKnT3vKwFOXIVelTlYlV+Pah7+2Qy/pjTQbvspHQWAx/QxdQGbnKudj10RemWXgWRVmrT9etLHVjUwkNPzMCYNWWuM1DIe42cRHKrPITPU8/rU1kMfED3lS1WdS4XRemUuWC59Lps0R2S756XVTKY1x53mTnfsbZfmZaLp+qlyHW7ZeADunRguWQtQacoVWW2k6LWqrAJ0U7bkGWH+FS5OyQ/sUB2xjw26cFBLoEETTaJ+yVlFbrtZ5p00FbLpSglJEV7PfRfFbrSb0r30D0Dh3rRhlZli+7kXO7xriWSNe1BrkJvMf95K9Ry6RChfYXuKvLZHPqvHrrSL8pWha1Utw34rprtdpbHeF9GUtS1XFyrp5Xl4kts+iyXuE/RYKmsc33tTlT7qELPRrRsUVFa0jcPvcukqC/56KpnO0QfkvXhLq2Soj7ykqLul1TRssXEfei3QheRK0Vku4jsEJEve14fFpHvRq//UEROKruhmW3zVLloUlRRksy25eIq9LLbkJ6Eq5U10qpssYiHXg/qCTXerkJPt79XtAzoIlIDbgN+ETgL+KyInJU6bB3wtjHmNOBrwJ+V3dDM9pn6nJ/LRVH6zWwnRS29UKZZE2VltSdr2H2roJu1qAe0LlvM+/LqZZWLZH3DxAeInA/8iTHmiuj5RgBjzK3OMY9Ex/yXiNSBPcByk/PmY2NjZnx8vO0GH3XLURyaOjSzYz6pXfczZFL70s8VRZmzLAgWMHnjZOsDPYjIU8aYMd9rRSyXDwP/5zx/NdrnPcYYMwXsA47zNGS9iIyLyPhEOKF591Q5gInzb7qf6X1Vvg+KUjHKmCDNx6xOzmWMuRO4E0KF3sl7HNx0sNQ2KYqiVIUiCn0XcILz/Phon/eYyHI5BnizjAYqiqIoxSgS0J8ERkXkZBEZAq4BHkod8xBwXbT9aeAHef65oiiKUj4tLRdjzJSI3AA8AtSAzcaYbSJyMzBujHkIuAv4jojsAN4iDPqKoijKLFLIQzfGbAG2pPbd5GwfAn693KYpiqIo7TDwI0UVRVGUEA3oiqIoFUEDuqIoSkXQgK4oilIRWg7979mFRSaA/+3w9A8Ab5TYnEFA+zw/0D7PD7rp84nGmOW+F/oW0LtBRMaz5jKoKtrn+YH2eX7Qqz6r5aIoilIRNKAriqJUhEEN6Hf2uwF9QPs8P9A+zw960ueB9NAVRVGUZgZVoSuKoigpNKAriqJUhIEL6K0WrB5URGSziOwVka3OvmNF5FEReTH6d1m0X0TkG9E9+ImIrOlfyztHRE4QkcdF5L9FZJuIfCnaX9l+i8iIiPxIRJ6N+vzVaP/J0QLrO6IF14ei/X1bgL1MRKQmIk+LyMPR80r3F0BEdorIT0XkGREZj/b19LM9UAG94ILVg8q3gCtT+74MPGaMGQUei55D2P/R6LEe+LtZamPZTAF/YIw5CzgP+J3o/7PK/T4MXGKM+VlgNXCliJxHuLD616KF1t8mXHgd+rgAe8l8CXjOeV71/lp+3hiz2qk57+1n2xgzMA/gfOAR5/lGYGO/21Vi/04CtjrPtwMro+2VwPZo+w7gs77jBvkBPAhcPl/6DSwEfgx8nHDUYD3aH3/OCdchOD/arkfHSb/b3mY/j4+C1yXAw4Qr4Fa2v06/dwIfSO3r6Wd7oBQ6xRasrhIfNMa8Fm3vAT4YbVfuPkQ/rT8G/JCK9zuyH54B9gKPAi8B75hwgXVI9qvQAuxznL8GNgCN6PlxVLu/FgN8X0SeEpH10b6efrZndZFopXOMMUZEKlljKiKLgfuA3zPG7HdXRK9iv40x08BqEVkKPACc0ecm9QwR+SVgrzHmKRFZ2+/2zDIXGWN2icgK4FERed59sRef7UFT6EUWrK4Sr4vISoDo373R/srcBxFZQBjM/9EYc3+0u/L9BjDGvAM8Tmg5LI0WWIdkvwZ9AfYLgU+KyE7gHkLb5etUt78xxphd0b97Cb+4z6XHn+1BC+hFFqyuEu7i29cResx2/7VRZvw8YJ/zM25gkFCK3wU8Z4z5K+elyvZbRJZHyhwROYowZ/AcYWD/dHRYus8DuwC7MWajMeZ4Y8xJhH+vPzDGfJ6K9tciIotE5Gi7DfwCsJVef7b7nTjoINFwFfACoe+4qd/tKbFf/wy8Bhwh9M/WEXqHjwEvAv8GHBsdK4TVPi8BPwXG+t3+Dvt8EaHP+BPgmehxVZX7DZwNPB31eStwU7T/FOBHwA7ge8BwtH8ker4jev2Ufvehi76vBR6eD/2N+vds9NhmY1WvP9s69F9RFKUiDJrloiiKomSgAV1RFKUiaEBXFEWpCBrQFUVRKoIGdEVRlIqgAV1RFKUiaEBXFEWpCP8PmooiLGC5Z0QAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "UFy3KUFHPEY4",
        "outputId": "f101f7e9-67eb-475d-9043-c72b413d21de"
      },
      "source": [
        "plt.plot(history.history['loss'], color='red')\n",
        "plt.plot(history.history['val_loss'], color='orange')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fdc0f71bc10>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wGsqLU3YrgI",
        "outputId": "4f2e1fd3-7680-4675-e44c-dd8e4221bf3b"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def test_case(sentence):\n",
        "  predictions = []\n",
        "  for i in model.predict([sentence]):\n",
        "    for j in i:\n",
        "      if j >= 0.1:\n",
        "        predictions.append(np.where(i==j))\n",
        "  return print(predictions)\n",
        "\n",
        "test_case(\"Saya terkena bully disekolah, saya sangat takut\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(array([0]),)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cV5bEdLUbeDj",
        "outputId": "f0736a97-1d6f-409b-d036-786382f93b53"
      },
      "source": [
        "test_case(\"saya di pukul suami saya sampai berdarah\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(array([0]),), (array([1]),)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7b87UjBboni",
        "outputId": "c25208d9-b42b-4868-8973-a3181544c36c"
      },
      "source": [
        "test_case(\"saya tidak mampu menafkahi anak saya saya bekerja seorang diri\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(array([0]),)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVvW7z6kobnF"
      },
      "source": [
        "model.save('my_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lI8g6kwNbRqV"
      },
      "source": [
        "* Layanan Hukum (0)\n",
        "* Layanan Medis (1)\n",
        "* Layanan Psikologis (2)\n",
        "* Rehabilitasi Sosial (3)\n",
        "* Jaminan Keselamatan (4)\n",
        "* Layanan Pendidikan (5)\n",
        "* Pengasuhan Pengganti (6)\n",
        "* Bantuan Sosial (7)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN_Jf91QkFFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d5a4712-e073-4b80-cbf5-759e40f7d2d0"
      },
      "source": [
        "import tempfile\n",
        "import os\n",
        "\n",
        "MODEL_DIR = tempfile.gettempdir()\n",
        "\n",
        "version = 1\n",
        "\n",
        "export_path = os.path.join(MODEL_DIR, str(version))\n",
        "\n",
        "if os.path.isdir(export_path):\n",
        "    print('\\nAlready saved a model, cleaning up\\n')\n",
        "    !rm -r {export_path}\n",
        "\n",
        "model.save(export_path, save_format=\"tf\")\n",
        "\n",
        "print('\\nexport_path = {}'.format(export_path))\n",
        "!ls -l {export_path}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Already saved a model, cleaning up\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/1/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/1/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "export_path = /tmp/1\n",
            "total 144\n",
            "drwxr-xr-x 2 root root   4096 May 12 03:16 assets\n",
            "-rw-r--r-- 1 root root 138536 May 12 03:16 saved_model.pb\n",
            "drwxr-xr-x 2 root root   4096 May 12 03:16 variables\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}