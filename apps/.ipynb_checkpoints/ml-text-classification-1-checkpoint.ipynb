{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Train-Baseline","metadata":{}},{"cell_type":"code","source":"# DATA_PATH = '../input/'\n# 数据输入路径\nDATA_PATH = '../input/shopee-product-matching/'","metadata":{"ExecuteTime":{"end_time":"2021-03-18T09:59:13.247406Z","start_time":"2021-03-18T09:59:13.24369Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd, gc # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2, matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n\nimport cudf, cuml, cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors","metadata":{"ExecuteTime":{"end_time":"2021-03-18T09:59:14.869532Z","start_time":"2021-03-18T09:59:14.482759Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# 计算F1 score\ndef getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target,row[col]) )\n        return 2*n / (len(row.target)+len(row[col]))\n    return f1score","metadata":{"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# 标识是不是计算验证得分\nCOMPUTE_CV = True\n\ntest = pd.read_csv(DATA_PATH + 'test.csv')\nif len(test)>3: COMPUTE_CV = False\n# 如果测试集大于3行，进入提交模式\nelse: print('this submission notebook will compute CV score, but commit notebook will not')\n\n# COMPUTE_CV = False\n\nif COMPUTE_CV:\n    train = pd.read_csv(DATA_PATH + 'train.csv')\n    # train['image'] = DATA_PATH + 'train_images/' + train['image']\n    tmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\n    train['target'] = train.label_group.map(tmp)\n    # train_gf = cudf.read_csv(DATA_PATH + 'train.csv')\n    train_gf = cudf.DataFrame(train)\nelse:\n    train = pd.read_csv(DATA_PATH + 'test.csv')\n    # train['image'] = DATA_PATH + 'test_images/' + train['image']\n    # train_gf = cudf.read_csv(DATA_PATH + 'test.csv')\n    train_gf = cudf.DataFrame(train)\n    \nprint('train shape is', train.shape )","metadata":{"ExecuteTime":{"end_time":"2021-03-18T09:59:15.92512Z","start_time":"2021-03-18T09:59:15.308672Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"this submission notebook will compute CV score, but commit notebook will not\ntrain shape is (34250, 6)\n","output_type":"stream"}]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# image hash","metadata":{}},{"cell_type":"code","source":"# 相同哈希值当作一组\ntmp = train.groupby('image_phash').posting_id.agg('unique').to_dict()\ntrain['oof_hash'] = train.image_phash.map(tmp)\ntrain.head()","metadata":{"ExecuteTime":{"end_time":"2021-03-18T09:59:19.569052Z","start_time":"2021-03-18T09:59:18.284395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if COMPUTE_CV:\n    train['f1'] = train.apply(getMetric('oof_hash'),axis=1)\n    print('CV score for baseline =',train.f1.mean())\n\n# 聚类包括自己的，应该是大于0.5的","metadata":{"ExecuteTime":{"end_time":"2021-03-18T09:59:21.98207Z","start_time":"2021-03-18T09:59:20.62671Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Efficientnetb0","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import EfficientNetB0","metadata":{"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# RESTRICT TENSORFLOW TO 1GB OF GPU RAM\n# SO THAT WE HAVE 15GB RAM FOR RAPIDS\nLIMIT = 1\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    tf.config.experimental.set_virtual_device_configuration(\n        gpus[0],\n        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    #print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    print(e)\nprint('We will restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\nprint('then RAPIDS can use %iGB GPU RAM'%(16-LIMIT))","metadata":{"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"We will restrict TensorFlow to max 1GB GPU RAM\nthen RAPIDS can use 15GB GPU RAM\n","output_type":"stream"}]},{"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, df, img_size=256, batch_size=32, path=''): \n        self.df = df\n        self.img_size = img_size\n        self.batch_size = batch_size\n        self.path = path\n        self.indexes = np.arange( len(self.df) )\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        ct = len(self.df) // self.batch_size\n        ct += int(( (len(self.df)) % self.batch_size)!=0)\n        return ct\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X = self.__data_generation(indexes)\n        return X\n            \n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' \n        X = np.zeros((len(indexes),self.img_size,self.img_size,3),dtype='float32')\n        df = self.df.iloc[indexes]\n        for i,(index,row) in enumerate(df.iterrows()):\n            img = cv2.imread(self.path+row.image)\n            X[i,] = cv2.resize(img,(self.img_size,self.img_size)) #/128.0 - 1.0\n        return X","metadata":{"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# model = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg', input_shape=None)\n# model.save_weights('efficientnetb0_notop.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE = '../input/shopee-product-matching/test_images/'\nif COMPUTE_CV: BASE = '../input/shopee-product-matching/train_images/'\n\nWGT = '../input/effnetb0/efficientnetb0_notop.h5'\nmodel = EfficientNetB0(weights=WGT,include_top=False, pooling='avg', input_shape=None)\n\nembeds = []\nCHUNK = 1024*4\n\nprint('Computing image embeddings...')\nCTS = len(train)//CHUNK\nif len(train)%CHUNK!=0: CTS += 1\nfor i,j in enumerate( range( CTS ) ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(train))\n    print('chunk',a,'to',b)\n    \n    test_gen = DataGenerator(train.iloc[a:b], batch_size=32, path=BASE)\n    image_embeddings = model.predict(test_gen,verbose=1,use_multiprocessing=True, workers=4)\n    embeds.append(image_embeddings)\n\n    #if i>=1: break\n    \ndel model\n_ = gc.collect()\nimage_embeddings = np.concatenate(embeds)\nprint('image embeddings shape',image_embeddings.shape)","metadata":{"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Computing image embeddings...\nchunk 0 to 4096\n128/128 [==============================] - 35s 250ms/step\nchunk 4096 to 8192\n128/128 [==============================] - 35s 262ms/step\nchunk 8192 to 12288\n128/128 [==============================] - 34s 254ms/step\nchunk 12288 to 16384\n128/128 [==============================] - 34s 255ms/step\nchunk 16384 to 20480\n128/128 [==============================] - 36s 271ms/step\nchunk 20480 to 24576\n128/128 [==============================] - 36s 261ms/step\nchunk 24576 to 28672\n128/128 [==============================] - 34s 260ms/step\nchunk 28672 to 32768\n128/128 [==============================] - 34s 259ms/step\nchunk 32768 to 34250\n47/47 [==============================] - 13s 256ms/step\nimage embeddings shape (34250, 1280)\n","output_type":"stream"}]},{"cell_type":"code","source":"KNN = 50\nif len(train)==3: KNN = 2\nmodel = NearestNeighbors(n_neighbors=KNN)\nmodel.fit(image_embeddings)","metadata":{"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"NearestNeighbors(n_neighbors=50, verbose=4, handle=<cuml.raft.common.handle.Handle object at 0x7fcf4a816fb0>, algorithm='brute', metric='euclidean', p=2, metric_params=None, output_type='numpy')"},"metadata":{}}]},{"cell_type":"code","source":"CHUNK = 1024*4\ndistance = []\n\nprint('Finding similar images...')\nCTS = len(image_embeddings)//CHUNK\nif len(image_embeddings)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(image_embeddings))\n    print('chunk',a,'to',b)\n    distances, indices = model.kneighbors(image_embeddings[a:b,])\n    distance.append(distances)\n    \ntemp = np.array(distance)\nplt.hist(temp.flatten(),bins = 100)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We notice that there is a tail to the left indicating duplicating items in the histogram. The spike at dist = 0 are the images that are exact duplicates and low numbers are near duplicates. So choosing 6.0 will cut that tail off.\n\nSecond, we try different distances and choose the one that maximizes CV score.","metadata":{}},{"cell_type":"code","source":"def get_knn_preds(df, embeddings, knnmodel, threshold = 6.0, PRINT_CHUNK=False):\n    preds = []\n    CHUNK = 1024*4\n\n    print('Finding similar embeddings...')\n    CTS = len(embeddings)//CHUNK\n    if len(embeddings)%CHUNK!=0: CTS += 1\n    for j in range( CTS ):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(embeddings))\n        if PRINT_CHUNK:\n            print('chunk', a, 'to', b)\n\n        distances, indices = knnmodel.kneighbors(embeddings[a:b,])\n\n        for k in range(b-a):\n            IDX = np.where(distances[k,]<threshold)[0]\n            IDS = indices[k,IDX]\n            o = df.iloc[IDS].posting_id.values\n            preds.append(o)\n    return preds","metadata":{"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def distance_threshold_searching(df, embeddings, knnmodel, LB=4.0, UB=7.0):\n    df1 = pd.DataFrame(columns = ['target', 'pred_matches'])\n    df1.target = df.target\n        \n    thresholds = list(np.arange(LB, UB, 0.2))\n    scores = []\n    for threshold in thresholds:\n        preds = get_knn_preds(df, embeddings, knnmodel, threshold)\n        df1.pred_matches = preds\n        MyCVScore = df1.apply(getMetric('pred_matches'), axis=1)\n        score = MyCVScore.mean()\n        print(f'CV score for threshold {round(threshold, 2)} = {score}')\n        scores.append(score)\n    thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n    max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n    best_threshold = max_score['thresholds'].values[0]\n    best_score = max_score['scores'].values[0]\n    print(f'Our best score is {best_score} and has a threshold {round(best_threshold, 2)}')","metadata":{"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"distance_threshold_searching(train, image_embeddings, model, LB=6.0, UB=7.6)","metadata":{"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Finding similar embeddings...\nCV score for threshold 4.0 = 0.604331913022176\nFinding similar embeddings...\nCV score for threshold 4.2 = 0.6085015290133909\nFinding similar embeddings...\nCV score for threshold 4.4 = 0.6126903603905952\nFinding similar embeddings...\nCV score for threshold 4.6 = 0.6166444106514297\nFinding similar embeddings...\nCV score for threshold 4.8 = 0.6200762311910106\nFinding similar embeddings...\nCV score for threshold 5.0 = 0.6241102965641212\nFinding similar embeddings...\nCV score for threshold 5.2 = 0.6275635861413381\nFinding similar embeddings...\nCV score for threshold 5.4 = 0.630622327260816\nFinding similar embeddings...\nCV score for threshold 5.6 = 0.6340223808341016\nFinding similar embeddings...\nCV score for threshold 5.8 = 0.6369304688092922\nFinding similar embeddings...\nCV score for threshold 6.0 = 0.6393196101848189\nFinding similar embeddings...\nCV score for threshold 6.2 = 0.6421880171402373\nFinding similar embeddings...\nCV score for threshold 6.4 = 0.6439851560811659\nFinding similar embeddings...\nCV score for threshold 6.6 = 0.6454957565448419\nFinding similar embeddings...\nCV score for threshold 6.8 = 0.6466830060536779\nFinding similar embeddings...\nCV score for threshold 7.0 = 0.646752767970401\nFinding similar embeddings...\nCV score for threshold 7.2 = 0.6456601353216277\nFinding similar embeddings...\nCV score for threshold 7.4 = 0.6436746778417369\nFinding similar embeddings...\nCV score for threshold 7.6 = 0.6412465710814036\nFinding similar embeddings...\nCV score for threshold 7.8 = 0.637586985426812\nOur best score is 0.646752767970401 and has a threshold 7.0\n","output_type":"stream"}]},{"cell_type":"code","source":"preds = get_knn_preds(train, image_embeddings, model, threshold=7.0)\ntrain['oof_enet'] = preds\n\nif COMPUTE_CV:\n    train['f1'] = train.apply(getMetric('oof_enet'),axis=1)\n    print('CV score for baseline =',train.f1.mean())","metadata":{"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Finding similar embeddings...\nCV score for baseline = 0.646752767970401\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# image CNN","metadata":{}},{"cell_type":"code","source":"from PIL import Image\n\nimport torch\ntorch.manual_seed(0)\ntorch.backends.cudnn.deterministic = False\ntorch.backends.cudnn.benchmark = True\n\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data.dataset import Dataset\n\n# 自定义一个数据集\n# 实例化d=ShopeeImageDataset()\n# d[10] getitem\n# len(d)\nclass ShopeeImageDataset(Dataset):\n    def __init__(self, img_path, transform):\n        self.img_path = img_path\n        self.transform = transform\n        \n    def __getitem__(self, index):\n        img = Image.open(self.img_path[index]).convert('RGB')\n        img = self.transform(img)\n        return img\n    \n    def __len__(self):\n        return len(self.img_path)","metadata":{"ExecuteTime":{"end_time":"2021-03-18T09:59:24.147684Z","start_time":"2021-03-18T09:59:23.6933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 实例化\n\nimagedataset = ShopeeImageDataset(\n    train['image'].values,\n    transforms.Compose([\n        transforms.Resize((512, 512)),\n        transforms.ToTensor(),# pillow->tensor,0-255=>0-1\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]))\n\n# dataloader批量读取，batch_size=10,shuffle不打乱\nimageloader = torch.utils.data.DataLoader(\n    imagedataset,\n    batch_size=10, shuffle=False, num_workers=2\n)","metadata":{"ExecuteTime":{"end_time":"2021-03-18T09:59:25.6502Z","start_time":"2021-03-18T09:59:25.64389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 用resnet18\nclass ShopeeImageEmbeddingNet(nn.Module):\n    def __init__(self):\n        super(ShopeeImageEmbeddingNet, self).__init__()\n        \n        #\n        model = models.resnet18(True) # True表示使用预训练参数\n        # mean-pooling=>max-pooling 会好一些\n        model.avgpool = nn.AdaptiveMaxPool2d(output_size=(1, 1))\n        \n        model = nn.Sequential(*list(model.children())[:-1])\n        \n        # 原始image_net1000类，不需要全连接，只需要embedding\n        model.eval()# 关闭bn，关闭dropout\n        self.model = model\n    # 正向传播\n    def forward(self, img):        \n        out = self.model(img)\n        return out","metadata":{"ExecuteTime":{"end_time":"2021-03-18T09:59:27.08827Z","start_time":"2021-03-18T09:59:27.083495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 本地与训练模型保存的文件夹\n!mkdir -p /root/.cache/torch/hub/checkpoints/\n!cp ../input/pretrained-pytorch-models/resnet18-5c106cde.pth /root/.cache/torch/hub/checkpoints/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = 'cuda'\n\nimgmodel = ShopeeImageEmbeddingNet()\nimgmodel = imgmodel.to(DEVICE)\n\nimagefeat = []\nwith torch.no_grad():\n    for data in tqdm_notebook(imageloader):\n        data = data.to(DEVICE)\n        feat = imgmodel(data)\n        \n        feat = feat.reshape(feat.shape[0], -1)\n        \n        feat = feat.data.cpu().numpy()\n        \n        imagefeat.append(feat)","metadata":{"ExecuteTime":{"end_time":"2021-03-18T10:01:20.420477Z","start_time":"2021-03-18T09:59:28.809744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import normalize\n\n# l2 norm to kill all the sim in 0-1\nimagefeat = np.vstack(imagefeat)\nimagefeat = normalize(imagefeat)","metadata":{"ExecuteTime":{"end_time":"2021-03-18T10:01:43.818543Z","start_time":"2021-03-18T10:01:43.401624Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nKNN = 50\nif len(test)==3: KNN = 2\nmodel = NearestNeighbors(n_neighbors=KNN)\nmodel.fit(imagefeat)\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = []\n# 4096一批计算相似度\nCHUNK = 1024*4\n\nimagefeat = cupy.array(imagefeat)\n\nprint('Finding similar images...')\nCTS = len(imagefeat)//CHUNK\nif len(imagefeat)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b, len(imagefeat))\n    print('chunk',a,'to',b)\n    \n    distances = cupy.matmul(imagefeat, imagefeat[a:b].T).T\n    # distances = np.dot(imagefeat[a:b,], imagefeat.T)\n    \n    for k in range(b-a):\n        # 如果相似度大于0.95就算\n        IDX = cupy.where(distances[k,]>0.95)[0]\n        # IDX = np.where(distances[k,]>0.95)[0][:]\n        o = train.iloc[cupy.asnumpy(IDX)].posting_id.values\n        preds.append(o)\n        \n# del imagefeat, imgmodel","metadata":{"ExecuteTime":{"end_time":"2021-03-18T10:01:54.771453Z","start_time":"2021-03-18T10:01:44.50243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['oof_cnn'] = preds\n\nif COMPUTE_CV:\n    train['f1'] = train.apply(getMetric('oof_cnn'),axis=1)\n    print('CV score for baseline =',train.f1.mean())","metadata":{"ExecuteTime":{"end_time":"2021-03-18T10:01:58.132852Z","start_time":"2021-03-18T10:01:56.678412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# title TFIDF","metadata":{}},{"cell_type":"code","source":"import string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom tqdm import tqdm\ntqdm.pandas()\n\ndef preprocess_text(sentence):\n    sentence = str(sentence)\n    sentence = sentence.lower()\n    # sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n    sentence = sentence.split()\n    sentence = [word for word in sentence if word not in stopwords.words('english')]\n    wl = WordNetLemmatizer()\n    sentence = [wl.lemmatize(word) for word in sentence]\n    sentence = ' '.join(sentence)\n    return sentence\n\ntrain['title_clean'] = train['title'].progress_apply(preprocess_text)","metadata":{"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/tqdm/std.py:701: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n  from pandas import Panel\n100%|██████████| 34250/34250 [00:38<00:00, 893.67it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_gf['title_clean'] = train['title_clean']\ntrain_gf","metadata":{"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"             posting_id                                 image  \\\n0       train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg   \n1      train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg   \n2      train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg   \n3      train_2406599165  00117e4fc239b1b641ff08340b429633.jpg   \n4      train_3369186413  00136d1cf4edede0203f32f05f660588.jpg   \n...                 ...                                   ...   \n34245  train_4028265689  fff1c07ceefc2c970a7964cfb81981c5.jpg   \n34246   train_769054909  fff401691371bdcb382a0d9075dfea6a.jpg   \n34247   train_614977732  fff421b78fa7284284724baf249f522e.jpg   \n34248  train_3630949769  fff51b87916dbfb6d0f8faa01bee67b8.jpg   \n34249  train_1792180725  ffffa0ab2ae542357671e96254fa7167.jpg   \n\n            image_phash                                              title  \\\n0      94974f937d4c2433                          Paper Bag Victoria Secret   \n1      af3f9460c2838f0f  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   \n2      b94cb00ed3e50f78        Maling TTS Canned Pork Luncheon Meat 397 gr   \n3      8514fc58eafea283  Daster Batik Lengan pendek - Motif Acak / Camp...   \n4      a6f319f924ad708c                  Nescafe \\xc3\\x89clair Latte 220ml   \n...                 ...                                                ...   \n34245  e3cd72389f248f21  Masker Bahan Kain Spunbond Non Woven 75 gsm 3 ...   \n34246  be86851f72e2853c    MamyPoko Pants Royal Soft - S 70 - Popok Celana   \n34247  ad27f0d08c0fcbf0  KHANZAACC Robot RE101S 1.2mm Subwoofer Bass Me...   \n34248  e3b13bd1d896c05c  Kaldu NON MSG HALAL Mama Kamu Ayam Kampung , S...   \n34249  af8bc4b2d2cf9083  FLEX TAPE PELAPIS BOCOR / ISOLASI AJAIB / ANTI...   \n\n       label_group                                             target  \\\n0        249114794                [train_129225211, train_2278313361]   \n1       2937985045               [train_3386243561, train_3423213080]   \n2       2395904891               [train_2288590299, train_3803689425]   \n3       4093212188               [train_2406599165, train_3342059966]   \n4       3648931069                [train_3369186413, train_921438619]   \n...            ...                                                ...   \n34245   3776555725               [train_2829161572, train_4028265689]   \n34246   2736479533                [train_1463059254, train_769054909]   \n34247   4101248785  [train_4126022211, train_3926241003, train_232...   \n34248   1663538013  [train_3419392575, train_1431563868, train_363...   \n34249    459464107                [train_795128312, train_1792180725]   \n\n                                             title_clean  \n0                              paper bag victoria secret  \n1      double tape 3m vhb 12 mm x 4,5 original / doub...  \n2             maling tt canned pork luncheon meat 397 gr  \n3      daster batik lengan pendek - motif acak / camp...  \n4                      nescafe \\xc3\\x89clair latte 220ml  \n...                                                  ...  \n34245  masker bahan kain spunbond non woven 75 gsm 3 ...  \n34246       mamypoko pant royal soft - 70 - popok celana  \n34247  khanzaacc robot re101s 1.2mm subwoofer bass me...  \n34248  kaldu non msg halal mama kamu ayam kampung , s...  \n34249  flex tape pelapis bocor / isolasi ajaib / anti...  \n\n[34250 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posting_id</th>\n      <th>image</th>\n      <th>image_phash</th>\n      <th>title</th>\n      <th>label_group</th>\n      <th>target</th>\n      <th>title_clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_129225211</td>\n      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n      <td>94974f937d4c2433</td>\n      <td>Paper Bag Victoria Secret</td>\n      <td>249114794</td>\n      <td>[train_129225211, train_2278313361]</td>\n      <td>paper bag victoria secret</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_3386243561</td>\n      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n      <td>af3f9460c2838f0f</td>\n      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n      <td>2937985045</td>\n      <td>[train_3386243561, train_3423213080]</td>\n      <td>double tape 3m vhb 12 mm x 4,5 original / doub...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2288590299</td>\n      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n      <td>b94cb00ed3e50f78</td>\n      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n      <td>2395904891</td>\n      <td>[train_2288590299, train_3803689425]</td>\n      <td>maling tt canned pork luncheon meat 397 gr</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_2406599165</td>\n      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n      <td>8514fc58eafea283</td>\n      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n      <td>4093212188</td>\n      <td>[train_2406599165, train_3342059966]</td>\n      <td>daster batik lengan pendek - motif acak / camp...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_3369186413</td>\n      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n      <td>a6f319f924ad708c</td>\n      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n      <td>3648931069</td>\n      <td>[train_3369186413, train_921438619]</td>\n      <td>nescafe \\xc3\\x89clair latte 220ml</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>34245</th>\n      <td>train_4028265689</td>\n      <td>fff1c07ceefc2c970a7964cfb81981c5.jpg</td>\n      <td>e3cd72389f248f21</td>\n      <td>Masker Bahan Kain Spunbond Non Woven 75 gsm 3 ...</td>\n      <td>3776555725</td>\n      <td>[train_2829161572, train_4028265689]</td>\n      <td>masker bahan kain spunbond non woven 75 gsm 3 ...</td>\n    </tr>\n    <tr>\n      <th>34246</th>\n      <td>train_769054909</td>\n      <td>fff401691371bdcb382a0d9075dfea6a.jpg</td>\n      <td>be86851f72e2853c</td>\n      <td>MamyPoko Pants Royal Soft - S 70 - Popok Celana</td>\n      <td>2736479533</td>\n      <td>[train_1463059254, train_769054909]</td>\n      <td>mamypoko pant royal soft - 70 - popok celana</td>\n    </tr>\n    <tr>\n      <th>34247</th>\n      <td>train_614977732</td>\n      <td>fff421b78fa7284284724baf249f522e.jpg</td>\n      <td>ad27f0d08c0fcbf0</td>\n      <td>KHANZAACC Robot RE101S 1.2mm Subwoofer Bass Me...</td>\n      <td>4101248785</td>\n      <td>[train_4126022211, train_3926241003, train_232...</td>\n      <td>khanzaacc robot re101s 1.2mm subwoofer bass me...</td>\n    </tr>\n    <tr>\n      <th>34248</th>\n      <td>train_3630949769</td>\n      <td>fff51b87916dbfb6d0f8faa01bee67b8.jpg</td>\n      <td>e3b13bd1d896c05c</td>\n      <td>Kaldu NON MSG HALAL Mama Kamu Ayam Kampung , S...</td>\n      <td>1663538013</td>\n      <td>[train_3419392575, train_1431563868, train_363...</td>\n      <td>kaldu non msg halal mama kamu ayam kampung , s...</td>\n    </tr>\n    <tr>\n      <th>34249</th>\n      <td>train_1792180725</td>\n      <td>ffffa0ab2ae542357671e96254fa7167.jpg</td>\n      <td>af8bc4b2d2cf9083</td>\n      <td>FLEX TAPE PELAPIS BOCOR / ISOLASI AJAIB / ANTI...</td>\n      <td>459464107</td>\n      <td>[train_795128312, train_1792180725]</td>\n      <td>flex tape pelapis bocor / isolasi ajaib / anti...</td>\n    </tr>\n  </tbody>\n</table>\n<p>34250 rows × 7 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# from sklearn.feature_extraction.text import TfidfVectorizer\n# 25000 \nmodel = TfidfVectorizer(stop_words=None, binary=True, max_features=25000)\n# text_embeddings = model.fit_transform(train_gf.title).toarray()\ntext_embeddings = model.fit_transform(train_gf.title_clean).toarray()\nprint('text embeddings shape',text_embeddings.shape)","metadata":{"ExecuteTime":{"end_time":"2021-03-18T10:02:00.631468Z","start_time":"2021-03-18T10:01:59.851964Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"text embeddings shape (34250, 24652)\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_text_preds(df, embeddings, threshold=0.75, PRINT_CHUNK=False):\n    print('Finding similar titles...')\n    CHUNK = 1024 * 4\n    CTS = len(df) // CHUNK\n    if (len(df)%CHUNK) != 0:\n        CTS += 1\n\n    preds = []\n    for j in range( CTS ):\n        a = j * CHUNK\n        b = (j+1) * CHUNK\n        b = min(b, len(df))\n        if PRINT_CHUNK:\n            print('chunk', a, 'to', b)\n\n        # COSINE SIMILARITY DISTANCE\n        cts = cupy.matmul(embeddings, embeddings[a:b].T).T\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>threshold)[0]\n            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            preds.append(o)\n            \n    return preds","metadata":{"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"def cos_threshold_searching(df, embeddings, LB=0.70, UB=0.80):\n    df1 = pd.DataFrame(columns = ['target', 'pred_matches'])\n    df1.target = df.target\n        \n    thresholds = list(np.arange(LB, UB, 0.02))\n    scores = []\n    for threshold in thresholds:\n        preds = get_text_preds(df, embeddings, threshold)\n        df1.pred_matches = preds\n        MyCVScore = df1.apply(getMetric('pred_matches'), axis=1)\n        score = MyCVScore.mean()\n        print(f'CV score for threshold {round(threshold, 2)} = {score}')\n        scores.append(score)\n    thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n    max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n    best_threshold = max_score['thresholds'].values[0]\n    best_score = max_score['scores'].values[0]\n    print(f'Our best score is {best_score} and has a threshold {round(best_threshold, 2)}')","metadata":{"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"cos_threshold_searching(train, text_embeddings, LB=0.50, UB=0.60)","metadata":{"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Finding similar titles...\nCV score for threshold 0.5 = 0.6591369293684559\nFinding similar titles...\nCV score for threshold 0.52 = 0.6610204806839047\nFinding similar titles...\nCV score for threshold 0.54 = 0.6614022356034018\nFinding similar titles...\nCV score for threshold 0.56 = 0.660011710952917\nFinding similar titles...\nCV score for threshold 0.58 = 0.6569429099082632\nOur best score is 0.6614022356034018 and has a threshold 0.54\n","output_type":"stream"}]},{"cell_type":"code","source":"preds = get_text_preds(train, text_embeddings, threshold=0.54, PRINT_CHUNK=False)\ntrain['oof_text'] = preds\n\nif COMPUTE_CV:\n    train['f1'] = train.apply(getMetric('oof_text'),axis=1)\n    print('CV score for baseline =',train.f1.mean())","metadata":{"ExecuteTime":{"end_time":"2021-03-18T10:06:03.146166Z","start_time":"2021-03-18T10:06:01.83687Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"Finding similar titles...\nCV score for baseline = 0.6614022356034018\n","output_type":"stream"}]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"         posting_id                                 image       image_phash  \\\n0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n\n                                               title  label_group  \\\n0                          Paper Bag Victoria Secret    249114794   \n1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891   \n3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188   \n4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069   \n\n                                 target                              oof_enet  \\\n0   [train_129225211, train_2278313361]                     [train_129225211]   \n1  [train_3386243561, train_3423213080]  [train_3386243561, train_3423213080]   \n2  [train_2288590299, train_3803689425]                    [train_2288590299]   \n3  [train_2406599165, train_3342059966]                    [train_2406599165]   \n4   [train_3369186413, train_921438619]   [train_3369186413, train_921438619]   \n\n         f1                                        title_clean  \\\n0  1.000000                          paper bag victoria secret   \n1  0.666667  double tape 3m vhb 12 mm x 4,5 original / doub...   \n2  1.000000         maling tt canned pork luncheon meat 397 gr   \n3  0.200000  daster batik lengan pendek - motif acak / camp...   \n4  0.666667                  nescafe \\xc3\\x89clair latte 220ml   \n\n                                            oof_text  \n0                [train_129225211, train_2278313361]  \n1  [train_3386243561, train_3423213080, train_183...  \n2               [train_2288590299, train_3803689425]  \n3  [train_2406599165, train_3576714541, train_150...  \n4                                 [train_3369186413]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posting_id</th>\n      <th>image</th>\n      <th>image_phash</th>\n      <th>title</th>\n      <th>label_group</th>\n      <th>target</th>\n      <th>oof_enet</th>\n      <th>f1</th>\n      <th>title_clean</th>\n      <th>oof_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_129225211</td>\n      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n      <td>94974f937d4c2433</td>\n      <td>Paper Bag Victoria Secret</td>\n      <td>249114794</td>\n      <td>[train_129225211, train_2278313361]</td>\n      <td>[train_129225211]</td>\n      <td>1.000000</td>\n      <td>paper bag victoria secret</td>\n      <td>[train_129225211, train_2278313361]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_3386243561</td>\n      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n      <td>af3f9460c2838f0f</td>\n      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n      <td>2937985045</td>\n      <td>[train_3386243561, train_3423213080]</td>\n      <td>[train_3386243561, train_3423213080]</td>\n      <td>0.666667</td>\n      <td>double tape 3m vhb 12 mm x 4,5 original / doub...</td>\n      <td>[train_3386243561, train_3423213080, train_183...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2288590299</td>\n      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n      <td>b94cb00ed3e50f78</td>\n      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n      <td>2395904891</td>\n      <td>[train_2288590299, train_3803689425]</td>\n      <td>[train_2288590299]</td>\n      <td>1.000000</td>\n      <td>maling tt canned pork luncheon meat 397 gr</td>\n      <td>[train_2288590299, train_3803689425]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_2406599165</td>\n      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n      <td>8514fc58eafea283</td>\n      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n      <td>4093212188</td>\n      <td>[train_2406599165, train_3342059966]</td>\n      <td>[train_2406599165]</td>\n      <td>0.200000</td>\n      <td>daster batik lengan pendek - motif acak / camp...</td>\n      <td>[train_2406599165, train_3576714541, train_150...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_3369186413</td>\n      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n      <td>a6f319f924ad708c</td>\n      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n      <td>3648931069</td>\n      <td>[train_3369186413, train_921438619]</td>\n      <td>[train_3369186413, train_921438619]</td>\n      <td>0.666667</td>\n      <td>nescafe \\xc3\\x89clair latte 220ml</td>\n      <td>[train_3369186413]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def combine_for_sub(row):\n    # x = np.concatenate([row.oof_text,row.oof_cnn, row.oof_hash])\n    x = np.concatenate([row.oof_text, row.oof_enet])\n    return ' '.join( np.unique(x) )\n\ndef combine_for_cv(row):\n    # x = np.concatenate([row.oof_text,row.oof_cnn, row.oof_hash])\n    x = np.concatenate([row.oof_text, row.oof_enet])\n    return np.unique(x)","metadata":{"ExecuteTime":{"end_time":"2021-03-18T10:06:04.931476Z","start_time":"2021-03-18T10:06:04.925838Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"if COMPUTE_CV:\n    # tmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\n    # train['target'] = train.label_group.map(tmp)\n    train['oof'] = train.apply(combine_for_cv,axis=1)\n    train['f1'] = train.apply(getMetric('oof'),axis=1)\n    print('CV Score =', train.f1.mean() )\n\ntrain['matches'] = train.apply(combine_for_sub,axis=1)","metadata":{"ExecuteTime":{"end_time":"2021-03-18T10:06:09.759812Z","start_time":"2021-03-18T10:06:05.955972Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"CV Score = 0.7260622833279889\n","output_type":"stream"}]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"         posting_id                                 image       image_phash  \\\n0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n\n                                               title  label_group  \\\n0                          Paper Bag Victoria Secret    249114794   \n1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891   \n3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188   \n4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069   \n\n                                 target                              oof_enet  \\\n0   [train_129225211, train_2278313361]                     [train_129225211]   \n1  [train_3386243561, train_3423213080]  [train_3386243561, train_3423213080]   \n2  [train_2288590299, train_3803689425]                    [train_2288590299]   \n3  [train_2406599165, train_3342059966]                    [train_2406599165]   \n4   [train_3369186413, train_921438619]   [train_3369186413, train_921438619]   \n\n         f1                                        title_clean  \\\n0  1.000000                          paper bag victoria secret   \n1  0.666667  double tape 3m vhb 12 mm x 4,5 original / doub...   \n2  1.000000         maling tt canned pork luncheon meat 397 gr   \n3  0.200000  daster batik lengan pendek - motif acak / camp...   \n4  1.000000                  nescafe \\xc3\\x89clair latte 220ml   \n\n                                            oof_text  \\\n0                [train_129225211, train_2278313361]   \n1  [train_3386243561, train_3423213080, train_183...   \n2               [train_2288590299, train_3803689425]   \n3  [train_2406599165, train_3576714541, train_150...   \n4                                 [train_3369186413]   \n\n                                                 oof  \\\n0                [train_129225211, train_2278313361]   \n1  [train_1831941588, train_3386243561, train_342...   \n2               [train_2288590299, train_3803689425]   \n3  [train_1508100548, train_1744956981, train_204...   \n4                [train_3369186413, train_921438619]   \n\n                                             matches  \n0                   train_129225211 train_2278313361  \n1  train_1831941588 train_3386243561 train_342321...  \n2                  train_2288590299 train_3803689425  \n3  train_1508100548 train_1744956981 train_204309...  \n4                   train_3369186413 train_921438619  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posting_id</th>\n      <th>image</th>\n      <th>image_phash</th>\n      <th>title</th>\n      <th>label_group</th>\n      <th>target</th>\n      <th>oof_enet</th>\n      <th>f1</th>\n      <th>title_clean</th>\n      <th>oof_text</th>\n      <th>oof</th>\n      <th>matches</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_129225211</td>\n      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n      <td>94974f937d4c2433</td>\n      <td>Paper Bag Victoria Secret</td>\n      <td>249114794</td>\n      <td>[train_129225211, train_2278313361]</td>\n      <td>[train_129225211]</td>\n      <td>1.000000</td>\n      <td>paper bag victoria secret</td>\n      <td>[train_129225211, train_2278313361]</td>\n      <td>[train_129225211, train_2278313361]</td>\n      <td>train_129225211 train_2278313361</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_3386243561</td>\n      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n      <td>af3f9460c2838f0f</td>\n      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n      <td>2937985045</td>\n      <td>[train_3386243561, train_3423213080]</td>\n      <td>[train_3386243561, train_3423213080]</td>\n      <td>0.666667</td>\n      <td>double tape 3m vhb 12 mm x 4,5 original / doub...</td>\n      <td>[train_3386243561, train_3423213080, train_183...</td>\n      <td>[train_1831941588, train_3386243561, train_342...</td>\n      <td>train_1831941588 train_3386243561 train_342321...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2288590299</td>\n      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n      <td>b94cb00ed3e50f78</td>\n      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n      <td>2395904891</td>\n      <td>[train_2288590299, train_3803689425]</td>\n      <td>[train_2288590299]</td>\n      <td>1.000000</td>\n      <td>maling tt canned pork luncheon meat 397 gr</td>\n      <td>[train_2288590299, train_3803689425]</td>\n      <td>[train_2288590299, train_3803689425]</td>\n      <td>train_2288590299 train_3803689425</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_2406599165</td>\n      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n      <td>8514fc58eafea283</td>\n      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n      <td>4093212188</td>\n      <td>[train_2406599165, train_3342059966]</td>\n      <td>[train_2406599165]</td>\n      <td>0.200000</td>\n      <td>daster batik lengan pendek - motif acak / camp...</td>\n      <td>[train_2406599165, train_3576714541, train_150...</td>\n      <td>[train_1508100548, train_1744956981, train_204...</td>\n      <td>train_1508100548 train_1744956981 train_204309...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_3369186413</td>\n      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n      <td>a6f319f924ad708c</td>\n      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n      <td>3648931069</td>\n      <td>[train_3369186413, train_921438619]</td>\n      <td>[train_3369186413, train_921438619]</td>\n      <td>1.000000</td>\n      <td>nescafe \\xc3\\x89clair latte 220ml</td>\n      <td>[train_3369186413]</td>\n      <td>[train_3369186413, train_921438619]</td>\n      <td>train_3369186413 train_921438619</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train[['posting_id','matches']].to_csv('submission.csv',index=False)\nsub = pd.read_csv('submission.csv')\nsub.head()","metadata":{"ExecuteTime":{"end_time":"2021-03-18T10:06:12.385916Z","start_time":"2021-03-18T10:06:12.180234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}