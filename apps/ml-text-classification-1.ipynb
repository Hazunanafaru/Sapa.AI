{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T09:59:13.247406Z",
     "start_time": "2021-03-18T09:59:13.24369Z"
    }
   },
   "outputs": [],
   "source": [
    "# DATA_PATH = '../input/'\n",
    "# 数据输入路径\n",
    "DATA_PATH = '../input/shopee-product-matching/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T09:59:14.869532Z",
     "start_time": "2021-03-18T09:59:14.482759Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd, gc # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import cv2, matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import cudf, cuml, cupy\n",
    "from cuml.feature_extraction.text import TfidfVectorizer\n",
    "from cuml.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算F1 score\n",
    "def getMetric(col):\n",
    "    def f1score(row):\n",
    "        n = len( np.intersect1d(row.target,row[col]) )\n",
    "        return 2*n / (len(row.target)+len(row[col]))\n",
    "    return f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T09:59:15.92512Z",
     "start_time": "2021-03-18T09:59:15.308672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this submission notebook will compute CV score, but commit notebook will not\n",
      "train shape is (34250, 6)\n"
     ]
    }
   ],
   "source": [
    "# 标识是不是计算验证得分\n",
    "COMPUTE_CV = True\n",
    "\n",
    "test = pd.read_csv(DATA_PATH + 'test.csv')\n",
    "if len(test)>3: COMPUTE_CV = False\n",
    "# 如果测试集大于3行，进入提交模式\n",
    "else: print('this submission notebook will compute CV score, but commit notebook will not')\n",
    "\n",
    "# COMPUTE_CV = False\n",
    "\n",
    "if COMPUTE_CV:\n",
    "    train = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "    # train['image'] = DATA_PATH + 'train_images/' + train['image']\n",
    "    tmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\n",
    "    train['target'] = train.label_group.map(tmp)\n",
    "    # train_gf = cudf.read_csv(DATA_PATH + 'train.csv')\n",
    "    train_gf = cudf.DataFrame(train)\n",
    "else:\n",
    "    train = pd.read_csv(DATA_PATH + 'test.csv')\n",
    "    # train['image'] = DATA_PATH + 'test_images/' + train['image']\n",
    "    # train_gf = cudf.read_csv(DATA_PATH + 'test.csv')\n",
    "    train_gf = cudf.DataFrame(train)\n",
    "    \n",
    "print('train shape is', train.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T09:59:19.569052Z",
     "start_time": "2021-03-18T09:59:18.284395Z"
    }
   },
   "outputs": [],
   "source": [
    "# 相同哈希值当作一组\n",
    "tmp = train.groupby('image_phash').posting_id.agg('unique').to_dict()\n",
    "train['oof_hash'] = train.image_phash.map(tmp)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T09:59:21.98207Z",
     "start_time": "2021-03-18T09:59:20.62671Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if COMPUTE_CV:\n",
    "    train['f1'] = train.apply(getMetric('oof_hash'),axis=1)\n",
    "    print('CV score for baseline =',train.f1.mean())\n",
    "\n",
    "# 聚类包括自己的，应该是大于0.5的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficientnetb0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will restrict TensorFlow to max 1GB GPU RAM\n",
      "then RAPIDS can use 15GB GPU RAM\n"
     ]
    }
   ],
   "source": [
    "# RESTRICT TENSORFLOW TO 1GB OF GPU RAM\n",
    "# SO THAT WE HAVE 15GB RAM FOR RAPIDS\n",
    "LIMIT = 1\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    #print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "print('We will restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\n",
    "print('then RAPIDS can use %iGB GPU RAM'%(16-LIMIT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, df, img_size=256, batch_size=32, path=''): \n",
    "        self.df = df\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.path = path\n",
    "        self.indexes = np.arange( len(self.df) )\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        ct = len(self.df) // self.batch_size\n",
    "        ct += int(( (len(self.df)) % self.batch_size)!=0)\n",
    "        return ct\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X = self.__data_generation(indexes)\n",
    "        return X\n",
    "            \n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' \n",
    "        X = np.zeros((len(indexes),self.img_size,self.img_size,3),dtype='float32')\n",
    "        df = self.df.iloc[indexes]\n",
    "        for i,(index,row) in enumerate(df.iterrows()):\n",
    "            img = cv2.imread(self.path+row.image)\n",
    "            X[i,] = cv2.resize(img,(self.img_size,self.img_size)) #/128.0 - 1.0\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg', input_shape=None)\n",
    "# model.save_weights('efficientnetb0_notop.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing image embeddings...\n",
      "chunk 0 to 4096\n",
      "128/128 [==============================] - 35s 250ms/step\n",
      "chunk 4096 to 8192\n",
      "128/128 [==============================] - 35s 262ms/step\n",
      "chunk 8192 to 12288\n",
      "128/128 [==============================] - 34s 254ms/step\n",
      "chunk 12288 to 16384\n",
      "128/128 [==============================] - 34s 255ms/step\n",
      "chunk 16384 to 20480\n",
      "128/128 [==============================] - 36s 271ms/step\n",
      "chunk 20480 to 24576\n",
      "128/128 [==============================] - 36s 261ms/step\n",
      "chunk 24576 to 28672\n",
      "128/128 [==============================] - 34s 260ms/step\n",
      "chunk 28672 to 32768\n",
      "128/128 [==============================] - 34s 259ms/step\n",
      "chunk 32768 to 34250\n",
      "47/47 [==============================] - 13s 256ms/step\n",
      "image embeddings shape (34250, 1280)\n"
     ]
    }
   ],
   "source": [
    "BASE = '../input/shopee-product-matching/test_images/'\n",
    "if COMPUTE_CV: BASE = '../input/shopee-product-matching/train_images/'\n",
    "\n",
    "WGT = '../input/effnetb0/efficientnetb0_notop.h5'\n",
    "model = EfficientNetB0(weights=WGT,include_top=False, pooling='avg', input_shape=None)\n",
    "\n",
    "embeds = []\n",
    "CHUNK = 1024*4\n",
    "\n",
    "print('Computing image embeddings...')\n",
    "CTS = len(train)//CHUNK\n",
    "if len(train)%CHUNK!=0: CTS += 1\n",
    "for i,j in enumerate( range( CTS ) ):\n",
    "    \n",
    "    a = j*CHUNK\n",
    "    b = (j+1)*CHUNK\n",
    "    b = min(b,len(train))\n",
    "    print('chunk',a,'to',b)\n",
    "    \n",
    "    test_gen = DataGenerator(train.iloc[a:b], batch_size=32, path=BASE)\n",
    "    image_embeddings = model.predict(test_gen,verbose=1,use_multiprocessing=True, workers=4)\n",
    "    embeds.append(image_embeddings)\n",
    "\n",
    "    #if i>=1: break\n",
    "    \n",
    "del model\n",
    "_ = gc.collect()\n",
    "image_embeddings = np.concatenate(embeds)\n",
    "print('image embeddings shape',image_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(n_neighbors=50, verbose=4, handle=<cuml.raft.common.handle.Handle object at 0x7fcf4a816fb0>, algorithm='brute', metric='euclidean', p=2, metric_params=None, output_type='numpy')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN = 50\n",
    "if len(train)==3: KNN = 2\n",
    "model = NearestNeighbors(n_neighbors=KNN)\n",
    "model.fit(image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK = 1024*4\n",
    "distance = []\n",
    "\n",
    "print('Finding similar images...')\n",
    "CTS = len(image_embeddings)//CHUNK\n",
    "if len(image_embeddings)%CHUNK!=0: CTS += 1\n",
    "for j in range( CTS ):\n",
    "    \n",
    "    a = j*CHUNK\n",
    "    b = (j+1)*CHUNK\n",
    "    b = min(b,len(image_embeddings))\n",
    "    print('chunk',a,'to',b)\n",
    "    distances, indices = model.kneighbors(image_embeddings[a:b,])\n",
    "    distance.append(distances)\n",
    "    \n",
    "temp = np.array(distance)\n",
    "plt.hist(temp.flatten(),bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that there is a tail to the left indicating duplicating items in the histogram. The spike at dist = 0 are the images that are exact duplicates and low numbers are near duplicates. So choosing 6.0 will cut that tail off.\n",
    "\n",
    "Second, we try different distances and choose the one that maximizes CV score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knn_preds(df, embeddings, knnmodel, threshold = 6.0, PRINT_CHUNK=False):\n",
    "    preds = []\n",
    "    CHUNK = 1024*4\n",
    "\n",
    "    print('Finding similar embeddings...')\n",
    "    CTS = len(embeddings)//CHUNK\n",
    "    if len(embeddings)%CHUNK!=0: CTS += 1\n",
    "    for j in range( CTS ):\n",
    "\n",
    "        a = j*CHUNK\n",
    "        b = (j+1)*CHUNK\n",
    "        b = min(b,len(embeddings))\n",
    "        if PRINT_CHUNK:\n",
    "            print('chunk', a, 'to', b)\n",
    "\n",
    "        distances, indices = knnmodel.kneighbors(embeddings[a:b,])\n",
    "\n",
    "        for k in range(b-a):\n",
    "            IDX = np.where(distances[k,]<threshold)[0]\n",
    "            IDS = indices[k,IDX]\n",
    "            o = df.iloc[IDS].posting_id.values\n",
    "            preds.append(o)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_threshold_searching(df, embeddings, knnmodel, LB=4.0, UB=7.0):\n",
    "    df1 = pd.DataFrame(columns = ['target', 'pred_matches'])\n",
    "    df1.target = df.target\n",
    "        \n",
    "    thresholds = list(np.arange(LB, UB, 0.2))\n",
    "    scores = []\n",
    "    for threshold in thresholds:\n",
    "        preds = get_knn_preds(df, embeddings, knnmodel, threshold)\n",
    "        df1.pred_matches = preds\n",
    "        MyCVScore = df1.apply(getMetric('pred_matches'), axis=1)\n",
    "        score = MyCVScore.mean()\n",
    "        print(f'CV score for threshold {round(threshold, 2)} = {score}')\n",
    "        scores.append(score)\n",
    "    thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n",
    "    max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n",
    "    best_threshold = max_score['thresholds'].values[0]\n",
    "    best_score = max_score['scores'].values[0]\n",
    "    print(f'Our best score is {best_score} and has a threshold {round(best_threshold, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar embeddings...\n",
      "CV score for threshold 4.0 = 0.604331913022176\n",
      "Finding similar embeddings...\n",
      "CV score for threshold 4.2 = 0.6085015290133909\n",
      "Finding similar embeddings...\n",
      "CV score for threshold 4.4 = 0.6126903603905952\n",
      "Finding similar embeddings...\n",
      "CV score for threshold 4.6 = 0.6166444106514297\n",
      "Finding similar embeddings...\n",
      "CV score for threshold 4.8 = 0.6200762311910106\n",
      "Finding similar embeddings...\n",
      "CV score for threshold 5.0 = 0.6241102965641212\n",
      "Finding similar embeddings...\n",
      "CV score for threshold 5.2 = 0.6275635861413381\n",
      "Finding similar embeddings...\n",
      "CV score for threshold 5.4 = 0.630622327260816\n",
      "Finding similar embeddings...\n",
      "CV score for threshold 5.6 = 0.6340223808341016\n",
      "Finding similar embeddings...\n",
      "CV score for threshold 5.8 = 0.6369304688092922\n",
      "Finding similar embeddings...\n",
      "CV score for threshold 6.0 = 0.6393196101848189\n",
      "Finding similar embeddings...\n",
      "CV score for threshold 6.2 = 0.6421880171402373\n",
      "Finding similar embeddings...\n",
      "CV score for threshold 6.4 = 0.6439851560811659\n",
      "Finding similar embeddings...\n",
      "CV score for threshold 6.6 = 0.6454957565448419\n",
      "Finding similar embeddings...\n",
      "CV score for threshold 6.8 = 0.6466830060536779\n",
      "Finding similar embeddings...\n",
      "CV score for threshold 7.0 = 0.646752767970401\n",
      "Finding similar embeddings...\n",
      "CV score for threshold 7.2 = 0.6456601353216277\n",
      "Finding similar embeddings...\n",
      "CV score for threshold 7.4 = 0.6436746778417369\n",
      "Finding similar embeddings...\n",
      "CV score for threshold 7.6 = 0.6412465710814036\n",
      "Finding similar embeddings...\n",
      "CV score for threshold 7.8 = 0.637586985426812\n",
      "Our best score is 0.646752767970401 and has a threshold 7.0\n"
     ]
    }
   ],
   "source": [
    "distance_threshold_searching(train, image_embeddings, model, LB=6.0, UB=7.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar embeddings...\n",
      "CV score for baseline = 0.646752767970401\n"
     ]
    }
   ],
   "source": [
    "preds = get_knn_preds(train, image_embeddings, model, threshold=7.0)\n",
    "train['oof_enet'] = preds\n",
    "\n",
    "if COMPUTE_CV:\n",
    "    train['f1'] = train.apply(getMetric('oof_enet'),axis=1)\n",
    "    print('CV score for baseline =',train.f1.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T09:59:24.147684Z",
     "start_time": "2021-03-18T09:59:23.6933Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "# 自定义一个数据集\n",
    "# 实例化d=ShopeeImageDataset()\n",
    "# d[10] getitem\n",
    "# len(d)\n",
    "class ShopeeImageDataset(Dataset):\n",
    "    def __init__(self, img_path, transform):\n",
    "        self.img_path = img_path\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.img_path[index]).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T09:59:25.6502Z",
     "start_time": "2021-03-18T09:59:25.64389Z"
    }
   },
   "outputs": [],
   "source": [
    "# 实例化\n",
    "\n",
    "imagedataset = ShopeeImageDataset(\n",
    "    train['image'].values,\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.ToTensor(),# pillow->tensor,0-255=>0-1\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "]))\n",
    "\n",
    "# dataloader批量读取，batch_size=10,shuffle不打乱\n",
    "imageloader = torch.utils.data.DataLoader(\n",
    "    imagedataset,\n",
    "    batch_size=10, shuffle=False, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T09:59:27.08827Z",
     "start_time": "2021-03-18T09:59:27.083495Z"
    }
   },
   "outputs": [],
   "source": [
    "# 用resnet18\n",
    "class ShopeeImageEmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ShopeeImageEmbeddingNet, self).__init__()\n",
    "        \n",
    "        #\n",
    "        model = models.resnet18(True) # True表示使用预训练参数\n",
    "        # mean-pooling=>max-pooling 会好一些\n",
    "        model.avgpool = nn.AdaptiveMaxPool2d(output_size=(1, 1))\n",
    "        \n",
    "        model = nn.Sequential(*list(model.children())[:-1])\n",
    "        \n",
    "        # 原始image_net1000类，不需要全连接，只需要embedding\n",
    "        model.eval()# 关闭bn，关闭dropout\n",
    "        self.model = model\n",
    "    # 正向传播\n",
    "    def forward(self, img):        \n",
    "        out = self.model(img)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本地与训练模型保存的文件夹\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints/\n",
    "!cp ../input/pretrained-pytorch-models/resnet18-5c106cde.pth /root/.cache/torch/hub/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T10:01:20.420477Z",
     "start_time": "2021-03-18T09:59:28.809744Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "\n",
    "imgmodel = ShopeeImageEmbeddingNet()\n",
    "imgmodel = imgmodel.to(DEVICE)\n",
    "\n",
    "imagefeat = []\n",
    "with torch.no_grad():\n",
    "    for data in tqdm_notebook(imageloader):\n",
    "        data = data.to(DEVICE)\n",
    "        feat = imgmodel(data)\n",
    "        \n",
    "        feat = feat.reshape(feat.shape[0], -1)\n",
    "        \n",
    "        feat = feat.data.cpu().numpy()\n",
    "        \n",
    "        imagefeat.append(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T10:01:43.818543Z",
     "start_time": "2021-03-18T10:01:43.401624Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# l2 norm to kill all the sim in 0-1\n",
    "imagefeat = np.vstack(imagefeat)\n",
    "imagefeat = normalize(imagefeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "KNN = 50\n",
    "if len(test)==3: KNN = 2\n",
    "model = NearestNeighbors(n_neighbors=KNN)\n",
    "model.fit(imagefeat)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T10:01:54.771453Z",
     "start_time": "2021-03-18T10:01:44.50243Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "# 4096一批计算相似度\n",
    "CHUNK = 1024*4\n",
    "\n",
    "imagefeat = cupy.array(imagefeat)\n",
    "\n",
    "print('Finding similar images...')\n",
    "CTS = len(imagefeat)//CHUNK\n",
    "if len(imagefeat)%CHUNK!=0: CTS += 1\n",
    "for j in range( CTS ):\n",
    "    \n",
    "    a = j*CHUNK\n",
    "    b = (j+1)*CHUNK\n",
    "    b = min(b, len(imagefeat))\n",
    "    print('chunk',a,'to',b)\n",
    "    \n",
    "    distances = cupy.matmul(imagefeat, imagefeat[a:b].T).T\n",
    "    # distances = np.dot(imagefeat[a:b,], imagefeat.T)\n",
    "    \n",
    "    for k in range(b-a):\n",
    "        # 如果相似度大于0.95就算\n",
    "        IDX = cupy.where(distances[k,]>0.95)[0]\n",
    "        # IDX = np.where(distances[k,]>0.95)[0][:]\n",
    "        o = train.iloc[cupy.asnumpy(IDX)].posting_id.values\n",
    "        preds.append(o)\n",
    "        \n",
    "# del imagefeat, imgmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T10:01:58.132852Z",
     "start_time": "2021-03-18T10:01:56.678412Z"
    }
   },
   "outputs": [],
   "source": [
    "train['oof_cnn'] = preds\n",
    "\n",
    "if COMPUTE_CV:\n",
    "    train['f1'] = train.apply(getMetric('oof_cnn'),axis=1)\n",
    "    print('CV score for baseline =',train.f1.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# title TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/std.py:701: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "100%|██████████| 34250/34250 [00:38<00:00, 893.67it/s]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def preprocess_text(sentence):\n",
    "    sentence = str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    # sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    sentence = sentence.split()\n",
    "    sentence = [word for word in sentence if word not in stopwords.words('english')]\n",
    "    wl = WordNetLemmatizer()\n",
    "    sentence = [wl.lemmatize(word) for word in sentence]\n",
    "    sentence = ' '.join(sentence)\n",
    "    return sentence\n",
    "\n",
    "train['title_clean'] = train['title'].progress_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>target</th>\n",
       "      <th>title_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "      <td>[train_129225211, train_2278313361]</td>\n",
       "      <td>paper bag victoria secret</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "      <td>[train_3386243561, train_3423213080]</td>\n",
       "      <td>double tape 3m vhb 12 mm x 4,5 original / doub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2395904891</td>\n",
       "      <td>[train_2288590299, train_3803689425]</td>\n",
       "      <td>maling tt canned pork luncheon meat 397 gr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>4093212188</td>\n",
       "      <td>[train_2406599165, train_3342059966]</td>\n",
       "      <td>daster batik lengan pendek - motif acak / camp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>3648931069</td>\n",
       "      <td>[train_3369186413, train_921438619]</td>\n",
       "      <td>nescafe \\xc3\\x89clair latte 220ml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34245</th>\n",
       "      <td>train_4028265689</td>\n",
       "      <td>fff1c07ceefc2c970a7964cfb81981c5.jpg</td>\n",
       "      <td>e3cd72389f248f21</td>\n",
       "      <td>Masker Bahan Kain Spunbond Non Woven 75 gsm 3 ...</td>\n",
       "      <td>3776555725</td>\n",
       "      <td>[train_2829161572, train_4028265689]</td>\n",
       "      <td>masker bahan kain spunbond non woven 75 gsm 3 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34246</th>\n",
       "      <td>train_769054909</td>\n",
       "      <td>fff401691371bdcb382a0d9075dfea6a.jpg</td>\n",
       "      <td>be86851f72e2853c</td>\n",
       "      <td>MamyPoko Pants Royal Soft - S 70 - Popok Celana</td>\n",
       "      <td>2736479533</td>\n",
       "      <td>[train_1463059254, train_769054909]</td>\n",
       "      <td>mamypoko pant royal soft - 70 - popok celana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34247</th>\n",
       "      <td>train_614977732</td>\n",
       "      <td>fff421b78fa7284284724baf249f522e.jpg</td>\n",
       "      <td>ad27f0d08c0fcbf0</td>\n",
       "      <td>KHANZAACC Robot RE101S 1.2mm Subwoofer Bass Me...</td>\n",
       "      <td>4101248785</td>\n",
       "      <td>[train_4126022211, train_3926241003, train_232...</td>\n",
       "      <td>khanzaacc robot re101s 1.2mm subwoofer bass me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34248</th>\n",
       "      <td>train_3630949769</td>\n",
       "      <td>fff51b87916dbfb6d0f8faa01bee67b8.jpg</td>\n",
       "      <td>e3b13bd1d896c05c</td>\n",
       "      <td>Kaldu NON MSG HALAL Mama Kamu Ayam Kampung , S...</td>\n",
       "      <td>1663538013</td>\n",
       "      <td>[train_3419392575, train_1431563868, train_363...</td>\n",
       "      <td>kaldu non msg halal mama kamu ayam kampung , s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34249</th>\n",
       "      <td>train_1792180725</td>\n",
       "      <td>ffffa0ab2ae542357671e96254fa7167.jpg</td>\n",
       "      <td>af8bc4b2d2cf9083</td>\n",
       "      <td>FLEX TAPE PELAPIS BOCOR / ISOLASI AJAIB / ANTI...</td>\n",
       "      <td>459464107</td>\n",
       "      <td>[train_795128312, train_1792180725]</td>\n",
       "      <td>flex tape pelapis bocor / isolasi ajaib / anti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34250 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             posting_id                                 image  \\\n",
       "0       train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg   \n",
       "1      train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg   \n",
       "2      train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg   \n",
       "3      train_2406599165  00117e4fc239b1b641ff08340b429633.jpg   \n",
       "4      train_3369186413  00136d1cf4edede0203f32f05f660588.jpg   \n",
       "...                 ...                                   ...   \n",
       "34245  train_4028265689  fff1c07ceefc2c970a7964cfb81981c5.jpg   \n",
       "34246   train_769054909  fff401691371bdcb382a0d9075dfea6a.jpg   \n",
       "34247   train_614977732  fff421b78fa7284284724baf249f522e.jpg   \n",
       "34248  train_3630949769  fff51b87916dbfb6d0f8faa01bee67b8.jpg   \n",
       "34249  train_1792180725  ffffa0ab2ae542357671e96254fa7167.jpg   \n",
       "\n",
       "            image_phash                                              title  \\\n",
       "0      94974f937d4c2433                          Paper Bag Victoria Secret   \n",
       "1      af3f9460c2838f0f  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   \n",
       "2      b94cb00ed3e50f78        Maling TTS Canned Pork Luncheon Meat 397 gr   \n",
       "3      8514fc58eafea283  Daster Batik Lengan pendek - Motif Acak / Camp...   \n",
       "4      a6f319f924ad708c                  Nescafe \\xc3\\x89clair Latte 220ml   \n",
       "...                 ...                                                ...   \n",
       "34245  e3cd72389f248f21  Masker Bahan Kain Spunbond Non Woven 75 gsm 3 ...   \n",
       "34246  be86851f72e2853c    MamyPoko Pants Royal Soft - S 70 - Popok Celana   \n",
       "34247  ad27f0d08c0fcbf0  KHANZAACC Robot RE101S 1.2mm Subwoofer Bass Me...   \n",
       "34248  e3b13bd1d896c05c  Kaldu NON MSG HALAL Mama Kamu Ayam Kampung , S...   \n",
       "34249  af8bc4b2d2cf9083  FLEX TAPE PELAPIS BOCOR / ISOLASI AJAIB / ANTI...   \n",
       "\n",
       "       label_group                                             target  \\\n",
       "0        249114794                [train_129225211, train_2278313361]   \n",
       "1       2937985045               [train_3386243561, train_3423213080]   \n",
       "2       2395904891               [train_2288590299, train_3803689425]   \n",
       "3       4093212188               [train_2406599165, train_3342059966]   \n",
       "4       3648931069                [train_3369186413, train_921438619]   \n",
       "...            ...                                                ...   \n",
       "34245   3776555725               [train_2829161572, train_4028265689]   \n",
       "34246   2736479533                [train_1463059254, train_769054909]   \n",
       "34247   4101248785  [train_4126022211, train_3926241003, train_232...   \n",
       "34248   1663538013  [train_3419392575, train_1431563868, train_363...   \n",
       "34249    459464107                [train_795128312, train_1792180725]   \n",
       "\n",
       "                                             title_clean  \n",
       "0                              paper bag victoria secret  \n",
       "1      double tape 3m vhb 12 mm x 4,5 original / doub...  \n",
       "2             maling tt canned pork luncheon meat 397 gr  \n",
       "3      daster batik lengan pendek - motif acak / camp...  \n",
       "4                      nescafe \\xc3\\x89clair latte 220ml  \n",
       "...                                                  ...  \n",
       "34245  masker bahan kain spunbond non woven 75 gsm 3 ...  \n",
       "34246       mamypoko pant royal soft - 70 - popok celana  \n",
       "34247  khanzaacc robot re101s 1.2mm subwoofer bass me...  \n",
       "34248  kaldu non msg halal mama kamu ayam kampung , s...  \n",
       "34249  flex tape pelapis bocor / isolasi ajaib / anti...  \n",
       "\n",
       "[34250 rows x 7 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gf['title_clean'] = train['title_clean']\n",
    "train_gf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T10:02:00.631468Z",
     "start_time": "2021-03-18T10:01:59.851964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text embeddings shape (34250, 24652)\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# 25000 \n",
    "model = TfidfVectorizer(stop_words=None, binary=True, max_features=25000)\n",
    "# text_embeddings = model.fit_transform(train_gf.title).toarray()\n",
    "text_embeddings = model.fit_transform(train_gf.title_clean).toarray()\n",
    "print('text embeddings shape',text_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_preds(df, embeddings, threshold=0.75, PRINT_CHUNK=False):\n",
    "    print('Finding similar titles...')\n",
    "    CHUNK = 1024 * 4\n",
    "    CTS = len(df) // CHUNK\n",
    "    if (len(df)%CHUNK) != 0:\n",
    "        CTS += 1\n",
    "\n",
    "    preds = []\n",
    "    for j in range( CTS ):\n",
    "        a = j * CHUNK\n",
    "        b = (j+1) * CHUNK\n",
    "        b = min(b, len(df))\n",
    "        if PRINT_CHUNK:\n",
    "            print('chunk', a, 'to', b)\n",
    "\n",
    "        # COSINE SIMILARITY DISTANCE\n",
    "        cts = cupy.matmul(embeddings, embeddings[a:b].T).T\n",
    "        for k in range(b-a):\n",
    "            IDX = cupy.where(cts[k,]>threshold)[0]\n",
    "            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n",
    "            preds.append(o)\n",
    "            \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_threshold_searching(df, embeddings, LB=0.70, UB=0.80):\n",
    "    df1 = pd.DataFrame(columns = ['target', 'pred_matches'])\n",
    "    df1.target = df.target\n",
    "        \n",
    "    thresholds = list(np.arange(LB, UB, 0.02))\n",
    "    scores = []\n",
    "    for threshold in thresholds:\n",
    "        preds = get_text_preds(df, embeddings, threshold)\n",
    "        df1.pred_matches = preds\n",
    "        MyCVScore = df1.apply(getMetric('pred_matches'), axis=1)\n",
    "        score = MyCVScore.mean()\n",
    "        print(f'CV score for threshold {round(threshold, 2)} = {score}')\n",
    "        scores.append(score)\n",
    "    thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n",
    "    max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n",
    "    best_threshold = max_score['thresholds'].values[0]\n",
    "    best_score = max_score['scores'].values[0]\n",
    "    print(f'Our best score is {best_score} and has a threshold {round(best_threshold, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar titles...\n",
      "CV score for threshold 0.5 = 0.6591369293684559\n",
      "Finding similar titles...\n",
      "CV score for threshold 0.52 = 0.6610204806839047\n",
      "Finding similar titles...\n",
      "CV score for threshold 0.54 = 0.6614022356034018\n",
      "Finding similar titles...\n",
      "CV score for threshold 0.56 = 0.660011710952917\n",
      "Finding similar titles...\n",
      "CV score for threshold 0.58 = 0.6569429099082632\n",
      "Our best score is 0.6614022356034018 and has a threshold 0.54\n"
     ]
    }
   ],
   "source": [
    "cos_threshold_searching(train, text_embeddings, LB=0.50, UB=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T10:06:03.146166Z",
     "start_time": "2021-03-18T10:06:01.83687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar titles...\n",
      "CV score for baseline = 0.6614022356034018\n"
     ]
    }
   ],
   "source": [
    "preds = get_text_preds(train, text_embeddings, threshold=0.54, PRINT_CHUNK=False)\n",
    "train['oof_text'] = preds\n",
    "\n",
    "if COMPUTE_CV:\n",
    "    train['f1'] = train.apply(getMetric('oof_text'),axis=1)\n",
    "    print('CV score for baseline =',train.f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>target</th>\n",
       "      <th>oof_enet</th>\n",
       "      <th>f1</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>oof_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "      <td>[train_129225211, train_2278313361]</td>\n",
       "      <td>[train_129225211]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>paper bag victoria secret</td>\n",
       "      <td>[train_129225211, train_2278313361]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "      <td>[train_3386243561, train_3423213080]</td>\n",
       "      <td>[train_3386243561, train_3423213080]</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>double tape 3m vhb 12 mm x 4,5 original / doub...</td>\n",
       "      <td>[train_3386243561, train_3423213080, train_183...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2395904891</td>\n",
       "      <td>[train_2288590299, train_3803689425]</td>\n",
       "      <td>[train_2288590299]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>maling tt canned pork luncheon meat 397 gr</td>\n",
       "      <td>[train_2288590299, train_3803689425]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>4093212188</td>\n",
       "      <td>[train_2406599165, train_3342059966]</td>\n",
       "      <td>[train_2406599165]</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>daster batik lengan pendek - motif acak / camp...</td>\n",
       "      <td>[train_2406599165, train_3576714541, train_150...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>3648931069</td>\n",
       "      <td>[train_3369186413, train_921438619]</td>\n",
       "      <td>[train_3369186413, train_921438619]</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>nescafe \\xc3\\x89clair latte 220ml</td>\n",
       "      <td>[train_3369186413]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n",
       "3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n",
       "4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n",
       "\n",
       "                                               title  label_group  \\\n",
       "0                          Paper Bag Victoria Secret    249114794   \n",
       "1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n",
       "2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891   \n",
       "3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188   \n",
       "4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069   \n",
       "\n",
       "                                 target                              oof_enet  \\\n",
       "0   [train_129225211, train_2278313361]                     [train_129225211]   \n",
       "1  [train_3386243561, train_3423213080]  [train_3386243561, train_3423213080]   \n",
       "2  [train_2288590299, train_3803689425]                    [train_2288590299]   \n",
       "3  [train_2406599165, train_3342059966]                    [train_2406599165]   \n",
       "4   [train_3369186413, train_921438619]   [train_3369186413, train_921438619]   \n",
       "\n",
       "         f1                                        title_clean  \\\n",
       "0  1.000000                          paper bag victoria secret   \n",
       "1  0.666667  double tape 3m vhb 12 mm x 4,5 original / doub...   \n",
       "2  1.000000         maling tt canned pork luncheon meat 397 gr   \n",
       "3  0.200000  daster batik lengan pendek - motif acak / camp...   \n",
       "4  0.666667                  nescafe \\xc3\\x89clair latte 220ml   \n",
       "\n",
       "                                            oof_text  \n",
       "0                [train_129225211, train_2278313361]  \n",
       "1  [train_3386243561, train_3423213080, train_183...  \n",
       "2               [train_2288590299, train_3803689425]  \n",
       "3  [train_2406599165, train_3576714541, train_150...  \n",
       "4                                 [train_3369186413]  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T10:06:04.931476Z",
     "start_time": "2021-03-18T10:06:04.925838Z"
    }
   },
   "outputs": [],
   "source": [
    "def combine_for_sub(row):\n",
    "    # x = np.concatenate([row.oof_text,row.oof_cnn, row.oof_hash])\n",
    "    x = np.concatenate([row.oof_text, row.oof_enet])\n",
    "    return ' '.join( np.unique(x) )\n",
    "\n",
    "def combine_for_cv(row):\n",
    "    # x = np.concatenate([row.oof_text,row.oof_cnn, row.oof_hash])\n",
    "    x = np.concatenate([row.oof_text, row.oof_enet])\n",
    "    return np.unique(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T10:06:09.759812Z",
     "start_time": "2021-03-18T10:06:05.955972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Score = 0.7260622833279889\n"
     ]
    }
   ],
   "source": [
    "if COMPUTE_CV:\n",
    "    # tmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\n",
    "    # train['target'] = train.label_group.map(tmp)\n",
    "    train['oof'] = train.apply(combine_for_cv,axis=1)\n",
    "    train['f1'] = train.apply(getMetric('oof'),axis=1)\n",
    "    print('CV Score =', train.f1.mean() )\n",
    "\n",
    "train['matches'] = train.apply(combine_for_sub,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>target</th>\n",
       "      <th>oof_enet</th>\n",
       "      <th>f1</th>\n",
       "      <th>title_clean</th>\n",
       "      <th>oof_text</th>\n",
       "      <th>oof</th>\n",
       "      <th>matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "      <td>[train_129225211, train_2278313361]</td>\n",
       "      <td>[train_129225211]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>paper bag victoria secret</td>\n",
       "      <td>[train_129225211, train_2278313361]</td>\n",
       "      <td>[train_129225211, train_2278313361]</td>\n",
       "      <td>train_129225211 train_2278313361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "      <td>[train_3386243561, train_3423213080]</td>\n",
       "      <td>[train_3386243561, train_3423213080]</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>double tape 3m vhb 12 mm x 4,5 original / doub...</td>\n",
       "      <td>[train_3386243561, train_3423213080, train_183...</td>\n",
       "      <td>[train_1831941588, train_3386243561, train_342...</td>\n",
       "      <td>train_1831941588 train_3386243561 train_342321...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2395904891</td>\n",
       "      <td>[train_2288590299, train_3803689425]</td>\n",
       "      <td>[train_2288590299]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>maling tt canned pork luncheon meat 397 gr</td>\n",
       "      <td>[train_2288590299, train_3803689425]</td>\n",
       "      <td>[train_2288590299, train_3803689425]</td>\n",
       "      <td>train_2288590299 train_3803689425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>4093212188</td>\n",
       "      <td>[train_2406599165, train_3342059966]</td>\n",
       "      <td>[train_2406599165]</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>daster batik lengan pendek - motif acak / camp...</td>\n",
       "      <td>[train_2406599165, train_3576714541, train_150...</td>\n",
       "      <td>[train_1508100548, train_1744956981, train_204...</td>\n",
       "      <td>train_1508100548 train_1744956981 train_204309...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>3648931069</td>\n",
       "      <td>[train_3369186413, train_921438619]</td>\n",
       "      <td>[train_3369186413, train_921438619]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>nescafe \\xc3\\x89clair latte 220ml</td>\n",
       "      <td>[train_3369186413]</td>\n",
       "      <td>[train_3369186413, train_921438619]</td>\n",
       "      <td>train_3369186413 train_921438619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n",
       "3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n",
       "4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n",
       "\n",
       "                                               title  label_group  \\\n",
       "0                          Paper Bag Victoria Secret    249114794   \n",
       "1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n",
       "2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891   \n",
       "3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188   \n",
       "4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069   \n",
       "\n",
       "                                 target                              oof_enet  \\\n",
       "0   [train_129225211, train_2278313361]                     [train_129225211]   \n",
       "1  [train_3386243561, train_3423213080]  [train_3386243561, train_3423213080]   \n",
       "2  [train_2288590299, train_3803689425]                    [train_2288590299]   \n",
       "3  [train_2406599165, train_3342059966]                    [train_2406599165]   \n",
       "4   [train_3369186413, train_921438619]   [train_3369186413, train_921438619]   \n",
       "\n",
       "         f1                                        title_clean  \\\n",
       "0  1.000000                          paper bag victoria secret   \n",
       "1  0.666667  double tape 3m vhb 12 mm x 4,5 original / doub...   \n",
       "2  1.000000         maling tt canned pork luncheon meat 397 gr   \n",
       "3  0.200000  daster batik lengan pendek - motif acak / camp...   \n",
       "4  1.000000                  nescafe \\xc3\\x89clair latte 220ml   \n",
       "\n",
       "                                            oof_text  \\\n",
       "0                [train_129225211, train_2278313361]   \n",
       "1  [train_3386243561, train_3423213080, train_183...   \n",
       "2               [train_2288590299, train_3803689425]   \n",
       "3  [train_2406599165, train_3576714541, train_150...   \n",
       "4                                 [train_3369186413]   \n",
       "\n",
       "                                                 oof  \\\n",
       "0                [train_129225211, train_2278313361]   \n",
       "1  [train_1831941588, train_3386243561, train_342...   \n",
       "2               [train_2288590299, train_3803689425]   \n",
       "3  [train_1508100548, train_1744956981, train_204...   \n",
       "4                [train_3369186413, train_921438619]   \n",
       "\n",
       "                                             matches  \n",
       "0                   train_129225211 train_2278313361  \n",
       "1  train_1831941588 train_3386243561 train_342321...  \n",
       "2                  train_2288590299 train_3803689425  \n",
       "3  train_1508100548 train_1744956981 train_204309...  \n",
       "4                   train_3369186413 train_921438619  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T10:06:12.385916Z",
     "start_time": "2021-03-18T10:06:12.180234Z"
    }
   },
   "outputs": [],
   "source": [
    "train[['posting_id','matches']].to_csv('submission.csv',index=False)\n",
    "sub = pd.read_csv('submission.csv')\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
